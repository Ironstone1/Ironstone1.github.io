<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>NNFS</title><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 h1 { color: #434344; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 51pt; }
 .s1 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s2 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 36pt; }
 .s3 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .p, p { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; margin:0pt; }
 .s4 { color: #434344; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .h2, h2 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .a, a { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s5 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s6 { color: #434344; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s7 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 25pt; }
 .h3, h3 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s8 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 24pt; }
 .s9 { color: #205E9E; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s10 { color: #231F20; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s11 { color: #C71F43; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s12 { color: #32A7BD; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s13 { color: #205E9E; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s14 { color: #7358A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s15 { color: #C71F43; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s16 { color: #205E9E; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s17 { color: #A5A4A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s18 { color: #434F54; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s19 { color: #212121; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s20 { color: #434F54; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s21 { color: #7358A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s22 { color: #231F20; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s23 { color: #CA6628; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s24 { color: #231F20; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s25 { color: #C71F43; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s26 { color: #7358A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s27 { color: #355CAA; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s29 { color: #CA6628; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s30 { color: #8F8633; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s31 { color: #32A7BD; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s32 { color: #427F3C; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s33 { color: #32A7BD; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s35 { color: #427F3C; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s36 { color: #32A7BD; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s38 { color: #8F8633; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s39 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s40 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s41 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 4pt; }
 .s42 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s43 { color: #32A7BD; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s44 { color: #427F3C; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s45 { color: #F9F8F0; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s46 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 16pt; }
 .s47 { color: #231F20; font-family:Cambria, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s49 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s51 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s53 { color: #32A7BD; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s54 { color: #231F20; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s55 { color: #7358A5; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s56 { color: #C71F43; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s57 { color: #231F20; font-family:"Courier New", monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s59 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: -4pt; }
 .s61 { color: #32A7BD; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt; }
 .s62 { color: #4C4D4F; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s63 { color: #4C4D4F; font-family:"Arial Black", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s64 { color: #32A7BD; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s65 { color: #A5A4A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s66 { color: #C71F43; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 11pt; }
 .s67 { color: #A5A4A5; font-family:Consolas, monospace; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 h4 { color: #231F20; font-family:Consolas, monospace; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s73 { color: #8F8633; font-family:Consolas, monospace; font-style: italic; font-weight: bold; text-decoration: none; font-size: 11pt; }
 .s74 { color: #00979D; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s75 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt; }
 .s76 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s78 { color: #231F20; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s79 { color: #7358A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s80 { color: #A5A4A5; font-family:Consolas, monospace; font-style: italic; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s81 { color: #A5A4A5; font-family:Consolas, monospace; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s82 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 .s83 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 11pt; }
 .s84 { color: #231F20; font-family:"Times New Roman", serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 18pt; }
 .s85 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s86 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s87 { color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 30pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 1; }
 #l1> li>*:first-child:before {counter-increment: c1; content: counter(c1, decimal)". "; color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l1> li:first-child>*:first-child:before {counter-increment: c1 0;  }
 li {display: block; }
 #l2 {padding-left: 0pt; }
 #l2> li>*:first-child:before {content: "- "; color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 li {display: block; }
 #l3 {padding-left: 0pt; }
 #l3> li>*:first-child:before {content: "- "; color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 li {display: block; }
 #l4 {padding-left: 0pt;counter-reset: f1 1; }
 #l4> li>*:first-child:before {counter-increment: f1; content: counter(f1, decimal)". "; color: #231F20; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt; }
 #l4> li:first-child>*:first-child:before {counter-increment: f1 0;  }
 li {display: block; }
 #l5 {padding-left: 0pt;counter-reset: g1 1; }
 #l5> li>*:first-child:before {counter-increment: g1; content: counter(g1, decimal)" "; color: black; font-style: italic; font-weight: normal; text-decoration: none; }
 #l5> li:first-child>*:first-child:before {counter-increment: g1 0;  }
 table, tbody {vertical-align: top; overflow: visible; }
</style></head><body><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-top: 12pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Neural Networks from Scratch in Python</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s1" style="padding-top: 42pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Harrison Kinsley &amp; Daniel Kukieła</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark0">Acknowledgements</a><a name="bookmark1">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Harrison Kinsley:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">My wife, Stephanie, for her unfailing support and faith in me throughout the years. You’ve never doubted me.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Each and every viewer and person who supported this book and project. Without my audience, none of this would have been possible.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">The Python programming community in general for being awesome!</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">Daniel Kukie<span class="s4">ł</span>a for your unwavering effort with this massive project that Neural Networks from Scratch became. From learning C++ to make mods in GTA V, to Python for various projects, to the calculus behind neural networks, there doesn’t seem to be any problem you cannot solve and it is a pleasure to do this for a living with you. I look forward to seeing what’s next!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Daniel Kukieła:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">My son, Oskar, for his patience and understanding during the busy days. My wife, Katarzyna,</p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">for the boundless love, faith and support in all the things I do, have ever done, and plan to do, the sunlight during most stormy days and the morning coffee every single day.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Harrison for challenging me to learn Python then pushing me towards learning neural networks. For showing me that things do not have to be perfectly done, all the support, and making me a part of so many interesting projects including “let’s make a tutorial on neural networks from scratch,” which turned into one the biggest challenges of my life — this book. I wouldn’t be at where I am now if all of that didn’t happen.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The Python community for making me a better programmer and for helping me to improve my language skills.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Table of Contents</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark1" class="h2">Acknowledgements </a><h2 href="#bookmark1">2</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark3" class="h2">Copyright </a><h2 href="#bookmark3">8</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark5" class="h2">License for Code </a><h2 href="#bookmark5">9</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><h2 href="#bookmark7">Readme 10</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark11" class="h2">Introducing Neural Networks </a><h2 href="#bookmark11">11</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark12" class="a">A Brief History </a><a href="#bookmark12">12</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark14" class="a">What is a Neural Network? </a><a href="#bookmark14">13</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark31" class="h2">Coding Our First Neurons </a><h2 href="#bookmark31">25</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark33" class="a">A Single Neuron </a><a href="#bookmark33">26</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark35" class="a">A Layer of Neurons </a><a href="#bookmark35">30</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark38" class="a">Tensors, Arrays and Vectors </a><a href="#bookmark38">34</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark42" class="a">Dot Product and Vector Addition </a><a href="#bookmark42">38</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark45" class="a">A Single Neuron with NumPy </a><a href="#bookmark45">40</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark49" class="a">A Batch of Data </a><a href="#bookmark49">44</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark52" class="a">Matrix Product </a><a href="#bookmark52">47</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark54" class="a">Transposition for the Matrix Product </a><a href="#bookmark54">50</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark56" class="a">A Layer of Neurons &amp; Batch of Data w/ NumPy </a><a href="#bookmark56">54</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark64" class="h2">Adding Layers </a><h2 href="#bookmark64">59</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark66" class="a">Training Data </a><a href="#bookmark66">62</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark68" class="a">Dense Layer Class </a><a href="#bookmark68">66</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark71" class="a">Full code up to this point: </a><a href="#bookmark71">70</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark84" class="h2">Activation Functions </a><h2 href="#bookmark84">72</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark86" class="a">The Step Activation Function </a><a href="#bookmark86">73</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark88" class="a">The Linear Activation Function </a><a href="#bookmark88">74</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark90" class="a">The Sigmoid Activation Function </a><a href="#bookmark90">75</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark92" class="a">The Rectified Linear Activation Function </a><a href="#bookmark92">76</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark94" class="a">Why Use Activation Functions? </a><a href="#bookmark94">77</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark96" class="a">Linear Activation in the Hidden Layers </a><a href="#bookmark96">79</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark97" class="a">ReLU Activation in a Pair of Neurons </a><a href="#bookmark97">81</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark99" class="a">ReLU Activation in the Hidden Layers </a><a href="#bookmark99">85</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark101" class="a">ReLU Activation Function Code </a><a href="#bookmark101">95</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark103" class="a">The Softmax Activation Function </a><a href="#bookmark103">98</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark105" class="a">Full code up to this point: </a><a href="#bookmark105">108</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark111" class="h2">Calculating Network Error with Loss </a><h2 href="#bookmark111">111</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark113" class="a">Categorical Cross-Entropy Loss </a><a href="#bookmark113">112</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark118" class="a">The Categorical Cross-Entropy Loss Class </a><a href="#bookmark118">123</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark119" class="a">Combining everything up to this point: </a><a href="#bookmark119">125</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark120" class="a">Accuracy Calculation </a><a href="#bookmark120">129</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark124" class="h2">Introducing Optimization </a><h2 href="#bookmark124">131</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark126" class="a">Full code up to this point: </a><a href="#bookmark126">136</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark133" class="h2">Derivatives </a><h2 href="#bookmark133">139</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark135" class="a">The Impact of a Parameter on the Output </a><a href="#bookmark135">140</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark136" class="a">The Slope </a><a href="#bookmark136">142</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark139" class="a">The Numerical Derivative </a><a href="#bookmark139">146</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark141" class="a">The Analytical Derivative </a><a href="#bookmark141">154</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark144" class="a">Summary </a><a href="#bookmark144">164</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark153" class="h2">Gradients, Partial Derivatives, and the Chain Rule </a><h2 href="#bookmark153">166</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark154" class="a">The Partial Derivative </a><a href="#bookmark154">167</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark156" class="a">The Partial Derivative of a Sum </a><a href="#bookmark156">168</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark157" class="a">The Partial Derivative of Multiplication </a><a href="#bookmark157">170</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark158" class="a">The Partial Derivative of </a><a href="#bookmark158" class="s5">Max 172</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark159" class="a">The Gradient </a><a href="#bookmark159">173</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark161" class="a">The Chain Rule </a><a href="#bookmark161">174</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark163" class="a">Summary </a><a href="#bookmark163">178</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark172" class="h2">Backpropagation </a><h2 href="#bookmark172">180</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark175" class="a">Categorical Cross-Entropy loss derivative </a><a href="#bookmark175">215</a></p><p style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark177" class="a">Categorical Cross-Entropy loss derivative code implementation </a><a href="#bookmark177">218</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark178" class="a">Softmax activation derivative </a><a href="#bookmark178">220</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark180" class="a">Softmax activation derivative code implementation </a><a href="#bookmark180">226</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;line-height: 112%;text-align: center;"><a href="#bookmark183" class="a">Common Categorical Cross-Entropy loss and Softmax activation derivative 230 Common Categorical Cross-Entropy loss and Softmax activation derivative - code implementation                                                                                        </a><a href="#bookmark183">234</a></p><p style="padding-left: 32pt;text-indent: 0pt;line-height: 16pt;text-align: center;"><a href="#bookmark184">Full code up to this point:                                                                                 243</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark194" class="h2">Optimizers </a><h2 href="#bookmark194">249</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark196" class="a">Stochastic Gradient Descent (SGD) </a><a href="#bookmark196">250</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark199" class="a">Learning Rate </a><a href="#bookmark199">257</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark204" class="a">Learning Rate Decay </a><a href="#bookmark204">274</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark206" class="a">Stochastic Gradient Descent with Momentum </a><a href="#bookmark206">283</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark208" class="a">AdaGrad </a><a href="#bookmark208">293</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark210" class="a">RMSProp </a><a href="#bookmark210">298</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark212" class="a">Adam </a><a href="#bookmark212">304</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark214" class="a">Full code up to this point: </a><a href="#bookmark214">309</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark216" class="h2">Testing with Out-of-Sample Data </a><h2 href="#bookmark216">321</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark222" class="h2">Validation Data </a><h2 href="#bookmark222">328</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark226" class="h2">Training Dataset </a><h2 href="#bookmark226">332</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark232" class="h2">L1 and L2 Regularization </a><h2 href="#bookmark232">335</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark234" class="a">Forward Pass </a><a href="#bookmark234">336</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark236" class="a">Backward pass </a><a href="#bookmark236">340</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark241" class="h2">Dropout </a><h2 href="#bookmark241">361</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark243" class="a">Forward Pass </a><a href="#bookmark243">362</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark245" class="a">Backward Pass </a><a href="#bookmark245">369</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark246" class="a">The Code </a><a href="#bookmark246">370</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark256" class="h2">Binary Logistic Regression </a><h2 href="#bookmark256">388</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark258" class="a">Sigmoid Activation Function </a><a href="#bookmark258">389</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark260" class="a">Sigmoid Function Derivative </a><a href="#bookmark260">391</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark261" class="a">Sigmoid Function Code </a><a href="#bookmark261">395</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark262" class="a">Binary Cross-Entropy Loss </a><a href="#bookmark262">396</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark264" class="a">Binary Cross-Entropy Loss Derivative </a><a href="#bookmark264">398</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark266" class="a">Binary Cross-Entropy Code </a><a href="#bookmark266">401</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark267" class="a">Implementing Binary Logistic Regression and Binary Cross-Entropy Loss . </a><a href="#bookmark267">404</a></p><p style="padding-top: 3pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark268" class="a">Full code up to this point: </a><a href="#bookmark268">407</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark280" class="h2">Regression </a><h2 href="#bookmark280">423</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark282" class="a">Linear Activation </a><a href="#bookmark282">425</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark284" class="a">Mean Squared Error Loss </a><a href="#bookmark284">426</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark286" class="a">Mean Squared Error Loss Derivative </a><a href="#bookmark286">427</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark287" class="a">Mean Squared Error (MSE) Loss Code </a><a href="#bookmark287">428</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark288" class="a">Mean Absolute Error Loss </a><a href="#bookmark288">429</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark290" class="a">Mean Absolute Error Loss Derivative </a><a href="#bookmark290">430</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark291" class="a">Mean Absolute Error Loss Code </a><a href="#bookmark291">431</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark292" class="a">Accuracy in Regression </a><a href="#bookmark292">432</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark294" class="a">Regression Model Training </a><a href="#bookmark294">433</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark295" class="a">Full code up to this point: </a><a href="#bookmark295">458</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark298" class="h2">Model Object </a><h2 href="#bookmark298">475</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark300" class="a">Full code up to this point: </a><a href="#bookmark300">512</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark309" class="h2">A Real Dataset </a><h2 href="#bookmark309">532</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark311" class="a">Data preparation </a><a href="#bookmark311">534</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark312" class="a">Data loading </a><a href="#bookmark312">536</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark315" class="a">Data preprocessing </a><a href="#bookmark315">543</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark319" class="a">Data Shuffling </a><a href="#bookmark319">546</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark321" class="a">Batches </a><a href="#bookmark321">549</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark323" class="a">Training </a><a href="#bookmark323">563</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark325" class="a">Full code up to now: </a><a href="#bookmark325">570</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark327" class="h2">Model Evaluation </a><h2 href="#bookmark327">594</h2></p><p style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;text-align: left;"><a href="#bookmark336" class="h2">Saving and Loading Models and Their Parameters </a><h2 href="#bookmark336">601</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark336" class="a">Retrieving Parameters </a><a href="#bookmark336">601</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark337" class="a">Setting Parameters </a><a href="#bookmark337">605</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark338" class="a">Saving Parameters </a><a href="#bookmark338">609</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark339" class="a">Loading Parameters </a><a href="#bookmark339">610</a></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark340" class="a">Saving the Model </a><a href="#bookmark340">612</a></p><p style="padding-top: 1pt;padding-left: 32pt;text-indent: 0pt;text-align: left;"><a href="#bookmark342" class="a">Loading the Model </a><a href="#bookmark342">615</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark345" class="h2">Prediction / Inference </a><h2 href="#bookmark345">617</h2></p><p style="padding-top: 1pt;padding-left: 33pt;text-indent: 0pt;text-align: left;"><a href="#bookmark347" class="a">Full code: </a><a href="#bookmark347">633</a></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark349" class="h2">Closing </a><h2 href="#bookmark349">661</h2></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark2">Copyright</a><a name="bookmark3">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Copyright © 2020 Harrison Kinsley</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Cover Design copyright © 2020 Harrison Kinsley</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">No part of this book may be reproduced in any form or by any electronic or mechanical means, with the following exceptions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li><p style="padding-left: 19pt;text-indent: -11pt;text-align: left;">Brief quotations from the book.</p></li><li><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Python Code/software (strings interpreted as logic with Python), which is housed under the MIT license, described on the next page.</p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark4">License for Code</a><a name="bookmark5">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 209%;text-align: left;">The Python code/software in this book is contained under the following MIT License: Copyright © 2020 Sentdex, Kinsley Enterprises Inc., https://nnfs.io</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction,</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark6">Readme</a><a name="bookmark7">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The objective of this book is to break down an extremely complex topic, neural networks, into small pieces, consumable by anyone wishing to embark on this journey. Beyond breaking down this topic, the hope is to dramatically demystify neural networks. As you will soon see, this subject, when explored from scratch, can be an educational and engaging experience. This book is for anyone willing to put in the time to sit down and work through it. In return, you will gain a far deeper understanding than most when it comes to neural networks and deep learning.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This book will be easier to understand if you already have an understanding of Python or another programming language. Python is one of the most clear and understandable programming languages; we have no real interest in padding page counts and exhausting an entire first chapter with a basics of Python tutorial. If you need one, we suggest you start here: <i>https://pythonprogramming.net/python-fundamental-tutorials/</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">To cite this material:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Harrison Kinsley &amp; Daniel Kukie<span class="s6">ł</span>a. Neural Networks from Scratch (NNFS). https://nnfs.io</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark8">Chapter 1</a><a name="bookmark11">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Introducing Neural Networks</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We begin with a general idea of what <b>neural networks </b>are and why you might be interested in them. Neural networks, also called <b>Artificial Neural Networks </b>(though it seems, in recent years, we’ve dropped the “artificial” part), are a type of machine learning often conflated with</p><p style="padding-bottom: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">deep learning. The defining characteristic of a <i>deep </i>neural network is having two or more <b>hidden layers </b>— a concept that will be explained shortly, but these hidden layers are ones that the neural network controls. It’s reasonably safe to say that most neural networks in use are a form of deep learning.</p><p style="padding-left: 143pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 9pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Fig 1.01: <span class="p">Depicting the various fields of artificial intelligence and where they fit in overall.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark9">A Brief History</a><a name="bookmark12">&zwnj;</a><a name="bookmark13">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since the advent of computers, scientists have been formulating ways to enable machines to take input and produce desired output for tasks like <b>classification </b>and <b>regression</b>. Additionally, in general, there’s <b>supervised </b>and <b>unsupervised </b>machine learning. Supervised machine learning is used when you have pre-established and labeled data that can be used for training. Let’s say you have sensor data for a server with metrics such as upload/download rates, temperature, and humidity, all organized by time for every 10 minutes. Normally, this server operates as intended</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">and has no outages, but sometimes parts fail and cause an outage. We might collect data and then divide it into two classes: one class for times/observations when the server is operating normally, and another class for times/observations when the server is experiencing an outage. When the server is failing, we want to label that sensor data leading up to failure as data that preceded a failure. When the server is operating normally, we simply label that data as “normal.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">What each sensor measures in this example is called a feature. A group of features makes up a feature set (represented as vectors/arrays), and the values of a feature set can be referred to as a sample. Samples are fed into neural network models to train them to fit desired outputs from these inputs or to predict based on them during the inference phase.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The “normal” and “failure” labels are <b>classifications </b>or <b>labels</b><i>. </i>You may also see these referred to as <b>targets </b>or <b>ground-truths </b>while we fit a machine learning algorithm. These targets are the classifications that are the <i>goal </i>or <i>target</i>, known to be <i>true and correct</i>, for the algorithm</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">to learn. For this example, the aim is to eventually train an algorithm to read sensor data and accurately predict when a failure is imminent. This is just one example of supervised learning in the form of classification. In addition to classification, there’s also regression, which is used to predict numerical values, like stock prices. There’s also unsupervised machine learning, where the machine finds structure in data without knowing the labels/classes ahead of time. There are additional concepts (e.g., reinforcement learning and semi-supervised machine learning) that fall under the umbrella of neural networks. For this book, we will focus on classification and regression with neural networks, but what we cover here leads to other use-cases.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Neural networks were conceived in the 1940s, but figuring out how to train them remained a mystery for 20 years. The concept of <b>backpropagation </b>(explained later) came in the 1960s, but neural networks still did not receive much attention until they started winning competitions in 2010. Since then, neural networks have been on a meteoric rise due to their sometimes seemingly</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark10">magical ability to solve problems previously deemed unsolvable, such as image captioning, language translation, audio and video synthesis, and more.</a><a name="bookmark14">&zwnj;</a><a name="bookmark15">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Currently, neural networks are the primary solution to most competitions and challenging technological problems like self-driving cars, calculating risk, detecting fraud, and early cancer detection, to name a few.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">What is a Neural Network?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">“Artificial” neural networks are inspired by the organic brain, translated to the computer. It’s not a perfect comparison, but there are neurons, activations, and lots of interconnectivity, even if the underlying processes are quite different.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 1.02: <span class="p">Comparing a biological neuron to an artificial neuron.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">A single neuron by itself is relatively useless, but, when combined with hundreds or thousands (or many more) of other neurons, the interconnectivity produces relationships and results that frequently outperform any other machine learning methods.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 61pt;text-indent: 0pt;text-align: left;"><a name="bookmark16"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 1.03: <span class="p">Example of a neural network with 3 hidden layers of 16 neurons each.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/ntr" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 1.03: </a><a href="https://nnfs.io/ntr" class="s9" target="_blank">https://nnfs.io/ntr</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The above animation shows the examples of the model structures and the numbers of parameters the model has to learn to adjust in order to produce the desired outputs. The details of what is seen here are the subjects of future chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It might seem rather complicated when you look at it this way. Neural networks are considered to be “black boxes” in that we often have no idea <i>why </i>they reach the conclusions they do. We do understand <i>how </i>they do this, though.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Dense layers, the most common layers, consist of interconnected neurons. In a dense layer, each neuron of a given layer is connected to every neuron of the next layer, which means that its output value becomes an input for the next neurons. Each connection between neurons has a weight associated with it, which is a trainable factor of how much of this input to use, and this weight gets multiplied by the input value. Once all of the <i>inputs·weights </i>flow into our neuron, they are</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;"><a name="bookmark17">summed, and a bias, another trainable parameter, is added. The purpose of the bias is to offset the output positively or negatively, which can further help us map more real-world types of dynamic data. In chapter 4, we will show some examples of how this works.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The concept of weights and biases can be thought of as “knobs” that we can tune to fit our model to data. In a neural network, we often have thousands or even millions of these parameters tuned by the optimizer during training. Some may ask, “why not just have biases or just weights?” Biases and weights are both tunable parameters, and both will impact the neurons’ outputs, but they do so in different ways. Since weights are multiplied, they will only change the magnitude or even completely flip the sign from positive to negative, or vice versa. <i>Output = weight·input+bias </i>is not unlike the equation for a line <i>y = mx+b</i>. We can visualize this with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><b>Fig 1.04: </b>Graph of a single-input neuron’s output with a weight of 1, bias of 0 and input <i>x</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Adjusting the weight will impact the slope of the function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 27pt;text-indent: 0pt;text-align: left;"><b>Fig 1.05: </b>Graph of a single-input neuron’s output with a weight of 2, bias of 0 and input <i>x</i>.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we increase the value of the weight, the slope will get steeper. If we decrease the weight, the slope will decrease. If we negate the weight, the slope turns to a negative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><b>Fig 1.06: </b>Graph of a single-input neuron’s output with a weight of -0.70, bias of 0 and input <i>x</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This should give you an idea of how the weight impacts the neuron’s output value that we get from <i>inputs·weights+bias. </i>Now, how about the bias parameter? The bias offsets the overall function. For example, with a weight of 1.0 and a bias of 2.0:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 27pt;text-indent: 0pt;text-align: left;"><b>Fig 1.07: </b>Graph of a single-input neuron’s output with a weight of 1, bias of 2 and input <i>x</i>.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we increase the bias, the function output overall shifts upward. If we decrease the bias, then the overall function output will move downward. For example, with a negative bias:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 134pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;"><b>Fig 1.08: </b>Graph of a single-input neuron’s output with a weight of 1.0, bias of -0.70 and input <i>x</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bru" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 1.04-1.08: </a><a href="https://nnfs.io/bru" class="s9" target="_blank">https://nnfs.io/bru</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, weights and biases help to impact the outputs of neurons, but they do so in slightly different ways. This will make even more sense when we cover <b>activation functions </b>in chapter 4. Still, you can hopefully already see the differences between weights and biases and how they might individually help to influence output. Why this matters will be conveyed shortly.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark18">As a very general overview, the step function meant to mimic a neuron in the brain, either “firing” or not — like an on-off switch. In programming, an on-off switch as a function would be called a </a><b>step function </b>because it looks like a step if we graph it.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 1.09: <span class="p">Graph of a step function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">For a step function, if the neuron’s output value, which is calculated by <i>sum(inputs · weights) + bias</i>, is greater than 0, the neuron fires (so it would output a 1). Otherwise, it does not fire and would pass along a 0. The formula for a single neuron might look something like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>(inputs <span style=" color: #C71F43;">* </span>weights) <span style=" color: #C71F43;">+ </span>bias</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We then usually apply an activation function to this output, noted by <i>activation()</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>activation(output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While you can use a step function for your activation function, we tend to use something slightly more advanced. Neural networks of today tend to use more informative activation functions (rather than a step function), such as the <b>Rectified Linear </b>(ReLU) activation function, which we will cover in-depth in Chapter 4. Each neuron’s output could be a part of the ending output layer, as well as the input to another layer of neurons. While the full function of a neural network can get very large, let’s start with a simple example with 2 hidden layers of 4 neurons each.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 141pt;text-indent: 0pt;text-align: left;"><a name="bookmark19"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 1.10: <span class="p">Example basic neural network.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Along with these 2 hidden layers, there are also two more layers here — the input and output layers. The input layer represents your actual input data, for example, pixel values from an image or data from a temperature sensor. While this data can be “raw” in the exact form it was collected, you will typically <b>preprocess </b>your data through functions like <b>normalization </b>and <b>scaling</b>,</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">and your input needs to be in numeric form. Concepts like scaling and normalization will be covered later in this book. However, it is common to preprocess data while retaining its features and having the values in similar ranges between 0 and 1 or -1 and 1. To achieve this, you will use either or both scaling and normalization functions. The output layer is whatever the neural network returns. With classification, where we aim to predict the class of the input, the output layer often has as many neurons as the training dataset has classes, but can also have a single output neuron for binary (two classes) classification. We’ll discuss this type of model later and,</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">for now, focus on a classifier that uses a separate output neuron per each class. For example, if our goal is to classify a collection of pictures as a “dog” or “cat,” then there are two classes in total.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This means our output layer will consist of two neurons; one neuron associated with “dog” and the other with “cat.” You could also have just a single output neuron that is “dog” or “not dog.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 9pt;text-indent: 0pt;text-align: justify;">Fig 1.11: <span class="p">Visual depiction of passing image data through a neural network, getting a classification</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">For each image passed through this neural network, the final output will have a calculated value in the “cat” output neuron, and a calculated value in the “dog” output neuron. The output neuron that received the highest score becomes the class prediction for the image used as input.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 83pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 11pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 1.12: <span class="p">Visual depiction of passing image data through a neural network, getting a classification</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/qtb" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 1.11-1.12: </a><a href="https://nnfs.io/qtb" class="s9" target="_blank">https://nnfs.io/qtb</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The thing that makes neural networks appear challenging is the math involved and how scary it can sometimes look. For example, let’s imagine a neural network, and take a journey through what’s going on during a simple forward pass of data, and the math behind it. Neural networks</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">are really only a bunch of math equations that we, programmers, can turn into code. For this, do not worry about understanding everything. The idea here is to give you a high-level impression of what’s going on overall. Then, this book’s purpose is to break down each of these elements into painfully simple explanations, which will cover both forward and backward passes involved in training neural networks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">When represented as one giant function, an example of a neural network’s forward pass would be computed with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 1.13: <span class="p">Full formula for the forward pass of an example neural network model.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/vkt" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 1.13: </a><a href="https://nnfs.io/vkt" class="s9" target="_blank">https://nnfs.io/vkt</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Naturally, that looks extremely confusing, and the above is actually the easy part of neural networks. This turns people away, understandably. In this book, however, we’re going to be coding everything from scratch, and, when doing this, you should find that there’s no step along the way to producing the above function that is very challenging to understand. For example, the above function can also be represented in nested python functions like:</p><p style="padding-left: 125pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 50pt;text-indent: 0pt;text-align: left;">Fig 1.14: <span class="p">Python code for the forward pass of an example neural network model.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">There may be some functions there that you don’t understand yet. For example, maybe you do not know what a log function is, but this is something simple that we’ll cover. Then we have a sum operation, an exponentiating operation (again, you may not exactly know what this does, but it’s nothing hard). Then we have a dot product, which is still just about understanding how it works, there’s nothing there that is over your head if you know how multiplication works! Finally, we have some transposes, noted as .T, which, again, once you learn what that operation does, is not a challenging concept. Once we’ve separated each of these elements, learning what they do and how they work, suddenly, things will not appear to be as daunting or foreign. Nothing in this forward pass requires education beyond basic high school algebra! For an animation that depicts how all</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">of this works in Python, you can check out the following animation, but it’s certainly not expected that you’d immediately understand what’s going on. The point is that this seemingly complex topic can be broken down into small, easy to understand parts, which is the purpose of the coming chapters!</p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><a name="bookmark20"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/vkr" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 1.14: </a><a href="https://nnfs.io/vkr" class="s9" target="_blank">https://nnfs.io/vkr</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A typical neural network has thousands or even up to millions of adjustable <b>parameters </b>(weights and biases). In this way, neural networks act as enormous functions with vast numbers of <b>parameters</b>. The concept of a long function with millions of variables that could be used to solve a problem isn’t all too difficult. With that many variables related to neurons, arranged as</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">interconnected layers, we can imagine there exist some combinations of values for these variables that will yield desired outputs. Finding that combination of parameter (weight and bias) values is the challenging part.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The end goal for neural networks is to adjust their weights and biases (the parameters), so when applied to a yet-unseen example in the input, they produce the desired output. When supervised machine learning algorithms are trained, we show the algorithm examples of inputs and their associated desired outputs. One major issue with this concept is <b>overfitting </b>— when the algorithm only learns to fit the training data but doesn’t actually “understand” anything about underlying input-output dependencies. The network basically just “memorizes” the training data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Thus, we tend to use “in-sample” data to train a model and then use “out-of-sample” data to validate an algorithm (or a neural network model in our case). Certain percentages are set aside for both datasets to partition the data. For example, if there is a dataset of 100,000 samples of data and labels, you will immediately take 10,000 and set them aside to be your “out-of-sample” or “validation” data. You will then train your model with the other 90,000 in-sample or “training” data and finally validate your model with the 10,000 out-of-sample data that the model hasn’t yet seen. The goal is for the model to not only accurately predict on the training data, but also to be similarly accurate while predicting on the withheld out-of-sample validation data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is called <b>generalization</b>, which means learning to fit the data instead of memorizing it. The idea is that we “train” (slowly adjusting weights and biases) a neural network on many examples of data. We then take out-of-sample data that the neural network has never been presented with and hope it can accurately predict on these data too.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">You should now have a general understanding of what neural networks are, or at least what the objective is, and how we plan to meet this objective. To train these neural networks, we calculate</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">how “wrong” they are using algorithms to calculate the error (called <b>loss</b>), and attempt to slowly adjust their parameters (weights and biases) so that, over many iterations, the network gradually becomes less wrong. The goal of all neural networks is to generalize, meaning the network can see many examples of never-before-seen data, and accurately output the values we hope to achieve.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Neural networks can be used for more than just classification. They can perform regression (predict a scalar, singular, value), clustering (assign unstructured data into groups), and many other tasks. Classification is just a common task for neural networks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch1" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch1<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark21">Chapter 2</a><a name="bookmark31">&zwnj;</a><a name="bookmark32">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Coding Our First Neurons</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 31pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While we assume that we’re all beyond beginner programmers here, we will still try to start slowly and explain things the first time we see them. To begin, we will be using <b>Python 3.7 </b>(although any version of Python 3+ will likely work). We will also be using <b>NumPy </b>after showing the pure-Python methods and Matplotlib for some visualizations. It should be the case that a huge variety of versions should work, but you may wish to match ours exactly to rule out any version issues. Specifically, we are using:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Python 3.7.5</p><p class="s3" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">NumPy 1.15.0</p><p class="s3" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Matplotlib 3.1.1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since this is a <i>Neural Networks from Scratch in Python </i>book, we will demonstrate how to do things without NumPy as well, but NumPy is Python’s all-things-numbers package. Building from scratch is the point of this book though ignoring NumPy would be a disservice since it is among the most, if not the most, important and useful packages for data science in Python.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark22">A Single Neuron</a><a name="bookmark33">&zwnj;</a><a name="bookmark34">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s say we have a single neuron, and there are three inputs to this neuron. As in most cases, when you initialize parameters in neural networks, our network will have weights initialized randomly, and biases set as zero to start. Why we do this will become apparent later on. The input will be either actual training data or the outputs of neurons from the previous layer in the neural network. We’re just going to make up values to start with as input for now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Each input also needs a weight associated with it. Inputs are the data that we pass into the model to get desired outputs, while the weights are the parameters that we’ll tune later on to get these results. Weights are one of the types of values that change inside the model during the training phase, along with biases that also change during training. The values for weights and biases are what get “trained,” and they are what make a model actually work (or not work). We’ll start by making up weights for now. Let’s say the first input, at index 0, which is a 1, has a weight of 0.2, the second input has a weight of 0.8, and the third input has a weight of -0.5. Our input and weights lists should now be:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, we need the bias. At the moment, we’re modeling a single neuron with three inputs. Since we’re modeling a single neuron, we only have one bias, as there’s just one bias value per neuron. The bias is an additional tunable value but is not associated with any input in contrast to the weights. We’ll randomly select a value of 2 as the bias for this example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">bias <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">This neuron sums each input multiplied by that input’s weight, then adds the bias. All the neuron does is take the fractions of inputs, where these fractions (weights) are the adjustable parameters, and adds another adjustable parameter — the bias — then outputs the result. Our output would be calculated up to this point like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>(inputs[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;line-height: 109%;text-align: left;">inputs[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">+ </span>bias)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2.3</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">The output here should be <b>2.3</b>. We will use <span class="s15">&gt;&gt;&gt; </span>to denote output in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.01: <span class="p">Visualizing the code that makes up the math of a basic neuron.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bkr" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.01: </a><a href="https://nnfs.io/bkr" class="s9" target="_blank">https://nnfs.io/bkr</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">What might we need to change if we have 4 inputs, rather than the 3 we’ve just shown? Next to the additional input, we need to add an associated weight, which this new input will be multiplied with. We’ll make up a value for this new weight as well. Code for this data could be:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>, <span style=" color: #7358A5;">2.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1.0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">bias <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Which could be depicted visually as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 183pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.02: <span class="p">Visualizing how the inputs, weights, and biases from the code interact with the neuron.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/djp" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.02: </a><a href="https://nnfs.io/djp" class="s9" target="_blank">https://nnfs.io/djp</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">All together in code, including the new input and weight, to produce output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>, <span style=" color: #7358A5;">2.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1.0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">bias <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>(inputs[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;line-height: 109%;text-align: left;">inputs[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">*</span>weights[<span style=" color: #7358A5;">3</span>] <span style=" color: #C71F43;">+ </span>bias)</p><p class="s12" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">4.8</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.03: <span class="p">Visualizing the code that makes up a basic neuron, with 4 inputs this time.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Anim 2.03: <span class="s16">https://nnfs.io/djp</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark23">A Layer of Neurons</a><a name="bookmark35">&zwnj;</a><a name="bookmark36">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Neural networks typically have layers that consist of more than one neuron. Layers are nothing more than groups of neurons. Each neuron in a layer takes exactly the same input — the input given to the layer (which can be either the training data or the output from the previous layer), but contains its own set of weights and its own bias, producing its own unique output. The layer’s output is a set of each of these outputs — one per each neuron. Let’s say we have a scenario with 3 neurons in a layer and 4 inputs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 193pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 9pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.04: <span class="p">Visualizing a layer of neurons with common input.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/mxo" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.04: </a><a href="https://nnfs.io/mxo" class="s9" target="_blank">https://nnfs.io/mxo</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll keep the initial 4 inputs and set of weights for the first neuron the same as we’ve been using so far. We’ll add 2 additional, made up, sets of weights and 2 additional biases to form 2 new neurons for a total of 3 in the layer. The layer’s output is going to be a list of 3 values, not just a single value like for a single neuron.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">weights1 <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights2 <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights3 <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">bias1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">bias2 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">3</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">bias3 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">outputs <span style=" color: #C71F43;">= </span>[</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Neuron 1: </span>inputs[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>weights1[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>weights1[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>weights1[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">*</span>weights1[<span style=" color: #7358A5;">3</span>] <span style=" color: #C71F43;">+ </span>bias1,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Neuron 2: </span>inputs[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>weights2[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>weights2[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>weights2[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">*</span>weights2[<span style=" color: #7358A5;">3</span>] <span style=" color: #C71F43;">+ </span>bias2,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Neuron 3: </span>inputs[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>weights3[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>weights3[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>weights3[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">+ </span>inputs[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">*</span>weights3[<span style=" color: #7358A5;">3</span>] <span style=" color: #C71F43;">+ </span>bias3]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(outputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, <span style=" color: #7358A5;">2.385</span>]</p><p style="padding-left: 9pt;text-indent: 0pt;text-align: left;"><a name="bookmark37"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 11pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.04.2: <span class="p">Code, math and visuals behind a layer of neurons.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/mxo" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.04: </a><a href="https://nnfs.io/mxo" class="s9" target="_blank">https://nnfs.io/mxo</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this code, we have three sets of weights and three biases, which define three neurons. Each neuron is “connected” to the same inputs. The difference is in the separate weights and bias that each neuron applies to the input. This is called a <b>fully connected </b>neural network — every neuron in the current layer has connections to every neuron from the previous layer. This is</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">a very common type of neural network, but it should be noted that there is no requirement to fully connect everything like this. At this point, we have only shown code for a single layer with very few neurons. Imagine coding many more layers and more neurons. This would get very challenging to code using our current methods. Instead, we could use a loop to scale and handle dynamically-sized inputs and layers. We’ve turned the separate weight variables into a list of weights so we can iterate over them, and we changed the code to use loops instead of the hardcoded operations.</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">0.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Output of current layer </span>layer_outputs <span style=" color: #C71F43;">= </span>[]</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;"># For each neuron</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;">for <span style=" color: #231F20;">neuron_weights, neuron_bias </span>in <span style=" color: #32A7BD;">zip</span><span style=" color: #231F20;">(weights, biases): </span><span style=" color: #A5A4A5;"># Zeroed output of given neuron</span></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">neuron_output <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># For each input and weight to the neuron</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 109%;text-align: left;">for <span style=" color: #231F20;">n_input, weight </span>in <span style=" color: #32A7BD;">zip</span><span style=" color: #231F20;">(inputs, neuron_weights): </span><span style=" color: #A5A4A5;"># Multiply this input by associated weight</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># and add to the neuron&#39;s output variable </span>neuron_output <span style=" color: #C71F43;">+= </span>n_input<span style=" color: #C71F43;">*</span>weight</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;"># Add bias</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">neuron_output <span style=" color: #C71F43;">+= </span>neuron_bias</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Put neuron&#39;s result to the layer&#39;s output list <span style=" color: #231F20;">layer_outputs.append(neuron_output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(layer_outputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, <span style=" color: #7358A5;">2.385</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">This does the same thing as before, just in a more dynamic and scalable way. If you find yourself confused at one of the steps, <span class="s12">print</span><span class="s10">() </span>out the objects to see what they are and what’s happening. The <span class="s12">zip</span><span class="s10">() </span>function lets us iterate over multiple iterables (lists in this case) simultaneously. Again, all we’re doing is, for each neuron (the outer loop in the code above, over neuron weights and biases), taking each input value multiplied by the associated weight for that input (the inner loop in the code above, over inputs and weights), adding all of these together, then adding a bias at the end. Finally, sending the neuron’s output to the layer’s output list.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">That’s it! How do we know we have three neurons? Why do we have three? We can tell we have three neurons because there are 3 sets of weights and 3 biases. When you make a neural network of your own, you also get to decide how many neurons you want for each of the layers. You can combine however many inputs you are given with however many neurons that you desire. As you progress through this book, you will gain some intuition of how many neurons to try using. We will start by using trivial numbers of neurons to aid in understanding how neural networks work at their core.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark24">With our above code that uses loops, we could modify our number of inputs or neurons in our layer to be whatever we wanted, and our loop would handle it. As we said earlier, it would be a disservice not to show NumPy here since Python alone doesn’t do matrix/tensor/array math very efficiently. But first, the reason the most popular deep learning library in Python is called “TensorFlow” is that it’s all about doing operations on </a><b>tensors</b>.<a name="bookmark38">&zwnj;</a><a name="bookmark39">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Tensors, Arrays and Vectors</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">What are “tensors?”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Tensors are <i>closely-related to </i>arrays. If you interchange tensor/array/matrix when it comes to machine learning, people probably won’t give you too hard of a time. But there are subtle differences, and they are primarily either the context or attributes of the tensor object. To understand a tensor, let’s compare and describe some of the other data containers in Python</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">(things that hold data). Let’s start with a list. A Python list is defined by comma-separated objects contained in brackets. So far, we’ve been using lists.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">This is an example of a simple list:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">l <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">A list of lists:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">lol <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">3</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">3</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">A list of lists of lists!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">lolol <span style=" color: #C71F43;">= </span>[[[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">3</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">3</span>]],</p><p class="s10" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">4</span>,<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">4</span>]],</p><p class="s10" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">3</span>],</p><p class="s10" style="padding-top: 3pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">9</span>,<span style=" color: #7358A5;">4</span>]]]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark40">Everything shown so far could also be an array or an array representation of a tensor. A list is just a list, and it can do pretty much whatever it wants, including:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 7pt;text-indent: 0pt;text-align: center;">another_list_of_lists <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">4</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">3</span>],</p><p class="s10" style="padding-top: 3pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">[<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">1</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The above list of lists cannot be an array because it is not <b>homologous</b>. A list of lists is homologous if each list along a dimension is identically long, and this must be true for each dimension. In the case of the list shown above, it’s a 2-dimensional list. The first dimension’s length is the number of sublists in the total list (2). The second dimension is the length of each of those sublists (3, then 2). In the above example, when reading across the “row” dimension (also called the second dimension), the first list is 3 elements long, and the second list is 2 elements long — this is not homologous and, therefore, cannot be an array. While failing to be consistent in one dimension is enough to show that this example is not homologous, we could also read down the “column” dimension (the first dimension); the first two columns are 2 elements long while the third column only contains 1 element. Note that every dimension does not necessarily need to be the same length; it is perfectly acceptable to have an array with 4 rows and 3 columns (i.e., 4x3).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A matrix is pretty simple. It’s a rectangular array. It has columns and rows. It is two dimensional. So a matrix can be an array (a 2D array). Can all arrays be matrices? No. An array can be far more than just columns and rows, as it could have four dimensions, twenty dimensions, and so on.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">list_matrix_array <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">4</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">2</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The above list could also be a valid matrix (because of its columns and rows), which automatically means it could also be an array. The “shape” of this array would be 3x2, or more formally described as a shape of <i>(3, 2) </i>as it has 3 rows and 2 columns.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">To denote a shape, we need to check every dimension. As we’ve already learned, a matrix is a</p><p class="s10" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">2-dimensional array. The first dimension is what’s inside the most outer brackets, and if we look at the above matrix, we can see 3 lists there: </span>[<span style=" color: #7358A5;">4</span>,<span style=" color: #7358A5;">2</span>]<span class="p">, </span>[<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">1</span>]<span class="p">, and </span>[<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">2</span>]<span class="p">; thus, the size in this dimension is 3 and each of those lists has to be the same shape to form an array (and matrix in this case). The next dimension’s size is the number of elements inside this more inner pair of brackets, and we see that it’s 2 as all of them contain 2 elements.</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">With 3-dimensional arrays, like in <i>lolol </i>below, we’ll have a 3rd level of brackets:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">lolol <span style=" color: #C71F43;">= </span>[[[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">3</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">3</span>]],</p><p class="s10" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">4</span>,<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">4</span>]],</p><p class="s10" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">3</span>],</p><p class="s10" style="padding-top: 3pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">9</span>,<span style=" color: #7358A5;">4</span>]]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The first level of this array contains 3 matrices:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s18" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">3</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">3</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="text-indent: 0pt;text-align: right;">[[<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">2</span>],</p><p class="s18" style="padding-top: 3pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">4</span>,<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">4</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">[[<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">8</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">3</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">9</span>,<span style=" color: #7358A5;">4</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">That’s what’s inside the most outer brackets and the size of this dimension is then 3. If we look at the first matrix, we can see that it contains 2 lists </span><span class="s19">— </span>[<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">5</span>,<span style=" color: #7358A5;">6</span>,<span style=" color: #7358A5;">2</span>] <span class="p">and </span>[<span style=" color: #7358A5;">3</span>,<span style=" color: #7358A5;">2</span>,<span style=" color: #7358A5;">1</span>,<span style=" color: #7358A5;">3</span>] <span class="p">so the size of this dimension is 2 </span><span class="s19">— </span><span class="p">while each list of this inner matrix includes 4 elements. These 4</span></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">elements make up the 3rd and last dimension of this matrix since there are no more inner brackets. Therefore, the shape of this array is <i>(3, 2, 4) </i>and it’s a 3-dimensional array, since the shape contains 3 dimensions<i>.</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 97pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.05: <span class="p">Example of a 3-dimensional array.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/jps" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.05: </a><a href="https://nnfs.io/jps" class="s9" target="_blank">https://nnfs.io/jps</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;"><a name="bookmark41">Finally, what’s a tensor? When it comes to the discussion of tensors versus arrays in the context of computer science, pages and pages of debate have ensued. This intense debate appears to be</a></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">caused by the fact that people are arguing from entirely different places. There’s no question that a tensor is not just an array, but the real question is: “What is a tensor, to a computer scientist, in the context of deep learning?” We believe that we can solve the debate in one line:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">A tensor object is an object that can be represented as an array.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">What this means is, as programmers, we can (and will) treat tensors as arrays in the context of deep learning, and that’s really all the thought we have to put into it. Are all tensors <i>just </i>arrays? No, but they are represented as arrays in our code, so, to us, they’re only arrays, and this is why there’s so much argument and confusion.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, what is an array? In this book, we define an array as an ordered homologous container for numbers, and mostly use this term when working with the NumPy package since that’s what the main data structure is called within it. A linear array, also called a 1-dimensional array, is the simplest example of an array, and in plain Python, this would be a list. Arrays can also consist of multi-dimensional data, and one of the best-known examples is what we call a matrix in mathematics, which we’ll represent as a 2-dimensional array. Each element of the array can be accessed using a tuple of indices as a key, which means that we can retrieve any array element.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We need to learn one more notion <span style=" color: #212121;">— </span>a vector. Put simply, a vector in math is what we call a list in Python or a 1-dimensional array in NumPy. Of course, lists and NumPy arrays do not have the same properties as a vector, but, just as we can write a matrix as a list of lists in Python, we can also write a vector as a list or an array! Additionally, we’ll look at the vector algebraically (mathematically) as a set of numbers in brackets. This is in contrast to the physics perspective,</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">where the vector’s representation is usually seen as an arrow, characterized by a magnitude and a direction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark25">Dot Product and Vector Addition</a><a name="bookmark42">&zwnj;</a><a name="bookmark43">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s now address vector multiplication, as that’s one of the most important operations we’ll perform on vectors. We can achieve the same result as in our pure Python implementation of multiplying each element in our inputs and weights vectors element-wise by using a <b>dot product</b>, which we’ll explain shortly<i>. </i>Traditionally, we use dot products for <b>vectors </b>(yet another name for a container), and we can certainly refer to what we’re doing here as working with vectors just as we can call them “tensors.” Nevertheless, this seems to add to the mysticism of neural networks</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 123%;text-align: left;">— like they’re these objects out in a complex multi-dimensional vector space that we’ll never understand. Keep thinking of vectors as arrays <span style=" color: #212121;">— </span>a 1-dimensional array is just a vector (or a list in Python).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Because of the sheer number of variables and interconnections made, we can model very complex and non-linear relationships with non-linear activation functions, and truly feel like wizards, but this might do more harm than good. Yes, we will be using the “dot product,” but we’re doing this because it results in a clean way to perform the necessary calculations. It’s nothing more in-depth than that — as you’ve already seen, we can do this math with far more rudimentary-sounding words. When multiplying vectors, you either perform a dot product or a cross product. A cross product results in a vector while a dot product results in a scalar (a single value/number).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">First, let’s explain what a dot product of two vectors is. Mathematicians would say:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A dot product of two vectors is a sum of products of consecutive vector elements. Both vectors must be of the same size (have an equal number of elements).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Let’s write out how a dot product is calculated in Python. For it, you have two vectors, which we can represent as lists in Python. We then multiply their elements from the same index values and then add all of the resulting products. Say we have two lists acting as our vectors:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">a <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark44">To obtain the dot product:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">dot_product <span style=" color: #C71F43;">= </span>a[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>b[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+ </span>a[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>b[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>a[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>b[<span style=" color: #7358A5;">2</span>] <span style=" color: #32A7BD;">print</span>(dot_product)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">20</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 98pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.06: <span class="p">Math behind the dot product example.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/xpo" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.06: </a><a href="https://nnfs.io/xpo" class="s9" target="_blank">https://nnfs.io/xpo</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, what if we called <i>a </i>“inputs” and <i>b </i>“weights?” Suddenly, this dot product looks like a succinct way to perform the operations we need and have already performed in plain Python. We need to multiply our weights and inputs of the same index values and add the resulting values together. The dot product performs this exact type of operation; thus, it makes lots of sense to use here. Returning to the neural network code, let’s make use of this dot product. Plain Python does not contain methods or functions to perform such an operation, so we’ll use the NumPy package, which is capable of this, and many more operations that we’ll use in the future.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll also need to perform a vector addition operation in the not-too-distant future. Fortunately, NumPy lets us perform this in a natural way — using the plus sign with the variables containing vectors of the data. The addition of the two vectors is an operation performed element-wise, which means that both vectors have to be of the same size, and the result will become a vector of this</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark26">size as well. The result is a vector calculated as a sum of the consecutive vector elements:</a><a name="bookmark45">&zwnj;</a><a name="bookmark46">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">A Single Neuron with NumPy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s code the solution, for a single neuron to start, using the dot product and the addition of the vectors with NumPy. This makes the code much simpler to read and write (and faster to run):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>, <span style=" color: #7358A5;">2.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1.0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">bias <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 217%;text-align: left;">outputs <span style=" color: #C71F43;">= </span>np.dot(weights, inputs) <span style=" color: #C71F43;">+ </span>bias <span style=" color: #32A7BD;">print</span>(outputs)</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">4.8</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">Fig 2.07: <span class="p">Visualizing the math of the dot product of inputs and weights for a single neuron.</span></h3><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><a name="bookmark47"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.08: <span class="p">Visualizing the math summing the dot product and bias.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 192pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/blq" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.07-2.08: </a><a href="https://nnfs.io/blq" class="s9" target="_blank">https://nnfs.io/blq</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">A Layer of Neurons with NumPy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we’re back to the point where we’d like to calculate the output of a layer of 3 neurons, which means the weights will be a matrix or list of weight vectors. In plain Python, we wrote this as a list of lists. With NumPy, this will be a 2-dimensional array, which we’ll call a matrix. Previously with the 3-neuron example, we performed a multiplication of those weights with a list containing inputs, which resulted in a list of output values <span style=" color: #212121;">— </span>one per each neuron.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We also described the dot product of two vectors, but the weights are now a matrix, and we need to perform a dot product of them and the input vector. NumPy makes this very easy for us <span style=" color: #212121;">— </span>treating this matrix as a list of vectors and performing the dot product one by one with the vector of inputs, returning a list of dot products.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;"><a name="bookmark48">The dot product’s result, in our case, is a vector (or a list) of sums of the weight and bias products for each of the neurons. From here, we still need to add corresponding biases to them. The biases can be easily added to the result of the dot product operation as they are a vector of the same size. We can also use the plain Python list directly here, as NumPy will convert it to an array internally.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Previously, we had calculated outputs of each neuron by performing a dot product and adding a bias, one by one. Now we have changed the order of those operations <span style=" color: #212121;">— </span>we’re performing dot product first as one operation on all neurons and inputs, and then we are adding a bias in the next operation. When we add two vectors using NumPy, each i-th element is added together, resulting in a new vector of the same size. This is both a simplification and an optimization, giving us simpler and faster code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>, <span style=" color: #7358A5;">2.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>, <span style=" color: #7358A5;">0.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 217%;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>np.dot(weights, inputs) <span style=" color: #C71F43;">+ </span>biases <span style=" color: #32A7BD;">print</span>(layer_outputs)</p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([<span style=" color: #7358A5;">4.8 1.21 2.385</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 90pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.09: <span class="p">Code and visuals for the dot product applied to the layer of neurons.</span></h3><p style="padding-left: 90pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 11pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.10: <span class="p">Code and visuals for the sum of the dot product and bias with a layer of neurons.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/cyx" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.09-2.10: </a><a href="https://nnfs.io/cyx" class="s9" target="_blank">https://nnfs.io/cyx</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This syntax involving the dot product of weights and inputs followed by the vector addition of bias is the most commonly used way to represent this calculation of <i>inputs·weights+bias</i>. To explain the order of parameters we are passing into <i>np.dot()</i>, we should think of it as whatever comes first will decide the output shape. In our case, we are passing a list of neuron weights first and then the inputs, as our goal is to get a list of neuron outputs. As we mentioned, a dot product of a matrix and a vector results in a list of dot products. The <i>np.dot() </i>method treats the matrix as a list of vectors and performs a dot product of each of those vectors with the other vector. In this example, we used that property to pass a matrix, which was a list of neuron weight vectors and a vector of inputs and get a list of dot products <span style=" color: #212121;">— </span>neuron outputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark27">A Batch of Data</a><a name="bookmark49">&zwnj;</a><a name="bookmark50">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 130%;text-align: left;">To train, neural networks tend to receive data in <b>batches</b><i>. </i>So far, the example input data have been only one sample (or <b>observation</b>) of various features called a feature set:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;"><span class="p">Here, the </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>] <span class="p">data are somehow meaningful and descriptive to the output we desire. Imagine each number as a value from a different sensor, from the example in chapter 1, all simultaneously. Each of these values is a feature observation datum, and together they form a </span><span class="h3">feature set instance</span><span class="p">, also called an </span><span class="h3">observation</span><span class="p">, or most commonly, a </span><span class="h3">sample</span><span class="p">.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 75pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.11: <span class="p">Visualizing a 1D array.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/lqw" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.11: </a><a href="https://nnfs.io/lqw" class="s9" target="_blank">https://nnfs.io/lqw</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Often, neural networks expect to take in many <b>samples </b>at a time for two reasons. One reason is that it’s faster to train in batches in parallel processing, and the other reason is that batches</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark51">help with generalization during training. If you fit (perform a step of a training process) on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than slowly producing general tweaks to weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases. For the concept of fitment in batches rather than one sample at a time, the following animation can help:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 122pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: center;">Fig 2.12: <span class="p">Example of a linear equation fitting batches of 32 chosen samples. See animation below for other sizes of samples at a time to see how much of a difference batch size can make.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/vyu" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.12: </a><a href="https://nnfs.io/vyu" class="s9" target="_blank">https://nnfs.io/vyu</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">An example of a batch of data could look like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 109pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.13: <span class="p">Example of a batch, its shape, and type.</span></h3><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/lqw" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.13: </a><a href="https://nnfs.io/lqw" class="s9" target="_blank">https://nnfs.io/lqw</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Recall that in Python, and in our case, lists are useful containers for holding a sample as well as multiple samples that make up a batch of observations. Such an example of a batch of observations, each with its own sample, looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>], [<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], [<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>, <span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.8</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">This list of lists could be made into an array since it is homologous. Note that each “list” in this larger list is a sample representing a feature set. </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>]<span class="p">, </span>[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>]<span class="p">, and</span></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>, <span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.8</span>] <span class="p">are all </span><span class="h3">samples</span><span class="p">, and are also referred to as </span><span class="h3">feature set instances </span><span class="p">or</span></p><h3 style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">observations<span class="p">.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have a matrix of inputs and a matrix of weights now, and we need to perform the dot product on them somehow, but how and what will the result be? Similarly, as we performed a dot product on a matrix and a vector, we treated the matrix as a list of vectors, resulting in a list of dot products. In this example, we need to manage both matrices as lists of vectors and perform dot products on all of them in all combinations, resulting in a list of lists of outputs, or a matrix; this operation is called the <b>matrix product</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a name="bookmark28">Matrix Product</a><a name="bookmark52">&zwnj;</a><a name="bookmark53">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">The <b>matrix product </b>is an operation in which we have 2 matrices, and we are performing dot products of all combinations of rows from the first matrix and the columns of the 2nd matrix, resulting in a matrix of those atomic <b>dot products</b>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 35pt;text-indent: 0pt;line-height: 130%;text-align: center;">Fig 2.14: <span class="p">Visualizing how a single element in the resulting matrix from matrix product is calculated. See animation for the full calculation of each element.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/jei" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.14: </a><a href="https://nnfs.io/jei" class="s9" target="_blank">https://nnfs.io/jei</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To perform a matrix product, the size of the second dimension of the left matrix must match the size of the first dimension of the right matrix. For example, if the left matrix has a shape of <i>(5, 4) </i>then the right matrix must match this 4 within the first shape value <i>(4, 7)</i>. The shape of the</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">resulting array is always the first dimension of the left array and the second dimension of the right array, <i>(5, 7)</i>. In the above example, the left matrix has a shape of <i>(5, 4)</i>, and the upper-right matrix has a shape of <i>(4, 5)</i>. The second dimension of the left array and the first dimension of the second array are both <i>4</i>, they match, and the resulting array has a shape of <i>(5, 5)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To elaborate, we can also show that we can perform the matrix product on vectors. In mathematics, we can have something called a column vector and row vector, which we’ll explain better shortly. They’re vectors, but represented as matrices with one of the dimensions having a size of 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">a <span class="p">is a row vector. It looks very similar to a vector </span>a <span class="p">(with an arrow above it) described earlier along with the vector product. The difference in notation between a row vector and vector are commas between values and the arrow above symbol </span>a <span class="p">is missing on a row vector. It’s called a row vector as it’s a vector of a row of a matrix. </span>b<span class="p">, on the other hand, is called a column vector because it’s a column of a matrix. As row and column vectors are technically matrices, we do not denote them with vector arrows anymore.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">When we perform the matrix product on them, the result becomes a matrix as well, like in the previous example, but containing just a single value, the same value as in the dot product example we have discussed previously:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-left: 186pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.15: <span class="p">Product of row and column vectors.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bkw" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.15: </a><a href="https://nnfs.io/bkw" class="s9" target="_blank">https://nnfs.io/bkw</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">In other words, row and column vectors are matrices with one of their dimensions being of a size of 1; and, we perform the <b>matrix product </b>on them instead of the <b>dot product</b>, which results in a matrix containing a single value. In this case, we performed a matrix multiplication of matrices with shapes <i>(1, 3) </i>and <i>(3, 1)</i>, then the resulting array has the shape <i>(1, 1) </i>or a size of <i>1x1</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark29">Transposition for the Matrix Product</a><a name="bookmark54">&zwnj;</a><a name="bookmark55">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">How did we suddenly go from 2 vectors to row and column vectors? We used the relation of the dot product and matrix product saying that a dot product of two vectors equals a matrix product of a row and column vector (the arrows above the letters signify that they are vectors):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We also have temporarily used some simplification, not showing that column vector <i>b </i>is actually a <b>transposed </b>vector <i>b</i>. The proper equation, matching the dot product of vectors <i>a </i>and <i>b </i>written as matrix product should look like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Here we introduced one more new operation <span style=" color: #212121;">— </span><b>transposition</b>. Transposition simply modifies a matrix in a way that its rows become columns and columns become rows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 98pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 8pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.16: <span class="p">Example of an array transposition.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/qut" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.16: </a><a href="https://nnfs.io/qut" class="s9" target="_blank">https://nnfs.io/qut</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.17: <span class="p">Another example of an array transposition.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/pnq" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.17: </a><a href="https://nnfs.io/pnq" class="s9" target="_blank">https://nnfs.io/pnq</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we need to get back to row and column vector definitions and update them with what we have just learned.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">A row vector is a matrix whose first dimension’s size (the number of rows) equals 1 and the second dimension’s size (the number of columns) equals <i>n </i>— the vector size. In other words, it’s a 1×n array or array of shape (1, n):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">With NumPy and with 3 values, we would define it as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Note the use of double brackets here. To transform a list into a matrix containing a single row (perform an equivalent operation of turning a vector into row vector), we can put it into a list and create numpy array:</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">a <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p class="s12" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(np.array([a]))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Again, note that we encase <span class="s20">a </span>in brackets before converting to an array in this case.</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Or we can turn it into a 1D array and expand dimensions using one of the NumPy abilities:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">a <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.expand_dims(np.array(a), <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Where <i>np.expand_dims() </i>adds a new dimension at the index of the <i>axis</i>.</p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A column vector is a matrix where the second dimension’s size equals 1, in other words, it’s an array of shape (n, 1):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With NumPy it can be created the same way as a row vector, but needs to be additionally transposed <span style=" color: #212121;">— </span>transposition turns rows into columns and columns into rows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">To turn vector <i>b </i>into row vector <i>b</i>, we’ll use the same method that we used to turn vector <i>a </i>into row vector <i>a</i>, then we can perform a transposition on it to make it a column vector <i>b</i>:</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">With NumPy code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np a <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">a <span style=" color: #C71F43;">= </span>np.array([a])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([b]).T</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(np.dot(a, b))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">array([[<span style=" color: #7358A5;">20</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have achieved the same result as the dot product of two vectors, but performed on matrices and returning a matrix <span style=" color: #212121;">— </span>exactly what we expected and wanted. It’s worth mentioning that NumPy does not have a dedicated method for performing matrix product <span style=" color: #212121;">— </span>the dot product and matrix product are both implemented in a single method: <i>np.dot()</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we can see, to perform a matrix product on two vectors, we took one as is, transforming it into a row vector, and the second one using transposition on it to turn it into a column vector. That allowed us to perform a matrix product that returned a matrix containing a single value. We also performed the matrix product on two example arrays to learn how a matrix product works <span style=" color: #212121;">— </span>it creates a matrix of dot products of all combinations of row and column vectors.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark30">A Layer of Neurons &amp; Batch of Data w/ NumPy</a><a name="bookmark56">&zwnj;</a><a name="bookmark57">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s get back to our inputs and weights <span style=" color: #212121;">— </span>when covering them, we mentioned that we need to perform dot products on all of the vectors that consist of both input and weight matrices. As we have just learned, that’s the operation that the matrix product performs. We just need to perform transposition on its second argument, which is the weights matrix in our case, to turn the row vectors it currently consists of into column vectors.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Initially, we were able to perform the dot product on the inputs and the weights without a transposition because the weights were a matrix, but the inputs were just a vector. In this case, the dot product results in a vector of atomic dot products performed on each row from the matrix and this single vector. When inputs become a batch of inputs (a matrix), we need to perform the matrix product. It takes all of the combinations of rows from the left matrix and columns from the right matrix, performing the dot product on them and placing the results in an output array. Both arrays have the same shape, but, to perform the matrix product, the shape’s value from the index 1 of the first matrix and the index 0 of the second matrix must match — they don’t right now.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 53pt;text-indent: 0pt;text-align: left;">Fig 2.18: <span class="p">Depiction of why we need to transpose to perform the matrix product.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">If we transpose the second array, values of its shape swap their positions.</p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><a name="bookmark58"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 2.19: <span class="p">After transposition, we can perform the matrix product.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/crq" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.18-2.19: </a><a href="https://nnfs.io/crq" class="s9" target="_blank">https://nnfs.io/crq</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">If we look at this from the perspective of the input and weights, we need to perform the dot product of each input and each weight set in all of their combinations. The dot product takes the row from the first array and the column from the second one, but currently the data in both arrays are row-aligned. Transposing the second array shapes the data to be column-aligned. The matrix product of inputs and transposed weights will result in a matrix containing all atomic dot products that we need to calculate. The resulting matrix consists of outputs of all neurons after operations performed on each input sample:</p><p style="padding-left: 50pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.20: <span class="p">Code and visuals depicting the dot product of inputs and transposed weights.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/gjw" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.20: </a><a href="https://nnfs.io/gjw" class="s9" target="_blank">https://nnfs.io/gjw</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We mentioned that the second argument for <i>np.dot() </i>is going to be our transposed weights, so first will be inputs, but previously weights were the first parameter. We changed that here. Before, we were modeling neuron output using a single sample of data, a vector, but now we are a step forward when we model layer behavior on a batch of data. We could retain the current parameter order, but, as we’ll soon learn, it’s more useful to have a result consisting of a list of layer outputs per each sample than a list of neurons and their outputs sample-wise. We want the resulting</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">array to be sample-related and not neuron-related as we’ll pass those samples further through the network, and the next layer will expect a batch of inputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can code this solution using NumPy now. We can perform <i>np.dot() </i>on a plain Python list of lists as NumPy will convert them to matrices internally. We are converting weights ourselves though to perform transposition operation first, <i>.T </i>in the code, as plain Python list of lists does not support it. Speaking of biases, we do not need to make it a NumPy array for the same reason <span style=" color: #212121;">— </span>NumPy is going to do that internally.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark59">Biases are a list, though, so they are a 1D array as a NumPy array. The addition of this bias vector to a matrix (of the dot products in this case) works similarly to the dot product of a matrix and vector that we described earlier; The bias vector will be added to each row vector of the matrix.</a></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since each column of the matrix product result is an output of one neuron, and the vector is going to be added to each row vector, the first bias is going to be added to each first element of those vectors, second to second, etc. That’s what we need <span style=" color: #212121;">— </span>the bias of each neuron needs to be added to all of the results of this neuron performed on all input vectors (samples).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 2.21: <span class="p">Code and visuals for inputs mulitplied by the weights, plus the bias.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/qty" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 2.21: </a><a href="https://nnfs.io/qty" class="s9" target="_blank">https://nnfs.io/qty</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Now we can implement what we have learned into code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:99pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">1.0</span>,</p></td><td style="width:124pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">2.0<span style=" color: #231F20;">, </span>3.0<span style=" color: #231F20;">, </span>2.5<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:99pt"><p class="s24" style="padding-right: 5pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">2.0</span>,</p></td><td style="width:124pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">5.0<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>1.0<span style=" color: #231F20;">, </span>2.0<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:99pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>,</p></td><td style="width:124pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.7<span style=" color: #231F20;">, </span>3.3<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.8<span style=" color: #231F20;">]]</span></p></td></tr><tr style="height:14pt"><td style="width:99pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.2</span>,</p></td><td style="width:124pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.8<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.5<span style=" color: #231F20;">, </span>1.0<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:99pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.5</span>,</p></td><td style="width:124pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.91<span style=" color: #231F20;">, </span>0.26<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.5<span style=" color: #231F20;">],</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>, <span style=" color: #7358A5;">0.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-bottom: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 217%;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>np.dot(inputs, np.array(weights).T) <span style=" color: #C71F43;">+ </span>biases <span style=" color: #32A7BD;">print</span>(layer_outputs)</p><table style="border-collapse:collapse;margin-left:17.533pt" cellspacing="0"><tr style="height:13pt"><td style="width:54pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:138pt" colspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:54pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">array([[</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;padding-right: 10pt;text-indent: 0pt;line-height: 12pt;text-align: center;">4.8</p></td><td style="width:49pt"><p class="s26" style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.21</p></td><td style="width:56pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.385<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:54pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;padding-right: 11pt;text-indent: 0pt;line-height: 12pt;text-align: center;">8.9</p></td><td style="width:49pt"><p class="s25" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">1.81</span></p></td><td style="width:56pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.2 <span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:54pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;padding-right: 4pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1.41</p></td><td style="width:49pt"><p class="s26" style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.051</p></td><td style="width:56pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.026<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, our neural network takes in a group of samples (inputs) and outputs a group of predictions. If you’ve used any of the deep learning libraries, this is why you pass in a list of inputs (even if it’s just one feature set) and are returned a list of predictions, even if there’s only one prediction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch2" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch2<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark60">Chapter 3</a><a name="bookmark64">&zwnj;</a><a name="bookmark65">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Adding Layers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 31pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The neural network we’ve built is becoming more respectable, but at the moment, we have only one layer. Neural networks become “deep” when they have 2 or more <b>hidden layers</b>. At the moment, we have just one layer, which is effectively an output layer. Why we want two or more <b>hidden </b>layers will become apparent in a later chapter. Currently, we have no hidden layers. A hidden layer isn’t an input or output layer; as the scientist, you see data as they are handed to the input layer and the resulting data from the output layer. Layers between these endpoints have values that we don’t necessarily deal with, hence the name “hidden.” Don’t let this name convince you that you can’t access these values, though. You will often use them to diagnose issues or improve your neural network. To explore this concept, let’s add another layer to this neural network, and, for now, let’s assume these two layers that we’re going to have will be the hidden layers, and we just have not coded our output layer yet.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Before we add another layer, let’s think about what will be coming. In the case of the first layer, we can see that we have an input with 4 features.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 204pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 3.01: <span class="p">Input layer with 4 features into a hidden layer with 3 neurons.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Samples (feature set data) get fed through the input, which does not change it in any way, to our first hidden layer, which we can see has 3 sets of weights, with 4 values each.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Each of those 3 unique weight sets is associated with its distinct neuron. Thus, since we have 3 weight sets, we have 3 neurons in this first hidden layer. Each neuron has a unique set of weights, of which we have 4 (as there are 4 inputs to this layer), which is why our initial weights have a shape of <i>(3,4)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, we wish to add another layer. To do that, we must make sure that the expected input to that layer matches the previous layer’s output. We have set the number of neurons in a layer by setting how many weight sets and biases we have. The previous layer’s influence on weight sets</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we’re talking the input). The previous layer has 3 weight sets and 3 biases, so we know it has 3 neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have 3 discrete weights.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">To create this new layer, we are going to copy and paste our <span class="s22">weights </span>and <span class="s22">biases </span>to <span class="s22">weights2</span></p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">and <span class="s22">biases2</span>, and change their values to new made up sets. Here’s an example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2.</span>, <span style=" color: #7358A5;">5.</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>, <span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.8</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">0.5</span>]</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights2 <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.14</span>, <span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.12</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.33</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.44</span>, <span style=" color: #7358A5;">0.73</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.13</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases2 <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Next, we will now call <i>outputs </i>“<i>layer1_ouputs</i>”<i>:</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">layer1_outputs <span style=" color: #C71F43;">= </span>np.dot(inputs, np.array(weights).T) <span style=" color: #C71F43;">+ </span>biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">As previously stated, inputs to layers are either inputs from the actual dataset you’re training with or outputs from a previous layer. That’s why we defined 2 versions of <i>weights </i>and <i>biases </i>but only 1 of <i>inputs </i>— because the inputs for layer 2 will be the outputs from the previous layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">layer2_outputs <span style=" color: #C71F43;">= </span>np.dot(layer1_outputs, np.array(weights2).T) <span style=" color: #C71F43;">+ </span>biases2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">All together now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np inputs <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2.</span>, <span style=" color: #7358A5;">5.</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">2</span>],</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:102pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>,</p></td><td style="width:121pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2.7<span style=" color: #231F20;">, </span>3.3<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.8<span style=" color: #231F20;">]]</span></p></td></tr><tr style="height:14pt"><td style="width:102pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">weights <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.2</span>,</p></td><td style="width:121pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.8<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.5<span style=" color: #231F20;">, </span>1<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:102pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.5</span>,</p></td><td style="width:121pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.91<span style=" color: #231F20;">, </span>0.26<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.5<span style=" color: #231F20;">],</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">0.5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">weights2 <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.14</span>, <span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.12</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.33</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.44</span>, <span style=" color: #7358A5;">0.73</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.13</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">biases2 <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">layer1_outputs <span style=" color: #C71F43;">= </span>np.dot(inputs, np.array(weights).T) <span style=" color: #C71F43;">+ </span>biases layer2_outputs <span style=" color: #C71F43;">= </span>np.dot(layer1_outputs, np.array(weights2).T) <span style=" color: #C71F43;">+ </span>biases2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(layer2_outputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.522pt" cellspacing="0"><tr style="height:13pt"><td style="width:102pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:127pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:102pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">array([[ <span style=" color: #7358A5;">0.5031</span></p></td><td style="width:55pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">1.04185</span></p></td><td style="width:72pt"><p class="s25" style="padding-left: 1pt;padding-right: 7pt;text-indent: 0pt;line-height: 12pt;text-align: center;">-<span style=" color: #7358A5;">2.03875</span><span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:102pt"><p class="s24" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[ <span style=" color: #7358A5;">0.2434</span></p></td><td style="width:55pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">2.7332</span></p></td><td style="width:72pt"><p class="s25" style="padding-left: 1pt;padding-right: 7pt;text-indent: 0pt;line-height: 12pt;text-align: center;">-<span style=" color: #7358A5;">5.7633 </span><span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:102pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.99314</span></p></td><td style="width:55pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.41254</p></td><td style="width:72pt"><p class="s25" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">-<span style=" color: #7358A5;">0.35655</span><span style=" color: #231F20;">]])</span></p></td></tr></table><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark61">At this point, our neural network could be visually represented as:</a><a name="bookmark66">&zwnj;</a><a name="bookmark67">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 168pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 3.02: <span class="p">4 features input into 2 hidden layers of 3 neurons each.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Training Data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, rather than hand-typing in random data, we’ll use a function that can create non-linear data. What do we mean by non-linear? Linear data can be fit with or represented by a straight line.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 3.03: <span class="p">Example of data (orange dots) that can be represented (fit) by a straight line (green line).</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Non-linear data cannot be represented well by a straight line.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 91pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 3.04: <span class="p">Example of data (orange dots) that is not well fit by a straight line.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you were to graph data points of the form <i>(x, y) </i>where <i>y = f(x)</i>, and it looks to be a line with a clear trend or slope, then chances are, they’re linear data! Linear data are very easily approximated by far simpler machine learning models than neural networks. What other machine learning algorithms cannot approximate so easily are non-linear datasets. To simplify this, we’ve created a Python package that you can install with pip, called <i>nnfs</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s20" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">pip install nnfs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The nnfs package contains functions that we can use to create data. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The <i>spiral_data </i><a href="https://cs231n.github.io/neural-networks-case-study/" class="s82" target="_blank">function was slightly modified from </a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">https://cs231n.github.io/neural-networks-</span><a href="https://cs231n.github.io/neural-networks-case-study/" style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank"> </a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">case-study/</span>, which is a great supplementary resource for this topic.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">You will typically not be generating training data from a function for your neural networks. You will have an actual dataset. Generating a dataset this way is purely for convenience at this stage. We will also use this package to ensure repeatability for everyone, using <span class="s10">nnfs.init()</span>, after importing NumPy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">nnfs.init()</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">The <span class="s22">nnfs.init() </span>does three things: it sets the random seed to 0 (by the default), creates a <i>float32 </i>dtype default, and overrides the original dot product from NumPy. All of these are meant to ensure repeatable results for following along.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: justify;">The <span class="s22">spiral_data </span>function allows us to create a dataset with as many classes as we want. The function has parameters to choose the number of classes and the number of points/observations per class in the resulting non-linear dataset. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>) plt.scatter(X[:, <span style=" color: #7358A5;">0</span>], X[:, <span style=" color: #7358A5;">1</span>])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 122pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 3.05: <span class="p">Uncolored spiral dataset.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you trace from the center, you can determine all 3 classes separately, but this is a very challenging problem for a machine learning classifier to solve. Adding color to the chart makes this more clear:</p><p class="s22" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 113%;text-align: left;">plt.scatter(X[:, <span style=" color: #7358A5;">0</span>], X[:, <span style=" color: #7358A5;">1</span>], <span class="s29">c</span><span style=" color: #C71F43;">=</span>y, <span class="s29">cmap</span><span style=" color: #C71F43;">=</span><span style=" color: #8F8633;">&#39;brg&#39;</span>) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 121pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 3.06: <span class="p">Spiral dataset colored by class.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Keep in mind that the neural network will not be aware of the color differences as the data have no class encodings. This is only made as an instruction for the reader. In the data above, each dot is the feature, and its coordinates are the samples that form the dataset. The “classification” for that dot has to do with which spiral it is a part of, depicted by blue, green, or red color in the previous image. These colors would then be assigned a class number for the model to fit to, like 0, 1, and 2.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark62">Dense Layer Class</a><a name="bookmark68">&zwnj;</a><a name="bookmark69">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we no longer need to hand-type our data, we should create something similar for our various types of neural network layers. So far, we’ve only used what’s called a <b>dense </b>or <b>fully- connected </b>layer. These layers are commonly referred to as “dense” layers in papers, literature, and code, but you will occasionally see them called fully-connected or “fc” for short in code. Our dense layer class will begin with two methods.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>): <span style=" color: #A5A4A5;"># Initialize weights and biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 13pt;text-align: left;">pass <span style=" color: #A5A4A5;"># using pass statement as a placeholder</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 124%;text-align: left;"># Calculate output values from inputs, weights and biases <span style=" color: #C71F43;">pass </span># using pass statement as a placeholder</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As previously stated, weights are often initialized randomly for a model, but not always. If you wish to load a pre-trained model, you will initialize the parameters to whatever that pre- trained model finished with. It’s also possible that, even for a new model, you have some other</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">initialization rules besides random. For now, we’ll stick with random initialization. Next, we have the <i>forward </i>method. When we pass data through a model from beginning to end, this is called a <b>forward pass</b>. Just like everything else, however, this is not the only way to do things. You can have the data loop back around and do other interesting things. We’ll keep it common and perform a regular forward pass.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">To continue the <span class="s35">Layer_Dense </span>class’ code let’s add the random initialization of weights and biases:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 124%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark70">Here, we’re setting weights to be random and biases to be 0. Note that we’re initializing weights to be </a><i>(inputs, neurons), </i>rather than (<i>neurons, inputs)</i>. We’re doing this ahead instead of</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">transposing every time we perform a forward pass, as explained in the previous chapter. Why zero biases? In specific scenarios, like with many samples containing values of 0, a bias can ensure that a neuron fires initially. It sometimes may be appropriate to initialize the biases to some non-zero number, but the most common initialization for biases is 0. However, in these scenarios, you may find success in doing things another way. This will vary depending on your use-case and is just one of many things you can tweak when trying to improve results. One situation where you might want to try something else is with what’s called <b>dead neurons</b>. We haven’t yet covered activation functions in practice, but imagine our step function again.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 106pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 3.07: <span class="p">Graph of a step function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">It’s possible for <i>weights · inputs + biases </i>not to meet the threshold of the step function, which means the neuron will output a 0. Alone, this is not a big issue, but it becomes a problem if this happens to this neuron for every one of the input samples (it’ll become clear why once we cover backpropagation). So then this neuron’s 0 output is the input to another neuron. Any weight multiplied by zero will be zero. With an increasing number of neurons outputting 0, more inputs to the next neurons will receive these 0s rendering the network essentially non-trainable, or “dead.”</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">Next, let’s explore <span class="s22">np.random.randn </span>and <span class="s22">np.zeros </span>in more detail. These methods are convenient ways to initialize arrays. <span class="s22">np.random.randn </span>produces a Gaussian distribution with a mean of 0 and a variance of 1, which means that it’ll generate random numbers, positive and negative, centered at 0 and with the mean value close to 0. In general, neural networks work best with values between -1 and +1, which we’ll discuss later in an upcoming chapter. So this <span class="s22">np.random.randn </span>generates values around those numbers. We’re going to multiply this</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Gaussian distribution for the weights by <i>0.01 </i>to generate numbers that are a couple of magnitudes smaller. Otherwise, the model will take more time to fit the data during the training process</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 14pt;text-align: left;">as starting values will be disproportionately large compared to the updates being made during</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">training. The idea here is to start a model with non-zero values small enough that they won’t affect training. This way, we have a bunch of values to begin working with, but hopefully none too large or as zeros. You can experiment with values other than <i>0.01 </i>if you like.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">Finally, the <span class="s22">np.random.randn </span>function takes dimension sizes as parameters and creates the output array with this shape. The weights here will be the number of inputs for the first dimension and the number of neurons for the 2nd dimension. This is similar to our previous made up array of weights, just randomly generated. Whenever there’s a function or block of code that you’re not sure about, you can always print it out. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 217%;text-align: left;">nnfs.init() <span style=" color: #32A7BD;">print</span>(np.random.randn(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">5</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:84pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:302pt" colspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:15pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[[ <span style=" color: #7358A5;">1.7640524</span></p></td><td style="width:142pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.4001572 0.978738</p></td><td style="width:79pt"><p class="s26" style="padding-left: 5pt;padding-right: 4pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2.2408931</p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.867558 <span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.9772779</span></p></td><td style="width:142pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.95008844 <span style=" color: #C71F43;">-</span>0.1513572</p></td><td style="width:79pt"><p class="s25" style="padding-left: 5pt;padding-right: 5pt;text-indent: 0pt;line-height: 12pt;text-align: center;">-<span style=" color: #7358A5;">0.10321885</span></p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.41059852<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The example function call has returned a 2x5 array (which we can also say is “<i>with a shape of (2,5)</i>”) with data randomly sampled from a Gaussian distribution with a mean of 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">Next, the <span class="s22">np.zeros </span>function takes a desired array shape as an argument and returns an array of that shape filled with zeros.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.zeros((<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">5</span>)))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">0. 0. 0. 0. 0.</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0. 0. 0. 0. 0.</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll initialize the biases with the shape of <i>(1, n_neurons)</i>, as a row vector, which will let us easily add it to the result of the dot product later, without additional operations like transposition.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">To see an example of how our method initializes weights and biases:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;">nnfs.init() n_inputs <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 11pt;text-align: left;">n_neurons <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">4</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">print<span style=" color: #231F20;">(weights) </span>print<span style=" color: #231F20;">(biases)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[[ <span style=" color: #7358A5;">0.01764052 0.00400157 0.00978738 0.02240893</span>]</p><p class="s14" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">[ </span>0.01867558 <span style=" color: #C71F43;">-</span>0.00977278 0.00950088 <span style=" color: #C71F43;">-</span>0.00151357<span style=" color: #231F20;">]]</span></p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">0. 0. 0. 0.</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">On to our forward method — we need to update it with the dot product+biases calculation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Nothing new here, just turning the previous code into a method. Our full <i>Layer_Dense </i>class so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Dense layer <span style=" color: #32A7BD;">class </span><span style=" color: #427F3C;">Layer_Dense</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">def</span><span class="s36"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span style=" color: #CA6628;">self</span>, <span style=" color: #CA6628;">n_inputs</span>, <span style=" color: #CA6628;">n_neurons</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">def </span><span style=" color: #427F3C;">forward</span>(<span style=" color: #CA6628;">self</span>, <span style=" color: #CA6628;">inputs</span>):</p><p class="s10" style="padding-top: 3pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark63">We’re ready to make use of this new class instead of hardcoded calculations, so let’s generate some data using discussed dataset creation method and use our new layer to perform a forward pass:</a><a name="bookmark71">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(dense1.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Go ahead and run everything.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full code up to this point:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 217%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>): <span style=" color: #A5A4A5;"># Initialize weights and biases</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(dense1.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 26pt;text-indent: -6pt;line-height: 109%;text-align: justify;"><span style=" color: #231F20;">[[ </span>0.0000000e+00 0.0000000e+00 0.0000000e+00<span style=" color: #231F20;">] [</span><span style=" color: #C71F43;">-</span>1.0475188e-04 1.1395361e-04 <span style=" color: #C71F43;">-</span>4.7983500e-05<span style=" color: #231F20;">] [</span><span style=" color: #C71F43;">-</span>2.7414842e-04 3.1729150e-04 <span style=" color: #C71F43;">-</span>8.6921798e-05<span style=" color: #231F20;">] [</span><span style=" color: #C71F43;">-</span>4.2188365e-04 5.2666257e-04 <span style=" color: #C71F43;">-</span>5.5912682e-05<span style=" color: #231F20;">] [</span><span style=" color: #C71F43;">-</span>5.7707680e-04 7.1401405e-04 <span style=" color: #C71F43;">-</span>8.9430439e-05<span style=" color: #231F20;">]]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the output, you can see we have 5 rows of data that have 3 values each. Each of those 3 values is the value from the 3 neurons in the <i>dense1 </i>layer after passing in each of the samples. Great!</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have a network of neurons, so our neural network model is almost deserving of its name, but we’re still missing the activation functions, so let’s do those next!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch3" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch3<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark72">Chapter 4</a><a name="bookmark84">&zwnj;</a><a name="bookmark85">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Activation Functions</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 31pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this chapter, we will tackle a few of the activation functions and discuss their roles. We use different activation functions for different cases, and understanding how they work can help you properly pick which of them is best for your task. The activation function is applied to the output of a neuron (or layer of neurons), which modifies outputs. We use activation functions because if the activation function itself is nonlinear, it allows for neural networks with usually two or more hidden layers to map nonlinear functions. We’ll be showing how this works in this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In general, your neural network will have two types of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer. Usually, the activation function used for hidden neurons will be the same for all of them, but it doesn’t have to.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark73">The Step Activation Function</a><a name="bookmark86">&zwnj;</a><a name="bookmark87">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Recall the purpose this activation function serves is to mimic a neuron “firing” or “not firing” based on input information. The simplest version of this is a step function. In a single neuron, if the <i>weights · inputs + bias </i>results in a value greater than 0, the neuron will fire and output a 1; otherwise, it will output a 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 71pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.01: <span class="p">Step function graph.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This activation function has been used historically in hidden layers, but nowadays, it is rarely a choice.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark74">The Linear Activation Function</a><a name="bookmark88">&zwnj;</a><a name="bookmark89">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A linear function is simply the equation of a line. It will appear as a straight line when graphed, where y=x and the output value equals the input.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 102pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.02: <span class="p">Linear function graph.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This activation function is usually applied to the last layer’s output in the case of a regression model — a model that outputs a scalar value instead of a classification. We’ll cover regression in chapter 17 and soon in an example in this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark75">The Sigmoid Activation Function</a><a name="bookmark90">&zwnj;</a><a name="bookmark91">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The problem with the step function is it’s not very informative. When we get to training and network optimizers, you will see that the way an optimizer works is by assessing individual impacts that weights and biases have on a network’s output. The problem with a step function is that it’s less clear to the optimizer what these impacts are because there’s very little information gathered from this function. It’s either on (1) or off (0). It’s hard to tell how “close” this step function was to activating or deactivating. Maybe it was very close, or maybe it was very far. In terms of the final output value from the network, it doesn’t matter if it was <i>close </i>to outputting something else. Thus, when it comes time to optimize weights and biases, it’s easier for the optimizer if we have activation functions that are more granular and informative.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The original, more granular, activation function used for neural networks was the <b>Sigmoid</b></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">activation function, which looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 80pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.03: <span class="p">Sigmoid function graph.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark76">This function returns a value in the range of 0 for negative infinity, through 0.5 for the input of 0, and to 1 for positive infinity. We’ll talk about this function more in chapter 16.</a><a name="bookmark92">&zwnj;</a><a name="bookmark93">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As mentioned earlier, with “dead neurons,” it’s usually better to have a more granular approach for the hidden neuron activation functions. In this case, we’re getting a value that can be reversed to its original value; the returned value contains all the information from the input, contrary to</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">a function like the step function, where an input of 3 will output the same value as an input of 300,000. The output from the Sigmoid function, being in the range of 0 to 1, also works better with neural networks — especially compared to the range of the negative to the positive infinity</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">— and adds nonlinearity. The importance of nonlinearity will become more clear shortly in this chapter. The Sigmoid function, historically used in hidden layers, was eventually replaced by the <b>Rectified Linear Units </b>activation function (or <b>ReLU</b>). That said, we will be using the Sigmoid function as the output layer’s activation function in chapter 16.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The Rectified Linear Activation Function</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 70pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.04: <span class="p">Graph of the ReLU activation function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 16pt;text-indent: 0pt;line-height: 109%;text-align: center;">The rectified linear activation function is simpler than the sigmoid. It’s quite literally <i>y=x</i>, clipped at 0 from the negative side. If <i>x </i>is less than or equal to <i>0</i>, then <i>y </i>is <i>0 </i>— otherwise, <i>y </i>is equal to <i>x</i>.</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark95"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark77">This simple yet powerful activation function is the most widely used activation function at the time of writing for various reasons — mainly speed and efficiency. While the sigmoid activation function isn’t the most complicated, it’s still much more challenging to compute than the ReLU activation function. The ReLU activation function is extremely close to being a linear activation function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective.</a><a name="bookmark94">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Why Use Activation Functions?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we understand what activation functions represent, how some of them look, and what they return, let’s discuss <i>why </i>we use activation functions in the first place. In most cases, for a neural network to fit a nonlinear function, we need it to contain two or more hidden layers, and we need those hidden layers to use a nonlinear activation function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First off, what’s a nonlinear function? A nonlinear function cannot be represented well by a straight line, such as a sine function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 137pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.05: <span class="p">Graph of y=sin(x)</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While there are certainly problems in life that are linear in nature, for example, trying to figure out the cost of some number of shirts, and we know the cost of an individual shirt, and that there are no bulk discounts, then the equation to calculate the price of any number of those products is a linear equation. Other problems in life are not so simple, like the price of a home. The number of factors that come into play, such as size, location, time of year attempting to sell, number of</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">rooms, yard, neighborhood, and so on, makes the pricing of a home a nonlinear equation. Many of the more interesting and hard problems of our time are nonlinear. The main attraction for neural networks has to do with their ability to solve nonlinear problems. First, let’s consider a situation where neurons have no activation function, which would be the same as having an activation function of <i>y=x</i>. With this linear activation function in a neural network with 2 hidden layers of 8 neurons each, the result of training this model will look like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 138pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 224pt;text-indent: -197pt;line-height: 130%;text-align: left;">Fig 4.06: <span class="p">Neural network with linear activation functions in hidden layers attempting to fit y=sin(x)</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">When using the same 2 hidden layers of 8 neurons each with the rectified linear activation function, we see the following result after training:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 138pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">Fig 4.07: <span class="p">ReLU activation functions in hidden layers attempting to fit y=sin(x)</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark78">Linear Activation in the Hidden Layers</a><a name="bookmark96">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that you can see that this is the case, we still should consider <i>why </i>this is the case. To begin, let’s revisit the linear activation function of <i>y=x</i>, and let’s consider this on a singular neuron level. Given values for weights and biases, what will the output be for a neuron with a <i>y=x </i>activation function? Let’s look at some examples — first, let’s try to update the first weight with a positive value:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 4.08: <span class="p">Example of output with a neuron using a linear activation function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">As we continue to tweak with weights, updating with a negative number this time:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 107pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 4.09: <span class="p">Example of output with a neuron using a linear activation function, updated weight.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And updating weights and additionally a bias:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 105pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 227pt;text-indent: -207pt;line-height: 130%;text-align: left;">Fig 4.10: <span class="p">Example of output with a neuron using a linear activation function, updated another weight.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">No matter what we do with this neuron’s weights and biases, the output of this neuron will be perfectly linear to <i>y=x </i>of the activation function. This linear nature will continue throughout the entire network:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 143pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.11: <span class="p">A neural network with all linear activation functions.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">No matter what we do, however many layers we have, this network can only depict linear relationships if we use linear activation functions. It should be fairly obvious that this will be the case as each neuron in each layer acts linearly, so the entire network is a linear function as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark79">ReLU Activation in a Pair of Neurons</a><a name="bookmark97">&zwnj;</a><a name="bookmark98">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We believe it is less obvious how, with a barely nonlinear activation function, like the rectified linear activation function, we can suddenly map nonlinear relationships and functions, so now let’s cover that. Let’s start again with a single neuron. We’ll begin with both a weight of 0 and a bias of 0:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 32pt;text-indent: 0pt;text-align: left;">Fig 4.12: <span class="p">Single neuron with single input (zeroed weight) and ReLU activation function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, no matter what input we pass, the output of this neuron will always be a 0, because the weight is 0, and there’s no bias. Let’s set the weight to be 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 28pt;text-indent: 0pt;text-align: left;">Fig 4.13: <span class="p">Single neuron with single input and ReLU activation function, weight set to 1.0.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now it looks just like the basic rectified linear function, no surprises yet! Now let’s set the bias to 0.50:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 39pt;text-indent: 0pt;text-align: left;">Fig 4.14: <span class="p">Single neuron with single input and ReLU activation function, bias applied.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can see that, in this case, with a single neuron, the bias offsets the overall function’s activation point <i>horizontally</i>. By increasing bias, we’re making this neuron activate earlier. What happens when we negate the weight to -1.0?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 7pt;padding-left: 30pt;text-indent: 0pt;text-align: left;">Fig 4.15: <span class="p">Single neuron with single input and ReLU activation function, negative weight.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">With a negative weight and this single neuron, the function has become a question of when this neuron <i>deactivates</i>. Up to this point, you’ve seen how we can use the bias to offset the function horizontally, and the weight to influence the slope of the activation. Moreover, we’re also able to control whether the function is one for determining where the neuron activates or deactivates.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">What happens when we have, rather than just the one neuron, a pair of neurons? For example, let’s pretend that we have 2 hidden layers of 1 neuron each. Thinking back to the <i>y=x </i>activation function, we unsurprisingly discovered that a linear activation function produced linear results no matter what chain of neurons we made. Let’s see what happens with the rectified linear function for the activation. We’ll begin with the last values for the 1st neuron and a weight of 1, with a bias of 0, for the 2nd neuron:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 4.16: <span class="p">Pair of neurons with single inputs and ReLU activation functions.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we can see so far, there’s no change. This is because the 2nd neuron’s bias is doing no offsetting, and the 2nd neuron’s weight is just multiplying output by 1, so there’s no change. Let’s try to adjust the 2nd neuron’s bias now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Fig 4.17: <span class="p">Pair of neurons with single inputs and ReLU activation functions, other bias applied.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Now we see some fairly interesting behavior. The bias of the second neuron indeed shifted the overall function, but, rather than shifting it <i>horizontally</i>, it shifted the function <i>vertically</i>. What then might happen if we make that 2nd neuron’s weight -2 rather than 1?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 88pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 4.18: <span class="p">Pair of neurons with single inputs and ReLU activation functions, other negative weight.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Something exciting has occurred! What we have here is a neuron that has both an activation and a deactivation point. When <i>both </i>neurons are activated, when their “area of effect” comes into play, they produce values in the range of the granular, variable, and output. If any neuron in the pair is inactive, the pair will produce non-variable output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 29pt;text-indent: 0pt;text-align: left;">Fig 4.19: <span class="p">Pair of neurons with single inputs and ReLU activation functions, area of effect.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark80">ReLU Activation in the Hidden Layers</a><a name="bookmark99">&zwnj;</a><a name="bookmark100">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s now take this concept and use it to fit to the sine wave function using 2 hidden layers of 8 neurons each, and we can hand-tune the values to fit the curve. We’ll do this by working with 1 pair of neurons at a time, which means 1 neuron from each layer individually. For simplicity, we are also going to assume that the layers are not densely connected, and each neuron from the first hidden layer connects to only one neuron from the second hidden layer. That’s usually not the case with the real models, but we want this simplification for the purpose of this demo. Additionally, this example model takes a single value as an input, the input to the sine function, and outputs a single value like the sine function. The output layer uses the Linear activation function, and the hidden layers will use the rectified linear activation function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">To start, we’ll set all weights to 0 and work with the first pair of neurons:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 4.20: <span class="p">Hand-tuning a neural network starting with the first pair of neurons.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, we can set the weight for the hidden layer neurons and the output neuron to 1, and we can see how this impacts the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.21: <span class="p">Adjusting weights for the first/top pair of neurons all to 1.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, we can see that the slope of the overall function is impacted. We can further increase this slope by adjusting the weight for the first neuron of the first layer to 6.0:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.22: <span class="p">Setting weight for first hidden neuron to 6.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can now see, for example, that the initial slope of this function is what we’d like, but we have a problem. Currently, this function never ends because this neuron pair never <i>deactivates</i>. We can visually see where we’d like the deactivation to occur. It’s where the red fitment line (our current neural network’s output) diverges initially from the green sine wave. So now, while we have the correct slope, we need to set this spot as our deactivation point. To do that, we start by increasing</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the bias for the 2nd neuron of the hidden layer pair to 0.70. Recall that this offsets the overall function <i>vertically</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Fig 4.23: <span class="p">Using the bias for the 2nd hidden neuron in the top pair to offset function vertically.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we can set the weight for the 2nd neuron to -1, causing a deactivation point to occur, at least horizontally, where we want it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.24: <span class="p">Setting the weight for the 2nd neuron in the top pair to -1.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we’d like to flip this slope back. How might we flip the output of these two neurons? It seems like we can take the weight of the connection to the output neuron, which is currently a 1.0, and just flip it to a -1, and that flips the function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.25: <span class="p">Setting the weight to the output neuron to -1.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’re certainly getting closer to making this first section fit how we want. Now, all we need to do is offset this up a bit. For this hand-optimized example, we’re going to use the first 7 pairs of neurons in the hidden layers to create the sine wave’s shape, then the bottom pair to offset</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">everything vertically. If we set the bias of the 2nd neuron in the bottom pair to 1.0 and the weight to the output neuron as 0.7, we can vertically shift the line like so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 35pt;text-indent: 0pt;text-align: left;">Fig 4.26: <span class="p">Using the bottom pair of neurons to offset the entire neural network function.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At this point, we have completed the first section with an “area of effect” being the first upward section of the sine wave. We can start on the next section that we wish to do. We can start by setting all weights for this 2nd pair of neurons to 1, including the output neuron:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 205pt;text-indent: -183pt;line-height: 130%;text-align: left;">Fig 4.27: <span class="p">Starting to adjust the 2nd pair of neurons (from the top) for the next segment of the overall function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">At this point, this 2nd pair of neurons’ activation is beginning too soon, which is impacting the “area of effect” of the top pair that we already aligned. To fix this, we want this second pair to start influencing the output where the first pair deactivates, so we want to adjust the function horizontally. As you can recall from earlier, we adjust the first neuron’s bias in this neuron pair to achieve this. Also, to modify the slope, we’ll set the weight coming into that first neuron for the 2nd pair, setting it to 3.5. This is the same method we used to set the slope for the first section, which is controlled by the top pair of neurons in the hidden layer. After these adjustments:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 4.28: <span class="p">Adjusting the weight and bias into the first neuron of the 2nd pair.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We will now use the same methodology as we did with the first pair to set the deactivation point. We set the weight for the 2nd neuron in the hidden layer pair to -1 and the bias to 0.27.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.29: <span class="p">Adjusting the bias of the 2nd neuron in the 2nd pair.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Then we can flip this section’s function, again the same way we did with the first one, by setting the weight to the output neuron from 1.0 to -1.0:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 63pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">Fig 4.30: <span class="p">Flipping the 2nd pair’s function segment, flipping the weight to the output neuron.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And again, just like the first pair, we will use the bottom pair to fix the vertical offset:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">Fig 4.31: <span class="p">Using the bottom pair of neurons to adjust the network’s overall function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We then just continue with this methodology. We’ll leave it flat for the top section, which means we will only begin the activation for the 3rd pair of hidden layer neurons when we wish for the slope to start going down:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.32: <span class="p">Adjusting the 3rd pair of neurons for the next segment.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">This process is simply repeated for each section, giving us a final result:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.33: <span class="p">The completed process (see anim for all values).</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">We can then begin to pass data through to see how these neuron’s areas of effect come into play</p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">— only when both neurons are activated based on input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 67pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.34: <span class="p">Example of data passing through this hand-crafted model.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, given an input of 0.08, we can see the only pairs activated are the top ones, as this is their area of effect. Continuing with another example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 67pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 5pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.35: <span class="p">Example of data passing through this hand-crafted model.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, only the fourth pair of neurons is activated. As you can see, even without any of the other weights, we’ve used some crude properties of a pair of neurons with rectified linear</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">activation functions to fit this sine wave pretty well. If we enable all of the weights now and allow a mathematical optimizer to train, we can see even better fitment:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 78pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 34pt;text-indent: 0pt;text-align: left;">Fig 4.36: <span class="p">Example of fitment after fully-connecting the neurons and using an optimizer.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Animation for the entirety of the concept of ReLU fitment:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/mvp" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 4.12-4.36: </a><a href="https://nnfs.io/mvp" class="s9" target="_blank">https://nnfs.io/mvp</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It should begin to make more sense to you now how more neurons can enable more unique areas of effect, why we need two or more hidden layers, and why we need nonlinear activation functions to map nonlinear problems. For further example, we can take the above example with 2 hidden layers of 8 neurons each, and instead use 64 neurons per hidden layer, seeing the even further continued improvement:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 126pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 4.37: <span class="p">Fitment with 2 hidden layers of 64 neurons each, fully connected, with optimizer.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Anim 4.37: <span class="s16">https://nnfs.io/moo</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark81">ReLU Activation Function Code</a><a name="bookmark101">&zwnj;</a><a name="bookmark102">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Despite the fancy sounding name, the rectified linear activation function is straightforward to code. Most closely to its definition:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">1.1</span>, <span style=" color: #7358A5;">2.2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">100</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>[]</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #231F20;">inputs: </span>if <span style=" color: #231F20;">i </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 109%;text-align: left;">output.append(i) <span style=" color: #C71F43;">else</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 48pt;line-height: 217%;text-align: left;">output.append(<span style=" color: #7358A5;">0</span>) <span style=" color: #32A7BD;">print</span>(output)</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1.1</span>, <span style=" color: #7358A5;">2.2</span>, <span style=" color: #7358A5;">0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We made up a list of values to start. The ReLU in this code is a loop where we’re checking if the current value is greater than 0. If it is, we’re appending it to the output list, and if it’s not, we’re appending 0. This can be written more simply, as we just need to take the largest of two values: 0 or neuron value. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">1.1</span>, <span style=" color: #7358A5;">2.2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">100</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span style=" color: #C71F43;">for </span>i <span style=" color: #C71F43;">in </span>inputs: output.append(<span style=" color: #32A7BD;">max</span>(<span style=" color: #7358A5;">0</span>, i))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1.1</span>, <span style=" color: #7358A5;">2.2</span>, <span style=" color: #7358A5;">0</span>]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">NumPy contains an equivalent — <span class="s10">np.maximum()</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">inputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">1.1</span>, <span style=" color: #7358A5;">2.2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">100</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs) <span style=" color: #32A7BD;">print</span>(output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0. 2. 0. 3.3 0. 1.1 2.2 0. </span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This method compares each element of the input list (or an array) and returns an object of the same shape filled with new values. We will use it in our new rectified linear activation class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from input </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s apply this activation function to the dense layer’s outputs in our code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Forward pass through activation func. # Takes in output from previous layer <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(activation1.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:54pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:169pt" colspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:54pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[[<span style=" color: #7358A5;">0.</span></p></td><td style="width:91pt"><p class="s26" style="padding-left: 27pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.</p></td><td style="width:39pt"><p class="s24" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">]</p></td></tr><tr style="height:14pt"><td style="width:54pt"><p class="s24" style="padding-right: 27pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span></p></td><td style="width:91pt"><p class="s26" style="padding-left: 27pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00011395</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.</p></td><td style="width:39pt"><p class="s24" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">]</p></td></tr><tr style="height:14pt"><td style="width:54pt"><p class="s24" style="padding-right: 27pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span></p></td><td style="width:91pt"><p class="s26" style="padding-left: 27pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00031729</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.</p></td><td style="width:39pt"><p class="s24" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">]</p></td></tr><tr style="height:14pt"><td style="width:54pt"><p class="s24" style="padding-right: 27pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span></p></td><td style="width:91pt"><p class="s26" style="padding-left: 27pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00052666</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.</p></td><td style="width:39pt"><p class="s24" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">]</p></td></tr><tr style="height:13pt"><td style="width:54pt"><p class="s24" style="padding-right: 27pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span></p></td><td style="width:91pt"><p class="s26" style="padding-left: 27pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00071401</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.</p></td><td style="width:39pt"><p class="s24" style="padding-left: 24pt;text-indent: 0pt;line-height: 12pt;text-align: left;">]]</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, negative values have been <b>clipped </b>(modified to be zero). That’s all there is to the rectified linear activation function used in the hidden layer. Let’s talk about the activation function that we are going to use on the output of the last layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark82">The Softmax Activation Function</a><a name="bookmark103">&zwnj;</a><a name="bookmark104">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In our case, we’re looking to get this model to be a classifier, so we want an activation function meant for classification. One of these is the Softmax activation function. First, why are we bothering with another activation function? It just depends on what our overall goals are. In this case, the rectified linear unit is unbounded, not normalized with other units, and exclusive. “Not normalized” implies the values can be anything, an output of <i>[12, 99, 318] </i>is without context, and “exclusive” means each output is independent of the others. To address this lack of context, the softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes. In the case of classification, what we want to see is a prediction of which class the network “thinks” the input represents. This distribution returned by the softmax activation function represents <b>confidence scores </b>for each class and will add up to 1. The predicted class is associated with the output neuron that returned</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the largest confidence score. Still, we can also note the other confidence scores in our overarching algorithm/program that uses this network. For example, if our network has a confidence distribution for two classes: <i>[0.45, 0.55]</i>, the prediction is the 2nd class, but the confidence in</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">this prediction isn’t very high. Maybe our program would not act in this case since it’s not very confident.</p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Here’s the function for the <b>Softmax</b>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">That might look daunting, but we can break it down into simple pieces and express it in Python code, which you may find is more approachable than the formula above. To start, here are example outputs from a neural network layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, <span style=" color: #7358A5;">2.385</span>]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The first step for us is to “exponentiate” the outputs. We do this with Euler’s number, <i>e, </i>which is roughly <i>2.71828182846 </i>and referred to as the “exponential growth” number. Exponentiating is taking this constant to the power of the given parameter:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Both the numerator and the denominator of the Softmax function contain <i>e </i>raised to the power of <i>z</i>, where <i>z</i>, given indices, means a singular output value — the index <i>i </i>means the current sample and the index <i>j </i>means the current output in this sample. The numerator exponentiates the current output value and the denominator takes a sum of all of the exponentiated outputs for a given sample. We need then to calculate these exponentiates to continue:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Values from the previous output when we described # what a neural network is</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, <span style=" color: #7358A5;">2.385</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># e - mathematical constant, we use E here to match a common coding # style where constants are uppercased</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">E <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2.71828182846 </span><span style=" color: #A5A4A5;"># you can also use math.e</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># For each value in a vector, calculate the exponential value </span>exp_values <span style=" color: #C71F43;">= </span>[]</p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">for <span style=" color: #231F20;">output </span>in <span style=" color: #231F20;">layer_outputs:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 24pt;line-height: 109%;text-align: left;">exp_values.append(E <span style=" color: #C71F43;">** </span>output) <span style=" color: #A5A4A5;"># ** - power operator in Python </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;exponentiated values:&#39;</span>)</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">print<span style=" color: #231F20;">(exp_values)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">exponentiated values:</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">121.51041751893969</span>, <span style=" color: #7358A5;">3.3534846525504487</span>, <span style=" color: #7358A5;">10.85906266492961</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;"><span class="p">Exponentiation serves multiple purposes. To calculate the probabilities, we need non-negative values. Imagine the output as </span>[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, -<span style=" color: #7358A5;">2.385</span>] <span class="p">— even after normalization, the last value will still be negative since we’ll just divide all of them by their sum. A negative probability (or confidence) does not make much sense. An exponential value of any number is always non- negative — it returns 0 for negative infinity, 1 for the input of 0, and increases for positive values:</span></p><p style="padding-left: 43pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 4.38: <span class="p">Graph of an exponential function.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The exponential function is a monotonic function. This means that, with higher input values, outputs are also higher, so we won’t change the predicted class after applying it while making sure that we get non-negative values. It also adds stability to the result as the normalized exponentiation is more about the difference between numbers than their magnitudes. Once we’ve exponentiated, we want to convert these numbers to a probability distribution (converting the values into the vector of confidences, one for each class, which add up to 1 for everything in the vector). What that means is that we’re about to perform a normalization where we take a given value and divide it by the sum of all of the values. For our outputs, exponentiated at this stage, that’s what the equation of the Softmax function describes next — to take a given exponentiated value and divide it by the sum of all of the exponentiated values. Since each output value normalizes to a fraction of the sum, all of the values are now in the range of 0 to 1 and add up to 1</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">— they share the probability of 1 between themselves. Let’s add the sum and normalization to the code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Now normalize values</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">norm_base <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>(exp_values) <span style=" color: #A5A4A5;"># We sum all values </span>norm_values <span style=" color: #C71F43;">= </span>[]</p><p class="s11" style="padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;">for <span style=" color: #231F20;">value </span>in <span style=" color: #231F20;">exp_values: norm_values.append(value </span>/ <span style=" color: #231F20;">norm_base)</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Normalized exponentiated values:&#39;</span>) <span style=" color: #32A7BD;">print</span>(norm_values)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Sum of normalized values:&#39;</span>, <span style=" color: #32A7BD;">sum</span>(norm_values))</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Normalized exponentiated values:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.8952826639573506</span>, <span style=" color: #7358A5;">0.024708306782070668</span>, <span style=" color: #7358A5;">0.08000902926057876</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Sum of normalized values: <span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We can perform the same set of operations with the use of NumPy in the following way:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Values from the earlier previous when we described # what a neural network is</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, <span style=" color: #7358A5;">2.385</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># For each value in a vector, calculate the exponential value </span>exp_values <span style=" color: #C71F43;">= </span>np.exp(layer_outputs)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;exponentiated values:&#39;</span>) <span style=" color: #32A7BD;">print</span>(exp_values)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Now normalize values</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">norm_values <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values) <span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;normalized exponentiated values:&#39;</span>) <span style=" color: #32A7BD;">print</span>(norm_values)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;sum of normalized values:&#39;</span>, np.sum(norm_values))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">exponentiated values:</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">121.51041752 3.35348465 10.85906266</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">normalized exponentiated values:</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.89528266 0.02470831 0.08000903</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">sum of normalized values: <span style=" color: #7358A5;">0.9999999999999999</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">Notice the results are similar, but faster to calculate and the code is easier to read with NumPy. We can exponentiate all of the values with a single call of the <span class="s10">np.exp()</span>, then immediately normalize them with the sum. To train in batches, we need to convert this functionality to accept layer outputs in batches. Doing this is as easy as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Get unnormalized probabilities </span>exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">We have some new functions. Specifically, <span class="s10">np.exp() </span>does the <span class="s10">E</span><span class="s11">**</span><span class="s10">output </span>part. We should also address what <i>axis </i>and <i>keepdims </i>mean in the above. Let’s first discuss the <i>axis</i>. Axis is easier</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">to show than tell, but, in a 2D array/matrix, axis 0 refers to the rows, and axis 1 refers to the columns. Let’s see some examples of how <i>axis </i>affects the sum using NumPy. First, we will just show the default, which is <i>None.</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">4.8</span>, <span style=" color: #7358A5;">1.21</span>, <span style=" color: #7358A5;">2.385</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">8.9</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.81</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1.41</span>, <span style=" color: #7358A5;">1.051</span>, <span style=" color: #7358A5;">0.026</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Sum without axis&#39;</span>) <span style=" color: #32A7BD;">print</span>(np.sum(layer_outputs))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;This will be identical to the above since default is None:&#39;</span>) <span style=" color: #32A7BD;">print</span>(np.sum(layer_outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">Sum without axis <span style=" color: #7358A5;">18.172</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">This will be identical to the above since default is None: <span style=" color: #7358A5;">18.172</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With no axis specified, we are just summing all of the values, even if they’re in varying dimensions. Next, <span class="s23">axis</span><span class="s11">=</span><span class="s14">0</span>. This means to sum row-wise, along axis 0. In other words, the output has the same size as this axis, as at each of the positions of this output, the values from all the other dimensions at this position are summed to form it. In the case of our 2D array, where we have only a single other dimension, the columns, the output vector will sum these columns. This means we’ll perform <i>4.8+8.9+1.41 </i>and so on.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Another way to think of it w/ a matrix == axis 0: columns:&#39;</span>) <span style=" color: #32A7BD;">print</span>(np.sum(layer_outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">Another way to think of it w/ a matrix == axis 0: columns: [<span style=" color: #7358A5;">15.11 0.451 2.611</span>]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This isn’t what we want, though. We want sums of the rows. You can probably guess how to do this with NumPy, but we’ll still show the “from scratch” version:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;But we want to sum the rows instead, like this w/ raw py:&#39;</span>) <span style=" color: #C71F43;">for </span>i <span style=" color: #C71F43;">in </span>layer_outputs:</p><p class="s12" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span>sum<span style=" color: #231F20;">(i))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">But we want to sum the rows instead, like this w/ raw py: <span style=" color: #7358A5;">8.395</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">7.29</p><p class="s14" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">2.4869999999999997</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With the above, we could append these to some list in any way we want. That said, we’re going to use NumPy. As you probably guessed, we’re going to sum along axis 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;So we can sum axis 1, but note the current shape:&#39;</span>) <span style=" color: #32A7BD;">print</span>(np.sum(layer_outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">So we can sum axis 1, but note the current shape:</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">8.395 7.29 2.487</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">As pointed out by “<span class="s20">note the current shape</span>,” we did get the sums that we expected, but actually, we want to simplify the outputs to a single value per sample. We’re trying to sum all the outputs from a layer for each sample in a batch; converting the layer’s output array with row length equal to the number of neurons in the layer, to just one value. We need a column vector with these values since it will let us normalize the whole batch of samples, sample-wise, with a single calculation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Sum axis 1, but keep the same dimensions as input:&#39;</span>) <span style=" color: #32A7BD;">print</span>(np.sum(layer_outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">Sum axis 1, but keep the same dimensions as input: [[<span style=" color: #7358A5;">8.395</span>]</p><p class="s10" style="padding-left: 26pt;text-indent: 0pt;line-height: 13pt;text-align: left;">[<span style=" color: #7358A5;">7.29 </span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2.487</span>]]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With this, we keep the same dimensions as the input. Now, if we divide the array containing a batch of the outputs with this array, NumPy will perform this sample-wise. That means that it’ll divide all of the values from each output row by the corresponding row from the sum array. Since this sum in each row is a single value, it’ll be used for the division with every value from the corresponding output row). We can combine all of this into a softmax class, like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Finally, we also included a subtraction of the largest of the inputs before we did the exponentiation. There are two main pervasive challenges with neural networks: “dead neurons” and very large numbers (referred to as “exploding” values). “Dead” neurons and enormous numbers can wreak havoc down the line and render a network useless over time. The exponential function used in softmax activation is one of the sources of exploding values. Let’s see some examples of how and why this can easily happen:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 217%;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np <span style=" color: #32A7BD;">print</span>(np.exp(<span style=" color: #7358A5;">1</span>))</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2.718281828459045</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.exp(<span style=" color: #7358A5;">10</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">22026.465794806718</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.exp(<span style=" color: #7358A5;">100</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2.6881171418161356e+43</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.exp(<span style=" color: #7358A5;">1000</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;"><u>  </u>main<u> </u>:1: <span class="s31">RuntimeWarning</span>: overflow encountered in exp inf</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It doesn’t take a very large number, in this case, a mere <i>1,000</i>, to cause an overflow error. We know the exponential function tends toward 0 as its input value approaches negative infinity, and the output is 1 when the input is 0 (as shown in the chart earlier):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.exp(<span style=" color: #C71F43;">-</span>np.inf), np.exp(<span style=" color: #7358A5;">0</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0 1.0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can use this property to prevent the exponential function from overflowing. Suppose we subtract the maximum value from a list of input values. We would then change the output values to always be in a range from some negative value up to 0, as the largest number subtracted by itself returns 0, and any smaller number subtracted by it will result in a negative number — exactly the range discussed above. With Softmax, thanks to the normalization, we can subtract any value from all of the inputs, and it will not change the output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">softmax <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">softmax.forward([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]]) <span style=" color: #32A7BD;">print</span>(softmax.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">0.09003057 0.24472847 0.66524096</span>]]</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">softmax.forward([[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>]]) <span style=" color: #A5A4A5;"># subtracted 3 - max from the list </span><span style=" color: #32A7BD;">print</span>(softmax.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">0.09003057 0.24472847 0.66524096</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;"><span class="p">This is another useful property of the exponentiated and normalized function. There’s one more thing to mention in addition to these calculations. What happens if we divide the layer’s output data, </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>]<span class="p">, for example, by 2?</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">softmax.forward([[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1.5</span>]]) <span style=" color: #32A7BD;">print</span>(softmax.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">0.18632372 0.30719589 0.50648039</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The output confidences have changed due to the nonlinearity nature of the exponentiation. This is one example of why we need to scale all of the input data to a neural network in the same way, which we’ll explain in further detail in chapter 22.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, we can add another dense layer as the output layer, setting it to contain as many inputs as the previous layer has outputs and as many outputs as our data includes classes. Then we can apply the softmax activation to the output of this new layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax activation (to be used with Dense layer): </span>activation2 <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass through activation function # it takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Make a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># it takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass through activation function # it takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(activation2.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[[<span style=" color: #7358A5;">0.33333334</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.33333334</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.33333334<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333316</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333332</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333364<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333287</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333329</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333418<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[<span style=" color: #7358A5;">0.3333326</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.33333263</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333477<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333233</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333324</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333528<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">As you can see, the distribution of predictions is almost equal, as each of the samples has ~33% (0.33) predictions for each class. This results from the random initialization of weights (a draw from the normal distribution, as not every random initialization will result in this) and zeroed biases. These outputs are also our “confidence scores.” To determine which classification the model has chosen to be the prediction, we perform an </span><span class="s3">argmax </span><span class="p">on these outputs, which checks which of the classes in the output distribution has the highest confidence and returns its index - the predicted class index. That said, the confidence score can be as important as the class prediction itself. For example, the argmax of </span>[<span style=" color: #7358A5;">0.22</span>, <span style=" color: #7358A5;">0.6</span>, <span style=" color: #7358A5;">0.18</span>] <span class="p">is the same as the argmax for</span></p><p class="s10" style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">[<span style=" color: #7358A5;">0.32</span>, <span style=" color: #7358A5;">0.36</span>, <span style=" color: #7358A5;">0.32</span>]<span class="p">. In both of these, the argmax function would return an index value of 1 (the 2nd element in Python’s zero-indexed paradigm), but obviously, a 60% confidence is much better than a 36% confidence.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark83">Full code up to this point:</a><a name="bookmark105">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 217%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>): <span style=" color: #A5A4A5;"># Initialize weights and biases</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax activation (to be used with Dense layer): </span>activation2 <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass through activation function # it takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Make a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># it takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Make a forward pass through activation function # it takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(activation2.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:78pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:145pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[[<span style=" color: #7358A5;">0.33333334</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.33333334</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333334<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333316</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333332</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333364<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333287</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333329</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333418<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[<span style=" color: #7358A5;">0.3333326</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.33333263</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333477<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333233</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333324</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333528<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ve completed what we need for forward-passing data through our model. We used the <b>Rectified Linear (ReLU</b>) activation function on the hidden layer, which works on a per-neuron basis. We additionally used the <b>Softmax </b>activation function for the output layer since it accepts non-normalized values as input and outputs a probability distribution, which we’re using as confidence scores for each class. Recall that, although neurons are interconnected, they each have their respective weights and biases and are not “normalized” with each other.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, our example model is currently random. To remedy this, we need a way to calculate how wrong the neural network is at current predictions and begin adjusting weights and biases to decrease error over time. Thus, our next step is to quantify how wrong the model is through what’s defined as a <b>loss function</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 103pt;text-indent: 30pt;line-height: 130%;text-align: left;">Supplementary Material: <span class="p">https://nnfs.io/ch4 Chapter code, further resources, and errata for this chapter.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark106">Chapter 5</a><a name="bookmark111">&zwnj;</a><a name="bookmark112">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;line-height: 130%;text-align: left;">Calculating Network Error with Loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With a randomly-initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach, a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. To do this, we calculate how much error the model has. The <b>loss function</b>, also referred to as the <b>cost function</b>, is the algorithm that quantifies how wrong a model is. <b>Loss </b>is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;"><span class="p">You may wonder why we do not calculate the error of a model based on the argmax accuracy. Recall our earlier example of confidence: </span>[<span style=" color: #7358A5;">0.22</span>, <span style=" color: #7358A5;">0.6</span>, <span style=" color: #7358A5;">0.18</span>] <span class="p">vs </span>[<span style=" color: #7358A5;">0.32</span>, <span style=" color: #7358A5;">0.36</span>, <span style=" color: #7358A5;">0.32</span>]<span class="s3">. </span><span class="p">If the correct class were indeed the middle one (index 1), the model accuracy would be identical between the two above. But are these two examples </span><span class="s3">really </span><span class="p">as accurate as each other? They are</span></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">not, because accuracy is simply applying an argmax to the output to find the index of the biggest value. The output of a neural network is actually confidence, and more confidence in the correct</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark107">answer is better. Because of this, we strive to increase correct confidence and decrease misplaced confidence.</a><a name="bookmark113">&zwnj;</a><a name="bookmark114">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Categorical Cross-Entropy Loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you’re familiar with linear regression, then you already know one of the loss functions used with neural networks that do regression: <b>squared error </b>(or <b>mean squared error </b>with neural networks).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’re not performing regression in this example; we’re classifying, so we need a different loss function. The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution. <b>Categorical cross-entropy </b>is explicitly used to compare a “ground-truth” probability (<i>y </i>or “<i>targets</i>”) and some predicted distribution (<i>y-hat </i>or</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">“<i>predictions</i>”), so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The formula for calculating the categorical cross-entropy of <i>y </i>(actual/desired distribution) and</p><p class="s3" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">y-hat <span class="p">(predicted distribution) is:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 16pt;text-align: left;">Where <i>L</i><span class="s39">i </span>denotes sample loss value, <i>i </i>is the i-th sample in the set, <i>j </i>is the label/output index, <i>y</i></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;">denotes the target values, and <i>y-hat </i>denotes the predicted values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Once we start coding the solution, we’ll simplify it further to -<i>log(correct_class_confidence)</i>, the formula for which is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Where <i>L</i><span class="s39">i </span>denotes sample loss value, <i>i </i>is the i-th sample in a set, <i>k </i>is the index of the target label (ground-true label), <i>y </i>denotes the target values and <i>y-hat </i>denotes the predicted values.</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark115">You may ask why we call this cross-entropy and not </a><b>log loss</b>, which is also a type of loss. If you do not know what log loss is, you may wonder why there is such a fancy looking formula for what looks to be a fairly basic description.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">In general, the log loss error function is what we apply to the output of a binary logistic regression model (which we’ll describe in chapter 16) — there are only two classes in the distribution, each of them applying to a single output (neuron) which is targeted as a 0 or 1. In our case, we have a classification model that returns a probability distribution over all of the outputs. Cross-entropy compares two probability distributions. In our case, we have a softmax output, let’s say it’s:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">softmax_output <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Which probability distribution do we intend to compare this to? We have 3 class confidences in the above output, and let’s assume that the desired prediction is the first class (index 0, which is currently 0.7). If that’s the intended prediction, then the desired probability distribution is</p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>]<span class="p">. Cross-entropy can also work on probability distributions like </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.3</span>]<span class="p">; they wouldn’t have to look like the one above. That said, the desired probabilities will consist of a 1 in the desired class, and a 0 in the remaining undesired classes. Arrays or vectors like this are called </span><span class="h3">one-hot</span><span class="s3">, </span><span class="p">meaning one of the values is “hot” (on), with a value of 1, and the rest are “cold” (off), with values of 0. When comparing the model’s results to a one-hot vector using cross- entropy, the other parts of the equation zero out, and the target probability’s log loss is multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy. To exemplify this — if we take a softmax output of </span>[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>] <span class="p">and targets of </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>]<span class="p">, we can apply the calculations as follows:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark116">Let’s see the Python code for this:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">math</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># An example output from the output layer of the neural network </span>softmax_output <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Ground truth </span>target_output <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 74pt;text-indent: -54pt;line-height: 109%;text-align: justify;">loss <span style=" color: #C71F43;">= -</span>(math.log(softmax_output[<span style=" color: #7358A5;">0</span>])<span style=" color: #C71F43;">*</span>target_output[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+ </span>math.log(softmax_output[<span style=" color: #7358A5;">1</span>])<span style=" color: #C71F43;">*</span>target_output[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+ </span>math.log(softmax_output[<span style=" color: #7358A5;">2</span>])<span style=" color: #C71F43;">*</span>target_output[<span style=" color: #7358A5;">2</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(loss)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.35667494393873245</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">That’s the full categorical cross-entropy calculation, but we can make a few assumptions given one-hot target vectors. First, what are the values for </span>target_output[<span style=" color: #7358A5;">1</span>] <span class="p">and </span>target_ output[<span style=" color: #7358A5;">2</span>] <span class="p">in this case? They’re both 0, and anything multiplied by 0 is 0. Thus, we don’t need to calculate these indices. Next, what’s the value for </span>target_output[<span style=" color: #7358A5;">0</span>] <span class="p">in this case? It’s 1. So this can be omitted as any number multiplied by 1 remains the same. The same output then, in this example, can be calculated with:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= -</span>math.log(softmax_output[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Which still gives us:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.35667494393873245</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see with one-hot vector targets, or scalar values that represent them, we can make some simple assumptions and use a more basic calculation — what was once an involved formula reduces to the negative log of the target class’ confidence score — the second formula presented at the beginning of this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">As we’ve already discussed, the example confidence level might look like </span>[<span style=" color: #7358A5;">0.22</span>, <span style=" color: #7358A5;">0.6</span>, <span style=" color: #7358A5;">0.18</span>] <span class="p">or </span>[<span style=" color: #7358A5;">0.32</span>, <span style=" color: #7358A5;">0.36</span>, <span style=" color: #7358A5;">0.32</span>]<span class="p">. In both cases, the </span><span class="s3">argmax </span><span class="p">of these vectors will return the second class as the prediction, but the model’s confidence about these predictions is high only for one of them. The </span><span class="h3">Categorical Cross-Entropy Loss </span><span class="p">accounts for that and outputs a larger loss the lower the confidence is:</span></p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a name="bookmark117">import </a><span style=" color: #231F20;">math</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">1.</span>)) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.95</span>)) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.9</span>)) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.8</span>)) <span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;...&#39;</span>) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.2</span>)) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.1</span>)) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.05</span>)) <span style=" color: #32A7BD;">print</span>(math.log(<span style=" color: #7358A5;">0.01</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.0</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">-<span style=" color: #7358A5;">0.05129329438755058</span></p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">0.10536051565782628</span></p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">0.2231435513142097</span></p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">1.6094379124341003</span></p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">2.3025850929940455</span></p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">2.995732273553991</span></p><p class="s11" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">4.605170185988091</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ve printed different log values for a few example confidences. When the confidence level equals <i>1</i>, meaning the model is 100% “sure” about its prediction, the loss value for this sample equals <i>0</i>. The loss value raises with the confidence level, approaching 0. You might also wonder why we did not print the result of <i>log(0) </i>— we’ll explain that shortly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far, we’ve applied log() to the softmax output, but have neither explained what “log” is nor why we use it. We will save the discussion of “why” until the next chapter, which covers</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">derivatives, gradients, and optimizations; suffice it to say that the log function has some desirable properties. <b>Log </b>is short for <b>logarithm </b>and is defined as the solution for the x-term in an equation</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 9pt;text-align: left;">of the form a<span class="s40">x</span> = b. For example, <i>10</i><i>x</i><i> = 100 </i>can be solved with a log: <i>log (100)</i>, which evaluates to</p><p class="s42" style="text-indent: 0pt;line-height: 7pt;text-align: right;">10</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">2. This property of the log function is <i>especially </i>beneficial when <i>e </i>(Euler’s number or <i>~2.71828</i>) is used in the base (where 10 is in the example). The logarithm with <i>e </i>as its base is referred to as the <b>natural logarithm</b>, <b>natural log</b>, or simply <b>log </b>— you may also see this written as <b>ln</b>: <i>ln(x)</i></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">= log(x) = log<span class="s39">e</span>(x) <span class="p">The variety of conventions can make this confusing, so to simplify things,</span></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><b>any mention of log will always be a natural logarithm throughout this book</b><span class="p">. The natural log represents the solution for the x-term in the equation </span>e<span class="s41">x</span> = b<span class="p">; for example, </span>e<span class="s41">x</span> = 5.2 <span class="p">is solved by </span>log(5.2)<span class="p">.</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">In Python code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np b </span>= <span style=" color: #7358A5;">5.2</span></p><p class="s12" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(np.log(b))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.6486586255873816</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can confirm this by exponentiating our result:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">math</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(math.e <span style=" color: #C71F43;">** </span><span style=" color: #7358A5;">1.6486586255873816</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">5.199999999999999</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The small difference is the result of floating-point precision in Python. Getting back to the loss calculation, we need to modify our output in two additional ways. First, we’ll update our process to work on batches of softmax output distributions; and second, make the negative log calculation dynamic to the target index (the target index has been hard-coded so far).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Consider a scenario with a neural network that performs classification between three classes, and the neural network classifies in batches of three. After running through the softmax activation function with a batch of 3 samples and 3 classes, the network’s output layer yields:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Probabilities for 3 samples </span>softmax_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-left: 189pt;text-indent: 0pt;line-height: 13pt;text-align: left;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 3pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We need a way to dynamically calculate the categorical cross-entropy, which we now know is a negative log calculation. To determine which value in the softmax output to calculate the negative log from, we simply need to know our target values. In this example, there are 3 classes; let’s say we’re trying to classify something as a “dog,” “cat,” or “human.” A dog is class 0 (at index 0), a cat class 1 (index 1), and a human class 2 (index 2). Let’s assume the batch of three sample inputs to this neural network is being mapped to the target values of a dog, cat, and cat. So the targets (as</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">a list of target indices) would be <i>[0, 1, 1]</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">softmax_outputs <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 134pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class_targets <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>] <span style=" color: #A5A4A5;"># dog, cat, cat</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">The first value, 0, in </span>class_targets <span class="p">means the first softmax output distribution’s intended prediction was the one at the 0th index of </span>[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>]<span class="p">; the model has a </span><span style=" color: #7358A5;">0.7 </span><span class="p">confidence score that this observation is a dog. This continues throughout the batch, where the intended target of the 2nd softmax distribution, </span>[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>]<span class="p">, was at an index of 1; the model only has a</span></p><p class="s21" style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">0.5 <span class="p">confidence score that this is a cat — the model is less certain about this observation. In the last sample, it’s also the 2nd index from the softmax distribution, a value of </span>0.9 <span class="p">in this case — a pretty high confidence.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">With a collection of softmax outputs and their intended targets, we can map these indices to retrieve the values from the softmax distributions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">softmax_outputs <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 134pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class_targets <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span style=" color: #C71F43;">for </span>targ_idx, distribution <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">zip</span>(class_targets, softmax_outputs): <span style=" color: #32A7BD;">print</span>(distribution[targ_idx])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.7</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.5</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.9</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">The <span class="s43">zip</span><span class="s22">() </span>function, again, lets us iterate over multiple iterables at the same time in Python. This can be further simplified using NumPy (we’re creating a NumPy array of the Softmax outputs this time):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">softmax_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class_targets <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(softmax_outputs[[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], class_targets])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.7 0.5 0.9</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">What are the 0, 1, and 2 values? NumPy lets us index an array in multiple ways. One of them is to use a list filled with indices and that’s convenient for us — we could use the <span class="s22">class_targets </span>for this purpose as it already contains the list of indices that we are interested in. The problem is that this has to filter data rows in the array — the second dimension. To perform that, we also need to explicitly filter this array in its first dimension. This dimension contains the predictions and we, of course, want to retain them all. We can achieve that by using a list containing numbers from 0 through all of the indices. We know we’re going to have as many indices as distributions in our entire batch, so we can use a <span class="s43">range</span><span class="s22">() </span>instead of typing each value ourselves:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;">print<span style=" color: #231F20;">(softmax_outputs[ </span>range<span style=" color: #231F20;">(</span>len<span style=" color: #231F20;">(softmax_outputs)), class_targets</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.7 0.5 0.9</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This returns a list of the confidences at the target indices for each of the samples. Now we apply the negative log to this list:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #C71F43;">-</span>np.log(softmax_outputs[ <span style=" color: #32A7BD;">range</span>(<span style=" color: #32A7BD;">len</span>(softmax_outputs)), class_targets</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.35667494 0.69314718 0.10536052</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Finally, we want an average loss per batch to have an idea about how our model is doing during training. There are many ways to calculate an average in Python; the most basic form of an average is the <b>arithmetic mean</b>: <i>sum(iterable) / len(iterable)</i>. NumPy has a method that computes this average on arrays, so we will use that instead. We add NumPy’s average to the code:</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">neg_log <span style=" color: #C71F43;">= -</span>np.log(softmax_outputs[</p><p class="s12" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;">range<span style=" color: #231F20;">(</span>len<span style=" color: #231F20;">(softmax_outputs)), class_targets</span></p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;">])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">average_loss <span style=" color: #C71F43;">= </span>np.mean(neg_log) <span style=" color: #32A7BD;">print</span>(average_loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.38506088005216804</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">We have already learned that targets can be one-hot encoded, where all values, except for one, are zeros, and the correct label’s position is filled with 1. They can also be sparse, which means that the numbers they contain are the correct class numbers — we are generating them this way with the <span class="s35">spiral_data</span><span class="s22">() </span>function, and we can allow the loss calculation to accept any of these forms. Since we implemented this to work with sparse labels (as in our training data), we have to add a check if they are one-hot encoded and handle it a bit differently in this new case. The check can be performed by counting the dimensions — if targets are single-dimensional (like a list), they</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">are sparse, but if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors. In this second case, we’ll implement a solution using the first equation from this chapter, instead of filtering out the confidences at the target labels. We have to multiply confidences by the targets, zeroing out all values except the ones at correct labels, performing a sum along the row axis (axis <i>1</i>). We have to add a test to the code we just wrote for the number of dimensions, move calculations of the log values outside of this new <i>if </i>statement, and implement the solution for the one-hot encoded labels following the first equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">softmax_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:13pt"><td style="width:96pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">class_targets <span style=" color: #C71F43;">=</span></p></td><td style="width:85pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">np.array([[<span style=" color: #7358A5;">1</span>,</p></td><td style="width:18pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:96pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:85pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0</span>,</p></td><td style="width:18pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:96pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:85pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0</span>,</p></td><td style="width:18pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(class_targets.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">softmax_outputs[</span></p><p class="s12" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">range<span style=" color: #231F20;">(</span>len<span style=" color: #231F20;">(softmax_outputs)), class_targets</span></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">]</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(class_targets.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 109%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( softmax_outputs<span style=" color: #C71F43;">*</span>class_targets, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">neg_log <span style=" color: #C71F43;">= -</span>np.log(correct_confidences)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">average_loss <span style=" color: #C71F43;">= </span>np.mean(neg_log) <span style=" color: #32A7BD;">print</span>(average_loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.38506088005216804</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Before we move on, there is one additional problem to solve. The softmax output, which is also an input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences. It is possible that the model will have full confidence for one label making all the remaining confidences zero. Similarly, it is also possible that the model will assign full confidence to a value that wasn’t the target. If we then try to calculate the loss of this confidence of 0:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 217%;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np <span style=" color: #32A7BD;">print</span>(<span style=" color: #C71F43;">-</span>np.log(<span style=" color: #7358A5;">0</span>))</p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><u>  </u>main<u> </u>:1: <span class="s31">RuntimeWarning</span>: divide by zero encountered in log inf</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-top: 7pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">Before we explain this, we need to talk about </span>log(0)<span class="p">. From the mathematical point of view, </span>log(0) <span class="p">is undefined. We already know the following dependence: if </span>y=log(x)<span class="p">, then </span>e<span class="s41">y</span>=x<span class="p">. The question of what the resulting </span>y <span class="p">is in </span>y=log(0) <span class="p">is the same as the question of what’s the </span>y <span class="p">in </span>e<span class="s41">y</span>=0<span class="p">. In simplified terms, the constant </span>e <span class="p">to any power is always a positive number, and there is no </span>y <span class="p">resulting in </span>e<span class="s41">y</span>=0<span class="p">. This means the </span>log(0) <span class="p">is undefined. We need to be aware of what the </span>log(0) <span class="p">is, and “undefined” does not mean that we don’t know anything about it. Since </span>log(0) <span class="p">is undefined, what’s the result for a value very close to </span>0<span class="p">? We can calculate the limit of a function. How to exactly calculate it exceeds this book, but the solution is:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We read it as the limit of a natural logarithm of <i>x</i>, with x approaching <i>0 </i>from a positive (it is</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">impossible to calculate the natural logarithm of a negative value) equals negative infinity. What this means is that the limit is negative infinity for an infinitely small <i>x</i>, where <i>x </i>never reaches <i>0</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">The situation is a bit different in programming languages. We do not have limits here, just a function which, given a parameter, returns some value. The negative natural logarithm of <i>0</i>, in Python with NumPy, equals an infinitely big number, rather than undefined, and prints a warning about a division by <i>0 </i>(which is a result of how this calculation is done). If -<span class="s22">np.log(</span><span class="s21">0</span><span class="s22">) </span>equals <span class="s22">inf</span>, is it possible to calculate e to the power of negative infinity with Python?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.e<span style=" color: #C71F43;">**</span>(<span style=" color: #C71F43;">-</span>np.inf))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In programming, the fewer things that are undefined, the better. Later on, we’ll see similar simplifications, for example when calculating a derivative of the absolute value function, which does not exist for an input of <i>0 </i>and we’ll have to make some decisions to work around this.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;"><span class="p">Back to the result of </span>inf <span class="p">for </span><span style=" color: #C71F43;">-</span>np.log(<span style=" color: #7358A5;">0</span>) <span class="p">— as much as that makes sense, since the model would be fully wrong, this will be a problem for us to do further calculations with. Later, with optimization, we will also have a problem calculating gradients, starting with a mean value of all sample-wise losses since a single infinite value in a list will cause the average of that list to also be infinite:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.mean([<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #C71F43;">-</span>np.log(<span style=" color: #7358A5;">0</span>)]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;"><u>  </u>main<u> </u>:<span style=" color: #7358A5;">1</span>: <span class="s31">RuntimeWarning</span>: divide by zero encountered in log inf</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We could add a very small value to the confidence to prevent it from being a zero, for example,</p><p class="s3" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">1e-7<span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #C71F43;">-</span>np.log(<span style=" color: #7358A5;">1e-7</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">16.11809565095832</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Adding a very small value, one-tenth of a million, to the confidence at its far edge will insignificantly impact the result, but this method yields an additional 2 issues. First, in the case where the confidence value is <i>1</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #C71F43;">-</span>np.log(<span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1e-7</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">9.999999505838704e-08</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">When the model is fully correct in a prediction and puts all the confidence in the correct label, loss becomes a negative value instead of being 0. The other problem here is shifting confidence towards <i>1</i>, even if by a very small value. To prevent both issues, it’s better to clip values from both sides by the same number, <i>1e-7 </i>in our case. That means that the lowest possible value will become <i>1e-7 </i>(like in the demonstration we just performed) but the highest possible value, instead of being <i>1+1e-7</i>, will become <i>1-1e-7 </i>(so slightly less than <i>1</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #C71F43;">-</span>np.log(<span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1e-7</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.0000000494736474e-07</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">This will prevent loss from being exactly <i>0</i>, making it a very small value instead, but won’t make it a negative value and won’t bias overall loss towards <i>1</i>. Within our code and using numpy, we’ll accomplish that using <span class="s22">np.clip() </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This method can perform clipping on an array of values, so we can apply it to the predictions directly and save this as a separate array, which we’ll use shortly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark108">The Categorical Cross-Entropy Loss Class</a><a name="bookmark118">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">In the later chapters, we’ll be adding more loss functions and some of the operations that we’ll be performing are common for all of them. One of these operations is how we calculate the overall loss — no matter which loss function we’ll use, the overall loss is always a mean value of all sample losses. Let’s create the <span class="s35">Loss </span>class containing the <span class="s35">calculate </span>method that will call our loss object’s forward method and calculate the mean value of the returned sample losses:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In later chapters, we’ll add more code to this class, and the reason for it to exist will become more clear. For now, we’ll use it for this single purpose.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s convert our loss code into a class for convenience down the line:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s17" style="padding-left: 92pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(<span style=" color: #231F20;">y_true.shape</span>) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">1</span>: <span style=" color: #231F20;">correct_confidences </span><span style=" color: #C71F43;">= </span><span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 109%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 13pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 109%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped<span style=" color: #C71F43;">*</span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 13pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">This class inherits the <span class="s35">Loss </span>class and performs all the error calculations that we derived throughout this chapter and can be used as an object. For example, using the manually-created output and targets:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">loss <span style=" color: #C71F43;">= </span>loss_function.calculate(softmax_outputs, class_targets) <span style=" color: #32A7BD;">print</span>(loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.38506088005216804</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark109">Combining everything up to this point:</a><a name="bookmark119">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 217%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>): <span style=" color: #A5A4A5;"># Initialize weights and biases</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s17" style="padding-left: 92pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(<span style=" color: #231F20;">y_true.shape</span>) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">1</span>: <span style=" color: #231F20;">correct_confidences </span><span style=" color: #C71F43;">= </span><span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 109%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 13pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 109%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped<span style=" color: #C71F43;">*</span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 13pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax activation (to be used with Dense layer): </span>activation2 <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Perform a forward pass through activation function # it takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># it takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Perform a forward pass through activation function # it takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(activation2.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through activation function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># it takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Print loss value </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:78pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:145pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[[<span style=" color: #7358A5;">0.33333334</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.33333334</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333334<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333316</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333332</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333364<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333287</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333329</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333418<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[<span style=" color: #7358A5;">0.3333326</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.33333263</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333477<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333233</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333324</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333528<span style=" color: #231F20;">]]</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss: <span style=" color: #7358A5;">1.0986104</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 9pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Again, we get <i>~0.33 </i>values since the model is random, and its average loss is also not great for these data, as we’ve not yet trained our model on how to correct its errors.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark110">Accuracy Calculation</a><a name="bookmark120">&zwnj;</a><a name="bookmark121">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the <b>accuracy</b>, which describes how often the largest confidence is the correct class in terms of a fraction. Conveniently, we can reuse existing variable definitions to calculate the</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">accuracy metric. We will use the <i>argmax </i>values from the <i>softmax outputs </i>and then compare these to the targets. This is as simple as doing (note that we slightly modified the <span class="s22">softmax_outputs </span>for the purpose of this example):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Probabilities of 3 samples </span>softmax_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.1</span>],</p><p class="s10" style="padding-left: 189pt;text-indent: 0pt;line-height: 13pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Target (ground-truth) labels for 3 samples </span>class_targets <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate values along second axis (axis of index 1) </span>predictions <span style=" color: #C71F43;">= </span>np.argmax(softmax_outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># If targets are one-hot encoded - convert them </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(class_targets.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 109%;text-align: left;">class_targets <span style=" color: #C71F43;">= </span>np.argmax(class_targets, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># True evaluates to 1; False to 0</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 217%;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>class_targets) <span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">acc: <span style=" color: #7358A5;">0.6666666666666666</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We are also handling one-hot encoded targets by converting them to sparse values using</p><p class="s22" style="padding-top: 2pt;padding-left: 11pt;text-indent: 0pt;text-align: left;">np.argmax()<span class="p">.</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">We can add the following to the end of our full script above to calculate its accuracy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(activation2.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 109%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Print accuracy </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">acc: <span style=" color: #7358A5;">0.34</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Now that you’ve learned how to perform a forward pass through our network and calculate the metrics to signal if the model is performing poorly, we will embark on optimization in the next chapter!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch5" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch5<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark122">Chapter 6</a><a name="bookmark124">&zwnj;</a><a name="bookmark125">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Introducing Optimization</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 31pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that the neural network is built, able to have data passed through it, and capable of calculating loss, the next step is to determine how to adjust the weights and biases to decrease the loss. Finding an intelligent way to adjust the neurons’ input’s weights and biases to minimize loss is the main difficulty of neural networks.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The first option one might think of is randomly changing the weights, checking the loss, and repeating this until happy with the lowest loss found. To see this in action, we’ll use a simpler dataset than we’ve been working with so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 217%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">vertical_data nnfs.init()</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">X, y <span style=" color: #C71F43;">= </span>vertical_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>) plt.scatter(X[:, <span style=" color: #7358A5;">0</span>], X[:, <span style=" color: #7358A5;">1</span>], <span class="s23">c</span><span style=" color: #C71F43;">=</span>y, <span class="s23">s</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">40</span>, <span class="s23">cmap</span><span style=" color: #C71F43;">=</span><span style=" color: #8F8633;">&#39;brg&#39;</span>) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Which looks like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 114pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 6.01: <span class="p">“Vertical data” graphed.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Using the previously created code up to this point, we can use this new dataset with a simple neural network:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>vertical_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>) <span style=" color: #A5A4A5;"># first dense layer, 2 inputs </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>) <span style=" color: #A5A4A5;"># second dense layer, 3 inputs, 3 outputs </span>activation2 <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Then create some variables to track the best loss and the associated weights and biases:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Helper variables</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">lowest_loss <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">9999999 </span><span style=" color: #A5A4A5;"># some initial value </span>best_dense1_weights <span style=" color: #C71F43;">= </span>dense1.weights.copy() best_dense1_biases <span style=" color: #C71F43;">= </span>dense1.biases.copy() best_dense2_weights <span style=" color: #C71F43;">= </span>dense2.weights.copy() best_dense2_biases <span style=" color: #C71F43;">= </span>dense2.biases.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">We initialized the loss to a large value and will decrease it when a new, lower, loss is found. We are also copying weights and biases (<span class="s22">copy() </span>ensures a full copy instead of a reference to the object). Now we iterate as many times as desired, pick random values for weights and biases, and save the weights and biases if they generate the lowest-seen loss:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>iteration <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10000</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Generate a new set of weights for iteration </span>dense1.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dense1.biases <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">dense2.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">dense2.biases <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: justify;"># Perform a forward pass of the training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: justify;">activation1.forward(dense1.output) dense2.forward(activation1.output) activation2.forward(dense2.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through activation function</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># it takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(activation2.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># If loss is smaller - print and save weights and biases aside </span>if <span style=" color: #231F20;">loss </span>&lt; <span style=" color: #231F20;">lowest_loss:</span></p><p class="s10" style="padding-left: 104pt;text-indent: -36pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;New set of weights found, iteration:&#39;</span>, iteration, <span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss, <span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">best_dense1_weights <span style=" color: #C71F43;">= </span>dense1.weights.copy() best_dense1_biases <span style=" color: #C71F43;">= </span>dense1.biases.copy() best_dense2_weights <span style=" color: #C71F43;">= </span>dense2.weights.copy() best_dense2_biases <span style=" color: #C71F43;">= </span>dense2.biases.copy() lowest_loss <span style=" color: #C71F43;">= </span>loss</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">0 </span>loss: <span style=" color: #7358A5;">1.0986564 </span>acc: <span style=" color: #7358A5;">0.3333333333333333</span></p><p class="s10" style="padding-bottom: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">3 </span>loss: <span style=" color: #7358A5;">1.098138 </span>acc: <span style=" color: #7358A5;">0.3333333333333333</span></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">117</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:117pt" colspan="2"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">1.0980115 <span style=" color: #231F20;">acc:</span></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:27pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">124</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:117pt" colspan="2"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.0977516 <span style=" color: #231F20;">acc: </span>0.6</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">165</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:117pt" colspan="2"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.097571 <span style=" color: #231F20;">acc:</span></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:27pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">552</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:117pt" colspan="2"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.0974693 <span style=" color: #231F20;">acc: </span>0.34</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">778</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:117pt" colspan="2"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.0968257 <span style=" color: #231F20;">acc:</span></p></td></tr><tr style="height:13pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:27pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:114pt"><p class="s24" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-top: 1pt;padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-top: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">4307</p></td><td style="width:37pt"><p class="s24" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">loss:</p></td><td style="width:63pt"><p class="s26" style="padding-top: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">1.0965533</p></td><td style="width:54pt"><p class="s24" style="padding-top: 1pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">acc:</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:27pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:63pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:54pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4615</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:63pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.0964499</p></td><td style="width:54pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:27pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:63pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:54pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:27pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9450</p></td><td style="width:37pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:63pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.0964295</p></td><td style="width:54pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td></tr><tr style="height:13pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:27pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:63pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:54pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Loss certainly falls, though not by much. Accuracy did not improve, except for a singular situation where the model randomly found a set of weights yielding better accuracy. Still, with a fairly large loss, this state is not stable. Running an additional 90,000 iterations for 100,000 in total:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">iteration:</p></td><td style="width:37pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">13361</p></td><td style="width:36pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:60pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.0963014</p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">acc:</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:37pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">14001</p></td><td style="width:36pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:60pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.0959858</p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:15pt"><td style="width:114pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">New set of weights</p></td><td style="width:43pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">found,</p></td><td style="width:66pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">iteration:</p></td><td style="width:37pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">24598</p></td><td style="width:36pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:60pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.0947444</p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td></tr><tr style="height:14pt"><td style="width:114pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3333333333333333</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:66pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:37pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Loss continued to drop, but accuracy did not change. This doesn’t appear to be a reliable method for minimizing loss. After running for 1 billion iterations, the following was the best (lowest loss) result:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 124%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">229865000 </span>loss: <span style=" color: #7358A5;">1.0911305 </span>acc: <span style=" color: #7358A5;">0.3333333333333333</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Even with this basic dataset, we see that randomly searching for weight and bias combinations will take far too long to be an acceptable method. Another idea might be, instead of setting parameters with randomly-chosen values each iteration, apply a fraction of these values to parameters. With this, weights will be updated from what currently yields us the lowest loss instead of aimlessly randomly. If the adjustment decreases loss, we will make it the new point to adjust from. If loss instead increases due to the adjustment, then we will revert to the previous point. Using similar code from earlier, we will first change from randomly selecting weights and biases to randomly <i>adjusting </i>them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights with some small random values </span>dense1.weights <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dense1.biases <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">dense2.weights <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">dense2.biases <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we will change our ending <span class="s11">if </span>statement to be:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># If loss is smaller - print and save weights and biases aside </span>if <span style=" color: #231F20;">loss </span>&lt; <span style=" color: #231F20;">lowest_loss:</span></p><p class="s10" style="padding-left: 104pt;text-indent: -36pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;New set of weights found, iteration:&#39;</span>, iteration, <span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss, <span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">best_dense1_weights <span style=" color: #C71F43;">= </span>dense1.weights.copy() best_dense1_biases <span style=" color: #C71F43;">= </span>dense1.biases.copy() best_dense2_weights <span style=" color: #C71F43;">= </span>dense2.weights.copy() best_dense2_biases <span style=" color: #C71F43;">= </span>dense2.biases.copy() lowest_loss <span style=" color: #C71F43;">= </span>loss</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Revert weights and biases <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">dense1.weights <span style=" color: #C71F43;">= </span>best_dense1_weights.copy() dense1.biases <span style=" color: #C71F43;">= </span>best_dense1_biases.copy() dense2.weights <span style=" color: #C71F43;">= </span>best_dense2_weights.copy() dense2.biases <span style=" color: #C71F43;">= </span>best_dense2_biases.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark123">Full code up to this point:</a><a name="bookmark126">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>vertical_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>) <span style=" color: #A5A4A5;"># first dense layer, 2 inputs </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>) <span style=" color: #A5A4A5;"># second dense layer, 3 inputs, 3 outputs </span>activation2 <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Helper variables</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">lowest_loss <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">9999999 </span><span style=" color: #A5A4A5;"># some initial value </span>best_dense1_weights <span style=" color: #C71F43;">= </span>dense1.weights.copy() best_dense1_biases <span style=" color: #C71F43;">= </span>dense1.biases.copy() best_dense2_weights <span style=" color: #C71F43;">= </span>dense2.weights.copy() best_dense2_biases <span style=" color: #C71F43;">= </span>dense2.biases.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>iteration <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10000</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights with some small random values </span>dense1.weights <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dense1.biases <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">dense2.weights <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">dense2.biases <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">0.05 </span><span style=" color: #C71F43;">* </span>np.random.randn(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: justify;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: justify;">activation1.forward(dense1.output) dense2.forward(activation1.output) activation2.forward(dense2.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through activation function</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># it takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation2.output, y)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(activation2.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># If loss is smaller - print and save weights and biases aside </span>if <span style=" color: #231F20;">loss </span>&lt; <span style=" color: #231F20;">lowest_loss:</span></p><p class="s10" style="padding-left: 104pt;text-indent: -36pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;New set of weights found, iteration:&#39;</span>, iteration, <span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss, <span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">best_dense1_weights <span style=" color: #C71F43;">= </span>dense1.weights.copy() best_dense1_biases <span style=" color: #C71F43;">= </span>dense1.biases.copy() best_dense2_weights <span style=" color: #C71F43;">= </span>dense2.weights.copy() best_dense2_biases <span style=" color: #C71F43;">= </span>dense2.biases.copy() lowest_loss <span style=" color: #C71F43;">= </span>loss</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"># Revert weights and biases <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 109%;text-align: left;">dense1.weights <span style=" color: #C71F43;">= </span>best_dense1_weights.copy() dense1.biases <span style=" color: #C71F43;">= </span>best_dense1_biases.copy() dense2.weights <span style=" color: #C71F43;">= </span>best_dense2_weights.copy() dense2.biases <span style=" color: #C71F43;">= </span>best_dense2_biases.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">0 </span>loss: <span style=" color: #7358A5;">1.0987684 </span>acc: <span style=" color: #7358A5;">0.3333333333333333</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">29 </span>loss: <span style=" color: #7358A5;">1.0725244 </span>acc: <span style=" color: #7358A5;">0.5266666666666666</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">30 </span>loss: <span style=" color: #7358A5;">1.0724432 </span>acc: <span style=" color: #7358A5;">0.3466666666666667</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">48 </span>loss: <span style=" color: #7358A5;">1.0303522 </span>acc: <span style=" color: #7358A5;">0.6666666666666666</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">49 </span>loss: <span style=" color: #7358A5;">1.0292586 </span>acc: <span style=" color: #7358A5;">0.6666666666666666</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">97 </span>loss: <span style=" color: #7358A5;">0.9277446 </span>acc: <span style=" color: #7358A5;">0.7333333333333333</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">152 </span>loss: <span style=" color: #7358A5;">0.73390484 </span>acc: <span style=" color: #7358A5;">0.8433333333333334</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">156 </span>loss: <span style=" color: #7358A5;">0.7235515 </span>acc: <span style=" color: #7358A5;">0.87 </span>New set of weights found, iteration: <span style=" color: #7358A5;">160 </span>loss: <span style=" color: #7358A5;">0.7049076 </span>acc: <span style=" color: #7358A5;">0.9066666666666666</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">7446 </span>loss: <span style=" color: #7358A5;">0.17280102 </span>acc: <span style=" color: #7358A5;">0.9333333333333333</span></p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">9397 </span>loss: <span style=" color: #7358A5;">0.17279711 </span>acc: <span style=" color: #7358A5;">0.93</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Loss descended by a decent amount this time, and accuracy raised significantly. Applying a fraction of random values actually lead to a result that we could almost call a solution. If you try 100,000 iterations, you will not progress much further:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">14206 </span>loss: <span style=" color: #7358A5;">0.1727932 </span>acc: <span style=" color: #7358A5;">0.9333333333333333</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">New set of weights found, iteration: <span style=" color: #7358A5;">63704 </span>loss: <span style=" color: #7358A5;">0.17278232 </span>acc: <span style=" color: #7358A5;">0.9333333333333333</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s try this with the previously-seen spiral dataset instead:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New <span class="s31">set </span>of weights found, iteration: <span style=" color: #7358A5;">0 </span>loss: <span style=" color: #7358A5;">1.1008677 </span>acc: <span style=" color: #7358A5;">0.3333333333333333</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New <span class="s31">set </span>of weights found, iteration: <span style=" color: #7358A5;">31 </span>loss: <span style=" color: #7358A5;">1.0982264 </span>acc: <span style=" color: #7358A5;">0.37333333333333335</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New <span class="s31">set </span>of weights found, iteration: <span style=" color: #7358A5;">65 </span>loss: <span style=" color: #7358A5;">1.0954362 </span>acc: <span style=" color: #7358A5;">0.38333333333333336</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New <span class="s31">set </span>of weights found, iteration: <span style=" color: #7358A5;">67 </span>loss: <span style=" color: #7358A5;">1.093989 </span>acc: <span style=" color: #7358A5;">0.4166666666666667</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">New <span class="s31">set </span>of weights found, iteration: <span style=" color: #7358A5;">129 </span>loss: <span style=" color: #7358A5;">1.0874122 </span>acc: <span style=" color: #7358A5;">0.42333333333333334</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p><p class="s10" style="padding-top: 3pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">New <span class="s31">set </span>of weights found, iteration: <span style=" color: #7358A5;">5415 </span>loss: <span style=" color: #7358A5;">1.0790575 </span>acc: <span style=" color: #7358A5;">0.39</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This training session ended with almost no progress. Loss decreased slightly and accuracy is barely above the initial value. Later, we’ll learn that the most probable reason for this is called a local minimum of loss. The data complexity is also not irrelevant here. It turns out hard problems are hard for a reason, and we need to approach this problem more intelligently.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch6" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch6<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark127">Chapter 7</a><a name="bookmark133">&zwnj;</a><a name="bookmark134">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Derivatives</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 31pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Randomly changing and searching for optimal weights and biases did not prove fruitful for one main reason: the number of possible combinations of weights and biases is infinite, and we need something smarter than pure luck to achieve any success. Each weight and bias may also have different degrees of influence on the loss — this influence depends on the parameters themselves as well as on the current sample, which is an input to the first layer. These input values are then multiplied by the weights, so the input data affects the neuron’s output and affects the impact that the weights make on the loss. The same principle applies to the biases and parameters in the next layers, taking the previous layer’s outputs as inputs. This means that the impact on the output values depends on the parameters as well as the samples — which is why we are calculating the loss value per each sample separately. Finally, the function of <i>how </i>a weight or bias impacts the overall loss is not necessarily linear. In order to know <i>how </i>to adjust weights and biases, we first need to understand their impact on the loss.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">One concept to note is that we refer to weights and biases and their impact on the loss function. The loss function doesn’t contain weights or biases, though. The input to this function is the output of the model, and the weights and biases of the neurons influence this output. Thus, even though we calculate loss from the model’s output, not weights/biases, these weights and biases</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark128">directly impact the loss.</a><a name="bookmark135">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the coming chapters, we will describe exactly how this happens by explaining partial derivatives, gradients, gradient descent, and backpropagation. Basically, we’ll calculate how much each singular weight and bias changes the loss value (how much of an impact it has on it) given a sample (as each sample produces a separate output, thus also a separate loss value), and how to change this weight or bias for the loss value to decrease. Remember — our goal here is to decrease loss, and we’ll do this by using gradient descent. Gradient, on the other hand, is a result of the calculation of the partial derivatives, and we’ll backpropagate it using the chain rule to update all of the weights and biases. Don’t worry if that doesn’t make much sense yet; we’ll explain all of these terms and how to perform these actions in this and the coming chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To understand partial derivatives, we need to start with derivatives, which are a special case of partial derivatives — they are calculated from functions taking single parameters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The Impact of a Parameter on the Output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s start with a simple function and discover what is meant by “impact.” A very simple function <i>y=2x</i>, which takes <i>x </i>as an input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">f</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #7358A5;">2</span>*<span style=" color: #231F20;">x</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now let’s create some code around this to visualize the data — we’ll import NumPy and Matplotlib, create an array of 5 input values from 0 to 4, calculate the function output for each of these input values, and plot the result as lines between consecutive points. These points’ coordinates are inputs as <i>x </i>and function outputs as <i>y</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">f</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #7358A5;">2</span>*<span style=" color: #231F20;">x</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">x <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">5</span>)) y <span style=" color: #C71F43;">= </span>f(x)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">print<span style=" color: #231F20;">(x) </span>print<span style=" color: #231F20;">(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 1 2 3 4</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 2 4 6 8</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">plt.plot(x, y) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 84pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.01: <span class="p">Linear function y=2x graphed</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark129">The Slope</a><a name="bookmark136">&zwnj;</a><a name="bookmark137">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This looks like an output of the <i>f(x) = 2x </i>function, which is a line. How might you define the <i>impact </i>that <i>x </i>will have on <i>y</i>? Some will say, <i>“y </i>is double <i>x.</i>” Another way to describe the <i>impact </i>of a linear function such as this comes from algebra: the <b>slope</b>. “Rise over run” might be a phrase you recall from school. The slope of a line is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It is change in <i>y </i>divided by change in <i>x, </i>or, in math — <i>delta y </i>divided by <i>delta x</i>. What’s the slope of <i>f(x) = 2x </i>then?</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate the slope, first we have to take any two points lying on the function’s graph and subtract them to calculate the change. Subtracting the points means to subtract their x and y dimensions respectively. Division of the change in y by the change in x returns the slope:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 164pt;text-indent: 0pt;text-align: left;"><span/></p><p class="s22" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;"><span class="p">Continuing the code, we keep all values of </span><span class="s3">x </span><span class="p">in a single-dimensional NumPy array, </span>x<span class="p">, and all results in a single-dimensional array, </span>y<span class="p">. To perform the same operation, we’ll take </span>x[<span style=" color: #7358A5;">0</span>] <span class="p">and </span>y[<span style=" color: #7358A5;">0</span>] <span class="p">for the first point, then </span>x[<span style=" color: #7358A5;">1</span>] <span class="p">and </span>y[<span style=" color: #7358A5;">1</span>] <span class="p">for the second one. Now we can calculate the slope between them:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>((y[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">-</span>y[<span style=" color: #7358A5;">0</span>]) <span style=" color: #C71F43;">/ </span>(x[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">-</span>x[<span style=" color: #7358A5;">0</span>]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It is not surprising that the slope of this line is 2. We could say the measure of the impact that <i>x </i>has on <i>y </i>is 2. We can calculate the slope in the same way for any linear function, including linear functions that aren’t as obvious.</p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><span class="p">What about a nonlinear function like </span>f(x)=2x<span class="s41">2</span> <span class="p">?</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">f</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 3pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #7358A5;">2</span>*<span style=" color: #231F20;">x</span>**<span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">This function creates a graph that does not form a straight line:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 74pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><b>Fig 7.02: </b>Approximation of the parabolic function y=2x<span class="s40">2</span> graphed</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Can we measure the slope of this curve? Depending on which 2 points we choose to use, we will measure varying slopes:</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 28pt;text-align: left;">y <span style=" color: #C71F43;">= </span>f(x) <span style=" color: #A5A4A5;"># Calculate function outputs for new function </span><span style=" color: #32A7BD;">print</span>(x)</p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;line-height: 11pt;text-align: left;">print<span style=" color: #231F20;">(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 1 2 3 4</span>]</p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[ <span style=" color: #7358A5;">0 2 8 18 32</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now for the first pair of points:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>((y[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">-</span>y[<span style=" color: #7358A5;">0</span>]) <span style=" color: #C71F43;">/ </span>(x[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">-</span>x[<span style=" color: #7358A5;">0</span>]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And for another one:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>((y[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">-</span>y[<span style=" color: #7358A5;">2</span>]) <span style=" color: #C71F43;">/ </span>(x[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">-</span>x[<span style=" color: #7358A5;">2</span>]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">10</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 71pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.03: <span class="p">Approximation of the parabolic function&#39;s example tangents</span></h3><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><a name="bookmark138"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bro" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.03: </a><a href="https://nnfs.io/bro" class="s9" target="_blank">https://nnfs.io/bro</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">How might we measure the impact that x has on y in this nonlinear function? Calculus proposes that we measure the slope of the <b>tangent line </b>at <i>x </i>(for a specific input value to the function), giving us the <b>instantaneous slope </b>(slope at this point)<b>, </b>which is the <b>derivative</b><i>. </i>The <b>tangent line </b>is created by drawing a line between two points that are “infinitely close” on a curve, but this curve has to be differentiable at the derivation point. This means that it has to be continuous and smooth (we cannot calculate the slope at something that we could describe as a “sharp corner,” since it contains an infinite number of slopes). Then, because this is a curve, there is no single slope. Slope depends on where we measure it. To give an immediate example, we can approximate a derivative of the function at <i>x </i>by using this point and another one also taken at x, but with</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">a very small delta added to it, such as <i>0.0001</i>. This number is a common choice as it does not introduce too large an error (when estimating the derivative) or cause the whole expression to be numerically unstable (Δ<i>x </i>might round to 0 due to floating-point number resolution). This lets us perform the same calculation for the slope as before, but on two points that are very close to each other, resulting in a good approximation of a slope at x:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">p2_delta <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.0001</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">x1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x2 <span style=" color: #C71F43;">= </span>x1 <span style=" color: #C71F43;">+ </span>p2_delta <span style=" color: #A5A4A5;"># add delta</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">y1 <span style=" color: #C71F43;">= </span>f(x1) <span style=" color: #A5A4A5;"># result at the derivation point </span>y2 <span style=" color: #C71F43;">= </span>f(x2) <span style=" color: #A5A4A5;"># result at the other, close point</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">approximate_derivative <span style=" color: #C71F43;">= </span>(y2<span style=" color: #C71F43;">-</span>y1)<span style=" color: #C71F43;">/</span>(x2<span style=" color: #C71F43;">-</span>x1) <span style=" color: #32A7BD;">print</span>(approximate_derivative)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 124%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">4.0001999999987845</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">As we will soon learn, the derivative of </span>2x<span class="s41">2</span> <span class="p">at </span>x=1 <span class="p">should be exactly </span>4<span class="p">. The difference we see (~4.0002) comes from the method used to compute the tangent. We chose a delta small enough to</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark130">approximate the derivative as accurately as possible but large enough to prevent a rounding error. To elaborate, an infinitely small delta value will approximate an accurate derivative; however, the delta value needs to be numerically stable, meaning, our delta can not surpass the limitations of Python’s floating-point precision (can’t be too small as it might be rounded to </a><i>0 </i>and, as we know, dividing by <i>0 </i>is “illegal”). Our solution is, therefore, restricted between estimating the derivative and remaining numerically stable, thus introducing this small but visible error.<a name="bookmark139">&zwnj;</a><a name="bookmark140">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The Numerical Derivative</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This method of calculating the derivative is called <b>numerical differentiation </b>— calculating the slope of the tangent line using two <i>infinitely </i>close points, or as with the code solution —</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">calculating the slope of a tangent line made from two points that were “sufficiently close.” We can visualize why we perform this on two close points with the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 127pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 9pt;padding-left: 27pt;text-indent: 0pt;text-align: left;">Fig 7.04: <span class="p">Why we want to use 2 points that are sufficiently close — large delta inaccuracy.</span></h3><p style="padding-left: 127pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 7.05: <span class="p">Why we want to use 2 points that are sufficiently close — very small delta accuracy.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/cat" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.04-7.05: </a><a href="https://nnfs.io/cat" class="s9" target="_blank">https://nnfs.io/cat</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can see that the closer these two points are to each other, the more correct the tangent line appears to be.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">Continuing with <b>numerical differentiation</b>, let us visualize the tangent lines and how they change depending on where we calculate them. To begin, we’ll make the graph of this function more granular using Numpy’s <span class="s22">arange()</span><i>, </i>allowing us to plot with smaller steps. The <span class="s22">np.arange() </span>function takes in <i>start, stop, </i>and <i>step </i>parameters, allowing us to take fractions of a step, such as</p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">0.001 <span class="p">at a time:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">f</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #7358A5;">2</span>*<span style=" color: #231F20;">x</span>**<span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># np.arange(start, stop, step) to give us smoother line </span>x <span style=" color: #C71F43;">= </span>np.arange(<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">0.001</span>)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">y <span style=" color: #C71F43;">= </span>f(x)</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 233%;text-align: left;">plt.plot(x, y) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 144pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><b>Fig 7.06: </b>Matplotlib output that you should see from graphing y=2x<span class="s40">2</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">To draw these tangent lines, we will derive the function for the tangent line at a point and plot the tangent on the graph at this point. The function for a straight line is <i>y = mx+b</i>. Where <i>m </i>is the slope or the <i>approximatex_derivative </i>that we’ve already calculated. And <i>x </i>is the input which leaves <i>b</i>, or the y-intercept, for us to calculate. The slope remains unchanged, but currently, you can “move” the line up or down using the y-intercept. We already know <i>x </i>and <i>m</i>, but <i>b </i>is still unknown. Let’s assume <i>m=1 </i>for the purpose of the figure and see what exactly it means:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 116pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 7pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.07: <span class="p">Various biases graphed where slope = 1.</span></h3><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/but" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.07: </a><a href="https://nnfs.io/but" class="s9" target="_blank">https://nnfs.io/but</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">To calculate <i>b, </i>the formula is <i>b = y - mx</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far we’ve used two points — the point that we want to calculate the derivative at and the “close enough” to it point to calculate the approximation of the derivative. Now, given the above equation for <i>b</i>, the approximation of the derivative and the same “close enough” point (its <i>x </i>and <i>y </i>coordinates to be specific), we can substitute them in the equation and get the y-intercept for the tangent line at the derivation point. Using code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>y2 <span style=" color: #C71F43;">- </span>approximate_derivative<span style=" color: #C71F43;">*</span>x2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Putting everything together:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">f</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #7358A5;">2</span>*<span style=" color: #231F20;">x</span>**<span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># np.arange(start, stop, step) to give us smoother line </span>x <span style=" color: #C71F43;">= </span>np.arange(<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">0.001</span>)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">y <span style=" color: #C71F43;">= </span>f(x) plt.plot(x, y)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># The point and the &quot;close enough&quot; point <span style=" color: #231F20;">p2_delta </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.0001</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">x1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x2 <span style=" color: #C71F43;">= </span>x1<span style=" color: #C71F43;">+</span>p2_delta</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;">y1 <span style=" color: #C71F43;">= </span>f(x1) y2 <span style=" color: #C71F43;">= </span>f(x2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">((x1, y1), (x2, y2))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative approximation and y-intercept for the tangent line </span>approximate_derivative <span style=" color: #C71F43;">= </span>(y2<span style=" color: #C71F43;">-</span>y1)<span style=" color: #C71F43;">/</span>(x2<span style=" color: #C71F43;">-</span>x1)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">b <span style=" color: #C71F43;">= </span>y2 <span style=" color: #C71F43;">- </span>approximate_derivative<span style=" color: #C71F43;">*</span>x2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># We put the tangent line calculation into a function so we can call # it multiple times for different values of x</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># approximate_derivative and b are constant for given function # thus calculated once above this function</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">tangent_line</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">approximate_derivative</span>*<span style=" color: #231F20;">x </span>+ <span style=" color: #231F20;">b</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># plotting the tangent line</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># +/- 0.9 to draw the tangent line on our graph</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 109%;text-align: left;"># then we calculate the y for given x using the tangent line function # Matplotlib will draw a line for us through these points</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">to_plot <span style=" color: #C71F43;">= </span>[x1<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.9</span>, x1, x1<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">0.9</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">plt.plot(to_plot, [tangent_line(i) <span style=" color: #C71F43;">for </span>i <span style=" color: #C71F43;">in </span>to_plot])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Approximate derivative for f(x)&#39;</span>,</p><p class="s38" style="padding-top: 1pt;padding-left: 19pt;text-indent: 36pt;line-height: 217%;text-align: left;"><span class="s31">f</span>&#39;where x = <span style=" color: #231F20;">{x1} </span>is <span style=" color: #231F20;">{approximate_derivative}</span>&#39;<span style=" color: #231F20;">) plt.show()</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">8</span>) (<span style=" color: #7358A5;">2.0001</span>, <span style=" color: #7358A5;">8.000800020000002</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Approximate derivative for f(x) where x = 2 is 8.000199999998785</p><p style="padding-left: 116pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.08: <span class="p">Graphed approximate derivative for f(x) where x=2</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The orange line is the approximate tangent line at <i>x=2 </i>for the function <i>f(x) = 2x</i><i>2</i>. Why do we care about this? You will soon find that we care only about the <i>slope </i>of this tangent line but both visualizing and understanding the <b>tangent line </b>are very important. We care about the slope of the tangent line because it informs us about the <i>impact </i>that <i>x </i>has on this function at a particular point, referred to as the <b>instantaneous rate of change</b>. We will use this concept to determine the effect of a specific weight or bias on the overall loss function given a sample. For now, with different values for <i>x</i>, we can observe resulting impacts on the function. We can continue the previous code to see the tangent line for various inputs (<i>x) </i>- we put a part of the code in a loop over example <i>x </i>values and plot multiple tangent lines:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">f</span>(<span class="s23">x</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #7358A5;">2</span>*<span style=" color: #231F20;">x</span>**<span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #A5A4A5;"># np.arange(start, stop, step) to give us a smoother curve </span>x <span style=" color: #C71F43;">= </span>np.array(np.arange(<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">0.001</span>))</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 217%;text-align: left;">y <span style=" color: #C71F43;">= </span>f(x) plt.plot(x, y)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">colors <span style=" color: #C71F43;">= </span>[<span style=" color: #8F8633;">&#39;k&#39;</span>, <span style=" color: #8F8633;">&#39;g&#39;</span>, <span style=" color: #8F8633;">&#39;r&#39;</span>, <span style=" color: #8F8633;">&#39;b&#39;</span>, <span style=" color: #8F8633;">&#39;c&#39;</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">approximate_tangent_line</span>(<span class="s23">x</span>, <span class="s23">approximate_derivative</span>): <span style=" color: #C71F43;">return </span>(approximate_derivative<span style=" color: #C71F43;">*</span>x) <span style=" color: #C71F43;">+ </span>b</p><p class="s10" style="padding-top: 2pt;padding-left: 44pt;text-indent: -24pt;line-height: 109%;text-align: left;"><span style=" color: #C71F43;">for </span>i <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">5</span>): p2_delta <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.0001 </span>x1 <span style=" color: #C71F43;">= </span>i</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 13pt;text-align: left;">x2 <span style=" color: #C71F43;">= </span>x1<span style=" color: #C71F43;">+</span>p2_delta</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;">y1 <span style=" color: #C71F43;">= </span>f(x1) y2 <span style=" color: #C71F43;">= </span>f(x2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;"><span style=" color: #32A7BD;">print</span>((x1, y1), (x2, y2)) approximate_derivative <span style=" color: #C71F43;">= </span>(y2<span style=" color: #C71F43;">-</span>y1)<span style=" color: #C71F43;">/</span>(x2<span style=" color: #C71F43;">-</span>x1) b <span style=" color: #C71F43;">= </span>y2<span style=" color: #C71F43;">-</span>(approximate_derivative<span style=" color: #C71F43;">*</span>x2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">to_plot <span style=" color: #C71F43;">= </span>[x1<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.9</span>, x1, x1<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">0.9</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 109%;text-align: left;">plt.scatter(x1, y1, <span class="s23">c</span><span style=" color: #C71F43;">=</span>colors[i]) plt.plot([point <span style=" color: #C71F43;">for </span>point <span style=" color: #C71F43;">in </span>to_plot],</p><p class="s10" style="padding-left: 122pt;text-indent: -24pt;line-height: 109%;text-align: left;">[approximate_tangent_line(point, approximate_derivative) <span style=" color: #C71F43;">for </span>point <span style=" color: #C71F43;">in </span>to_plot],</p><p class="s23" style="padding-left: 98pt;text-indent: 0pt;line-height: 13pt;text-align: left;">c<span class="s11">=</span><span class="s10">colors[i])</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Approximate derivative for f(x)&#39;</span>,</p><p class="s38" style="padding-top: 1pt;padding-left: 80pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;where x = <span style=" color: #231F20;">{x1} </span>is <span style=" color: #231F20;">{approximate_derivative}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.401pt" cellspacing="0"><tr style="height:27pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">plt.show()</p></td><td style="width:192pt" colspan="5" rowspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:28pt"><td style="width:230pt"><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>) (<span style=" color: #7358A5;">0.0001</span>, <span style=" color: #7358A5;">2e-08</span>)</p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Approximate derivative for f(x) where</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">=</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">0</p></td><td style="width:18pt"><p class="s45" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">is</p></td><td style="width:138pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00019999999999999998</p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>) (<span style=" color: #7358A5;">1.0001</span>, <span style=" color: #7358A5;">2.00040002</span>)</p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:18pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Approximate derivative for f(x) where</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">=</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">1</p></td><td style="width:18pt"><p class="s45" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">is</p></td><td style="width:138pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4.0001999999987845</p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">8</span>) (<span style=" color: #7358A5;">2.0001</span>, <span style=" color: #7358A5;">8.000800020000002</span>)</p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:18pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Approximate derivative for f(x) where</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">=</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">2</p></td><td style="width:18pt"><p class="s45" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">is</p></td><td style="width:138pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">8.000199999998785</p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">18</span>) (<span style=" color: #7358A5;">3.0001</span>, <span style=" color: #7358A5;">18.001200020000002</span>)</p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:18pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Approximate derivative for f(x) where</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">=</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">3</p></td><td style="width:18pt"><p class="s45" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">is</p></td><td style="width:138pt"><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">12.000199999998785</p></td></tr><tr style="height:14pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<span style=" color: #7358A5;">4</span>, <span style=" color: #7358A5;">32</span>) (<span style=" color: #7358A5;">4.0001</span>, <span style=" color: #7358A5;">32.00160002</span>)</p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:12pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:18pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:138pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:230pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">Approximate derivative for f(x) where</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">x</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">=</p></td><td style="width:12pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: center;">4</p></td><td style="width:18pt"><p class="s45" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">is</p></td><td style="width:138pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">16.000200000016548</p></td></tr></table><p style="padding-left: 112pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.09: <span class="p">Derivative calculated at various points.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">For this simple function, <i>f(x) = 2x</i><i>2</i>, we didn’t pay a high penalty by approximating the derivative (i.e., the slope of the tangent line) like this, and received a value that was close enough for our needs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The problem is that the <i>actual </i>function employed in our neural network is not so simple. The loss function contains all of the layers, weights, and biases — it’s an absolutely massive function operating in multiple dimensions! Calculating derivatives using <b>numerical differentiation</b></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">requires multiple forward passes for a single parameter update (we’ll talk about parameter updates in chapter 10). We need to perform the forward pass as a reference, then update a single parameter by the delta value and perform the forward pass through our model again to see the change of</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">the loss value. Next, we need to calculate the <b>derivative </b>and revert the parameter change that we made for this calculation. We have to repeat this for every weight and bias and for every</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">sample, which will be very time-consuming. We can also think of this method as brute-forcing the derivative calculations. To reiterate, as we quickly covered many terms, the <b>derivative </b>is the <b>slope </b>of the <b>tangent line </b>for a function that takes a single parameter as an input. We’ll use this ability to calculate the slopes of the loss function at each of the weight and bias points — this brings us to the multivariate function, which is a function that takes multiple parameters and is a topic for the next chapter — the partial derivative.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark131">The Analytical Derivative</a><a name="bookmark141">&zwnj;</a><a name="bookmark142">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we have a better idea of what a derivative <i>is</i>, how to calculate the numerical (also called universal) derivative, and why it’s not a good approach for us, we can move on to the <b>Analytical Derivative</b>, the actual solution to the derivative that we’ll implement in our code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">In mathematics, there are two general ways to solve problems: <b>numerical </b>and <b>analytical </b>methods. Numerical solution methods involve coming up with a number to find a solution, like the above approach with <span class="s20">approximate_derivative</span>. The numerical solution is also an approximation. On the other hand, the analytical method offers the exact and much quicker, in</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">terms of calculation, solution. However, identifying the analytical solution for the derivative of a given function, as we’ll quickly learn, will vary in complexity, whereas the numerical approach never gets more complicated — it’s always calling the method twice with two inputs to calculate the approximate derivative at a point. Some analytical solutions are quite obvious, some can be calculated with simple rules, and some complex functions can be broken down into simpler parts and calculated using the so-called <b>chain rule</b>. We can leverage already-proven derivative solutions for certain functions, and others — like our loss function — can be solved with combinations of the above.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">To compute the derivative of functions using the analytical method, we can split them into simple, elemental functions, finding the derivatives of those and then applying the <b>chain rule</b>, which we will explain soon, to get the full derivative. To start building an intuition, let’s start with simple functions and their respective derivatives.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">The derivative of a simple constant function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 205pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 9pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.10: <span class="p">Derivative of a constant function — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><a name="bookmark143"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/cow" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.10: </a><a href="https://nnfs.io/cow" class="s9" target="_blank">https://nnfs.io/cow</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">When calculating the derivative of a function, recall that the derivative can be interpreted as a slope. In this example, the result of this function is a horizontal line as the output value for any x is 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">By looking at it, it becomes evident that the derivative equals 0 since there’s no change from one value of x to any other value of x (i.e., there’s no slope).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far, we are calculating derivatives of the functions by taking a single parameter, <i>x </i>in our case, in each example. This changes with partial derivatives since they take functions with multiple parameters, and we’ll be calculating the derivative with respect to only one of them at a time. For now, with derivatives, it’s always with respect to a single parameter. To denote the derivative, we can use prime notation, where, for the function <i>f(x), </i>we add a prime (&#39;) like f<i>&#39;(x)</i>. For our example, <i>f(x) = 1</i>, the derivative <i>f&#39;(x) = 0</i>. Another notation we can use is called the Leibniz’s notation</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">— the dependence on the prime notation and multiple ways of writing the derivative with the Leibniz’s notation is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Each of these notations has the same meaning — the derivative of a function (with respect to <i>x</i>).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the following examples, we use both notations, since sometimes it’s convenient to use one notation or another. We can also use both of them in a single equation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">In summary: the derivative of a constant function equals <i>0</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of a linear function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 186pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.11: <span class="p">Derivative of a linear function — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/tob" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.11: </a><a href="https://nnfs.io/tob" class="s9" target="_blank">https://nnfs.io/tob</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, the derivative is 1, and the intuition behind this is that for every change of x, y changes by the same amount, so y changes one times the x.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">The derivative of the linear function equals <i>1 </i>(but not in every case, which we’ll explain next):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">What if we try 2x, which is also a linear function?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 195pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.12: <span class="p">Derivative of another linear function — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/pop" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.12: </a><a href="https://nnfs.io/pop" class="s9" target="_blank">https://nnfs.io/pop</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">When calculating the derivative, we can take any constant that function is multiplied by and move it outside of the derivative — in this case it’s 2 multiplied by the derivative of x. Since we already determined that the derivative of <i>f(x) = x </i>was <i>1</i>, we now multiply it by <i>2 </i>to give us the result.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">The derivative of a linear function equals the slope, <i>m. </i>In this case <i>m = 2</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you associate this with numerical differentiation, you’re absolutely right — we already concluded that the derivative of a linear function equals its slope:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">m, <span class="p">in this case, is a constant, no different than the value </span>2, <span class="p">as it’s not a parameter — every non- parameter to the function can’t change its value; thus, we consider it to be a constant. We have just found a simpler way to calculate the derivative of a linear function and also generalized it for the equations of different slopes, </span>m<span class="p">. It’s also an exact derivative, not an approximation, as with the numerical differentiation.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">What happens when we introduce exponents to the function?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 199pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 7.13: <span class="p">Derivative of quadratic function — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/rok" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.13: </a><a href="https://nnfs.io/rok" class="s9" target="_blank">https://nnfs.io/rok</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, we are applying the rule of a constant — we can move the coefficient (the value that multiplies the other value) outside of the derivative. The rule for handling exponents is as follows: take the exponent, in this case a <i>2</i>, and use it as a coefficient for the derived value, then, subtract 1 from the exponent, as seen here: <i>2 - 1 = 1</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">If </span>f(x) = 3x<span class="s41">2</span> <span class="p">then </span>f&#39;(x) = 3·2x<span class="s41">1</span> <span class="p">or simply </span>6x.<span class="p">This means the slope of the tangent line, at any point, </span>x<span class="p">, for this quadratic function, will be </span>6x<span class="p">. As discussed with the numerical solution of the quadratic function differentiation, the derivative of a quadratic function depends on the </span>x <span class="p">and in this case it equals </span>6x<span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A commonly used operator in functions is addition, how do we calculate the derivative in this case?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 165pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 7.14: <span class="p">Derivative of quadratic function with addition — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/mob" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.14: </a><a href="https://nnfs.io/mob" class="s9" target="_blank">https://nnfs.io/mob</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The derivative of a sum operation is the sum of derivatives, so we can split the derivative of a more complex sum operation into a sum of the derivatives of each term of the equation and solve the rest of the derivative using methods we already know.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">The derivative of a sum of functions equals their derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">In this case, we’ve shown the rule using both notations.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s try a couple more examples:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 137pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">Fig 7.15: <span class="p">Analytical derivative of multi-dimensional function example — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/tom" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.15: </a><a href="https://nnfs.io/tom" class="s9" target="_blank">https://nnfs.io/tom</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The derivative of a constant 5 equals 0, as we already discussed at the beginning of this chapter. We also have to apply the other rules that we’ve learned so far to perform this calculation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 80pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 32pt;text-indent: 0pt;line-height: 130%;text-align: center;">Fig 7.16: <span class="p">Analytical derivative of another multi-dimensional function example — calculation steps.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/sun" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 7.16: </a><a href="https://nnfs.io/sun" class="s9" target="_blank">https://nnfs.io/sun</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This looks relatively straight-forward so far, but, with neural networks, we’ll work with functions that take multiple parameters as inputs, so we’re going to calculate the partial derivatives as well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark132">Summary</a><a name="bookmark144">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s summarize some of the solutions and rules that we have learned in this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s46" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Solutions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The derivative of a constant equals 0 (m is a constant in this case, as it’s not a parameter that we are deriving with respect to, which is <i>x </i>in this example):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of <i>x </i>equals 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of a linear function equals its slope:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s46" style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Rules:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of a constant multiple of the function equals the constant multiple of the function’s</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of a sum of functions equals the sum of their derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The same concept applies to subtraction:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of an exponentiation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We used the value x instead of the whole function f(x) here since the derivative of an entire function is calculated a bit differently. We’ll explain this concept along with the chain rule in the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Since we’ve already learned what derivatives are and how to calculate them analytically, which we’ll later implement in code, we can go a step further and cover partial derivatives in the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch7" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch7<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark145">Chapter 8</a><a name="bookmark153">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;line-height: 130%;text-align: left;">Gradients, Partial Derivatives, and the Chain Rule</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Two of the last pieces of the puzzle, before we continue coding our neural network, are the related concepts of <b>gradients </b>and <b>partial derivatives</b>. The derivatives that we’ve solved so far have been cases where there is only one independent variable in the function — that is, the result depended solely on, in our case, <i>x</i>. However, our neural network consists, for example, of neurons, which have multiple inputs. Each input gets multiplied by the corresponding weight (a function of 2 parameters), and they get summed with the bias (a function of as many parameters as there are inputs, plus one for a bias). As we’ll explain soon in detail, to learn the impact of all of the inputs, weights, and biases to the neuron output and at the end of the loss function, we need to calculate the derivative of each operation performed during the forward pass in the neuron and the whole model. To do that and get answers, we’ll need to use the <b>chain rule</b>, which we’ll explain soon in this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a name="bookmark146">The Partial Derivative</a><a name="bookmark154">&zwnj;</a><a name="bookmark155">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">The <b>partial derivative </b>measures how much impact a single input has on a function’s output. The method for calculating a partial derivative is the same as for derivatives explained in the previous chapter; we simply have to repeat this process for each of the independent inputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Each of the function’s inputs has some impact on this function’s output, even if the impact is 0. We need to know these impacts; this means that we have to calculate the derivative with respect to each input separately to learn about each of them. That’s why we call these partial derivatives with respect to given input — we are calculating a partial of the derivative, related to a singular input. The partial derivative is a single equation, and the full multivariate function’s derivative consists of a set of equations called the <b>gradient</b>. In other words, the <b>gradient </b>is a vector of the size of inputs containing partial derivative solutions with respect to each of the inputs. We’ll get back to gradients shortly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To denote the partial derivative, we’ll be using Euler’s notation. It’s very similar to Leibniz’s notation, as we only need to replace the differential operator <i>d </i>with <span class="s47">∂</span>. While the <i>d </i>operator might be used to denote the differentiation of a multivariate function, its meaning is a bit different — it can mean the rate of the function’s change in relation to the given input, but when other inputs might change as well, and it is used mostly in physics. We are interested in the partial derivatives, a situation where we try to find the impact of the given input to the output while treating all of the other inputs as constants. We are interested in the impact of singular inputs since our goal, in the model, is to update parameters. The ∂ operator means explicitly that — the partial derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark147">The Partial Derivative of a Sum</a><a name="bookmark156">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Calculating the partial derivative with respect to a given input means to calculate it like the regular derivative of one input, just while treating other inputs as constants. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, we applied the sum rule — the derivative of a sum is the sum of derivatives. Then, we already know that the derivative of <i>x </i>with respect to <i>x </i>equals <i>1</i>. The new thing is the derivative of <i>y </i>with respect to <i>x</i>. As we mentioned, <i>y </i>is treated as a constant, as it does not change when we are deriving with respect to <i>x</i>, and the derivative of a constant equals <i>0</i>. In the second case, we derived with respect to <i>y</i>, thus treating <i>x </i>as constant. Put another way, regardless of the value of y in this example, the slope of <i>x </i>does not depend on <i>y</i>. This will not always be the case, though, as we will soon see.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s try another example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this example, we also applied the sum rule first, then moved constants to the outside of the derivatives and calculated what remained with respect to <i>x </i>and <i>y </i>individually. The only difference to the non-multivariate derivatives from the previous chapter is the “partial” part, which means we</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">are deriving with respect to each of the variables separately. Other than that, there is nothing new here.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Let’s try something seemingly more complicated:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Pretty straight-forward — we’re constantly applying the same rules over and over again, and we did not add any new calculation or rules in this example.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark148">The Partial Derivative of Multiplication</a><a name="bookmark157">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Before we move on, let’s introduce the partial derivative of multiplication operation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have already mentioned that we need to treat the other independent variables as constants, and we also have learned that we can move constants to the outside of the derivative. That’s exactly how we solve the calculation of the partial derivative of multiplication — we treat other variables as constants, like numbers, and move them outside of the derivative. It turns out that when</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">we derive with respect to <i>x</i>, <i>y </i>is treated as a constant, and the result equals <i>y </i>multiplied by the derivative of <i>x </i>with respect to <i>x, </i>which is <i>1</i>. The whole derivative then results with <i>y</i>. The intuition behind this example is when calculating the partial derivative with respect to <i>x</i>, every change of <i>x </i>by <i>1 </i>changes the function’s output by <i>y</i>. For example, if <i>y=3 </i>and <i>x=1</i>, the result is <i>1·3=3</i>. When we change <i>x </i>by <i>1 </i>so <i>y=3 </i>and <i>x=2</i>, the result is <i>2·3=6</i>. We changed <i>x </i>by <i>1 </i>and the result changed by <i>3</i>, by the <i>y</i>. That’s what the partial derivative of this function with respect to <i>x </i>tells us.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s introduce a third input variable and add multiplication of variables for another example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The only new operation here is, as mentioned, moving variables other than the one that we derive with respect to, outside of the derivative. The results in this example appear more complicated, but only because of the existence of other variables in them — variables that are treated as constants during derivation. Equations of the derivatives are longer, but not necessarily more complicated.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The reason to learn about partial derivatives is we’ll be calculating the partial derivatives of multivariate functions soon, an example of which is the neuron. From the code perspective and the <i>Dense </i>layer class, more specifically, the <i>forward </i>method of this class, we’re passing in a single variable — the input array, containing either a batch of samples or outputs from the previous layer. From the math perspective, each value of this single variable (an array) is a separate input — it contains as many inputs as we have data in the input array. For example, if we pass a vector of <i>4 </i>values to the neuron, it’s a singular variable in the code, but <i>4 </i>separate inputs in the equation. This forms a function that takes multiple inputs. To learn about the impact that each input makes to the function’s output, we’ll need to calculate the partial derivative of this function with respect to each of its inputs, which we’ll explain in detail in the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark149">The Partial Derivative of </a><i>Max</i><a name="bookmark158">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Derivatives and partial derivatives are not limited to addition and multiplication operations, or constants. We need to derive them for the other functions that we used in the forward pass, one of which is the derivative of the <i>max() </i>function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The max function returns the greatest input. We know that the derivative of <i>x </i>with respect to x equals <i>1, </i>so the derivative of this function with respect to <i>x </i>equals 1 if <i>x </i>is greater than <i>y</i>, since the function will return <i>x</i>. In the other case, where <i>y </i>is greater than <i>x </i>and will get returned instead, the derivative of <i>max() </i>with respect to <i>x </i>equals 0 — we treat <i>y </i>as a constant, and the derivative of <i>y </i>with respect to <i>x </i>equals 0. We can denote that as <i>1(x &gt; y)</i>, which means <i>1 </i>if the condition is met, and <i>0 </i>otherwise. We could also calculate the partial derivative of <i>max() </i>with respect to <i>y</i>, but we won’t need it anywhere in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">One special case for the derivative of the <i>max() </i>function is when we have only one variable parameter, and the other parameter is always constant at <i>0</i>. This means that we want whichever is bigger in return — <i>0 </i>or the input value, effectively clipping the input value at <i>0 </i>from the positive side. Handling this is going to be useful when we calculate the derivative of the <b>ReLU </b>activation function since that activation function is defined as <i>max(x, 0)</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: justify;">Notice that since this function takes a single parameter, we used the <i>d </i>operator instead of the <span class="s47">∂ </span>to calculate the non-partial derivative. In this case, the derivative is <i>1 </i>when <i>x </i>is greater than <i>0, </i>otherwise, it’s <i>0</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark150">The Gradient</a><a name="bookmark159">&zwnj;</a><a name="bookmark160">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we mentioned at the beginning of this chapter, the gradient is a <b>vector </b>composed of all of the partial derivatives of a function, calculated with respect to each input variable.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s return to one of the partial derivatives of the sum operation that we calculated earlier:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If we calculate all of the partial derivatives, we can form a gradient of the function. Using different notations, it looks as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 122%;text-align: left;">That’s all we have to know abxout the <b>gradient </b>- it’s a vector of all of the possible partial derivatives of the function, and we denote it using the <span class="s47">∇ </span>— nabla symbol that looks like an inverted delta symbol.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll be using <b>derivatives </b>of single-parameter functions and <b>gradients </b>of multivariate functions to perform <b>gradient descent </b>using the <b>chain rule, </b>or, in other words, to perform the <b>backward pass</b>, which is a part of the model training. How exactly we’ll do that is the subject of the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark151">The Chain Rule</a><a name="bookmark161">&zwnj;</a><a name="bookmark162">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">During the forward pass, we’re passing the data through the neurons, then through the activation function, then through the neurons in the next layer, then through another activation function, and so on. We’re calling a function with an input parameter, taking an output, and using that output as an input to another function. For this simple example, let’s take 2 functions: <i>f </i>and <i>g</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">x is the input data, z is an output of the function f, but also an input for the function g, and y is an output of the function g. We could write the same calculation as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this form, we do not use the intermediate z variable, showing that function g takes the output of function f directly as an input. This does not differ much from the above 2 equations but shows an important property of functions chained this way — since x is an input to the function f and then the output of the function f is an input to the function g, the output of the function g is influenced by x in some way, so there must exist a derivative which can inform us of this influence.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The forward pass through our model is a chain of functions similar to these examples. We are passing in samples, the data flows through all of the layers, and activation functions to form an output. Let’s bring the equation and the code of the example model from chapter 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-left: 117pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 8.01: <span class="p">Code for a forward pass of an example neural network model.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you look closely, you’ll see that we are presenting the loss as a big function, or a chain of functions, of multiple inputs — input data, weights, and biases. We are passing input data to the first layer where we also have that layer’s weights and biases, then the outputs flow through the ReLU activation function, and another layer, which brings more weights and biases, and another ReLU activation, up to the end — the output layer and softmax activation. The model output, along with the targets, is passed to the loss function, which returns the model’s error. We can look at the loss function not only as a function that takes the model’s output and targets as parameters to produce the error, but also as a function that takes targets, samples, and all of the weights and biases as inputs if we chain all of the functions performed during the forward pass as we’ve just shown in the images. To improve loss, we need to learn how each weight and bias impacts it. How to do that for a chain of functions? By using the chain rule. This rule says that the derivative of a function chain is a product of derivatives of all of the functions in this chain, for example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, we wrote the derivative of the outer function, <i>f(g(x))</i>, with respect to the inner function, <i>g(x)</i>, as this inner function is its parameter. Next, we multiplied it by the derivative of the inner function, <i>g(x)</i>, with respect to its parameters, <i>x</i>. We also denoted this derivative using 2 different notations. With 3 functions and multiple inputs, the partial derivative of this function with respect to <i>x </i>is as follows (we can’t use the prime notation in this case since we have to mention which variable we are deriving with respect to):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate the partial derivative of a chain of functions with respect to some parameter, we take the partial derivative of the outer function with respect to the inner function in a chain to the</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">parameter. Then multiply this partial derivative by the partial derivative of the inner function with respect to the more inner function in a chain to the parameter, then multiply this by the partial derivative of the more inner function with respect to the other function in the chain. We repeat this all the way down to the parameter in question. Notice, for example, how the middle derivative is with respect to <i>h(x, z) </i>and not <i>y </i>as <i>h(x, z) </i>is in the chain to the parameter <i>x</i>. The <b>chain rule </b>turns out to be the most important rule in finding the impact of singular input to the output of a chain of functions, which is the calculation of loss in our case. We’ll use it again in the next chapter when we discuss and code backpropagation. For now, let’s cover an example of the chain rule.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">Let’s solve the derivative of </span>h(x) = 3(2x<span class="s41">2</span>)<span class="s41">5</span><span class="p">. The first thing that we can notice here is that we have a complex function that can be split into two simpler functions. First is an equation part</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">contained inside the parentheses, which we can write as <i>g(x) = 2x</i><i>2</i>. That’s the inside function that we exponentiate and multiply with the rest of the equation. The remaining part of the equation can then be written as <i>f(y) = 3(y)</i><i>5</i>. <i>y </i>in this case is what we denoted as <i>g(x)=2x</i><i>2</i><i> </i>and when we combine it back, we get <i>h(x) = f(g(x)) = 3(2x</i><i>2</i><i>)</i><i>5</i><i> </i>. To calculate a derivative of this function, we start by taking that outside exponent, the <span class="s40">5</span>, and place it in front of the component that we are exponentiating to multiply it later by the leading 3, giving us 15. We then subtract 1 from the <span class="s40">5</span> exponent, leaving us with a <span class="s40">4</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Then the chain rule informs us to multiply the above derivative of the outer function, with the derivative of the interior function, giving us:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Recall that <i>4x </i>was the derivative of <i>2x</i><i>2</i>, which is the inner function, <i>g(x)</i>. This highlights the <b>chain rule </b>concept in an example, allowing us to calculate the derivatives of more complex functions</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">by chaining together the derivatives. Note that we multiplied by the derivative of that interior function, but left the interior function <i>unchanged </i>within the derivative of the outer function<i>.</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">In theory, we could just stop here with a perfectly-usable derivative of the function. We can enter some input into </span>15(2x<span class="s41">2</span>)<span class="s41">4</span> · 4x <span class="p">and get the answer. That said, we can also go ahead and simplify this function for more practice. Coming back to the original problem, so far we’ve found:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span class="p">To simplify this derivative function, we first take </span>(2x<span class="s41">2</span>)<span class="s41">4</span> <span class="p">and distribute the </span><span class="s40">4</span><span class="p"> exponent:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Combine the <i>x’s</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And the constants:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll simplify derivatives later as well for faster computation — there’s no reason to repeat the same operations when we can solve them in advance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Hopefully, now you understand what derivatives and partial derivatives are, what the gradient is, what the derivative of the loss function with respect to weights and biases means, and how to use the chain rule. For now, these terms might sound disconnected, but we’re going to use them all to perform gradient descent in the backpropagation step, which is the subject of the next chapters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark152">Summary</a><a name="bookmark163">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s summarize the rules that we have learned in this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The partial derivative of the sum with respect to any input equals 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The partial derivative of the multiplication operation with 2 inputs, with respect to any input, equals the other input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The partial derivative of the max function of 2 variables with respect to any of them is 1 if this variable is the biggest and 0 otherwise. An example of x:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The derivative of the max function of a single variable and 0 equals 1 if the variable is greater than 0 and 0 otherwise:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The derivative of chained functions equals the product of the partial derivatives of the subsequent functions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The same applies to the partial derivatives. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The gradient is a vector of all possible partial derivatives. An example of a triple-input function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch8" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch8<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark164">Chapter 9</a><a name="bookmark172">&zwnj;</a><a name="bookmark173">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Backpropagation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we have an idea of how to measure the impact of variables on a function’s output, we can begin to write the code to calculate these partial derivatives to see their role in minimizing the model’s loss. Before applying this to a complete neural network, let’s start with a simplified forward pass with just one neuron. Rather than backpropagating from the loss function for a full neural network, let’s backpropagate the ReLU function for a single neuron and act as if we intend to minimize the output for this single neuron. We’re first doing this only as a demonstration to simplify the explanation, since minimizing the output from a ReLU activated neuron doesn’t serve any purpose other than as an exercise. Minimizing the loss value is our end goal, but in</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">this case, we’ll start by showing how we can leverage the chain rule with derivatives and partial derivatives to calculate the impact of each variable on the ReLU activated output. We’ll also start by minimizing this more basic output before jumping to the full network and overall loss.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Let’s quickly recall the forward pass and atomic operations that we need to perform for this single neuron and ReLU activation. We’ll use an example neuron with 3 inputs, which means that it also has 3 weights and a bias:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 118%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;"><span class="p">We then start with the first input, </span>x[<span style=" color: #7358A5;">0</span>]<span class="p">, and the related weight, </span>w[<span style=" color: #7358A5;">0</span>]<span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 209pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.01: <span class="p">Beginning a forward pass with the first input and weight.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">We have to multiply the input by the weight:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>] <span style=" color: #32A7BD;">print</span>(xw0)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s11" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">3.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.02: <span class="p">The first input and weight multiplication.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We repeat this operation for <span class="s22">x1</span>, <span class="s22">w1 </span>and <span class="s22">x2</span>, <span class="s22">w2 </span>pairs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>] <span style=" color: #32A7BD;">print</span>(xw1, xw2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">2.0 6.0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Visually:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.03: <span class="p">Input and weight multiplication of all of the inputs.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 11pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Code all together:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>] <span style=" color: #32A7BD;">print</span>(xw0, xw1, xw2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s11" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">3.0 2.0 6.0</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The next operation to perform is a sum of all weighted inputs with a bias:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>] <span style=" color: #32A7BD;">print</span>(xw0, xw1, xw2, b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(z)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s11" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">3.0 2.0 6.0 1.0</span></p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">6.0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.04: <span class="p">Weighted inputs and bias addition.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This forms the neuron’s output. The last step is to apply the ReLU activation function on this output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>] <span style=" color: #32A7BD;">print</span>(xw0, xw1, xw2, b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(z)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s11" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">3.0 2.0 6.0 1.0</span></p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">6.0</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">6.0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.05: <span class="p">ReLU activation applied to the neuron output.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is the full forward pass through a single neuron and a ReLU activation function. Let’s treat all of these chained functions as one big function which takes input values (<i>x</i>), weights (<i>w</i>), and bias (<i>b</i>), as inputs, and outputs <i>y</i>. This big function consists of multiple simpler functions — there is a multiplication of input values and weights, sum of these values and bias, as well as a <i>max </i>function as the ReLU activation — 3 chained functions in total:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The first step is to backpropagate our gradients by calculating derivatives and partial derivatives with respect to each of our parameters and inputs. To do this, we’re going to use the <b>chain rule</b>. Recall that the chain rule for a function stipulates that the derivative for nested functions like <i>f(g(x)) </i>solves to:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This big function that we just mentioned can be, in the context of our neural network, loosely interpreted as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Or in the form that matches code more precisely as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">Our current task is to calculate how much each of the inputs, weights, and a bias impacts the output. We’ll start by considering what we need to calculate for the partial derivative of <i>w</i><span class="s39">0</span>, for example. But first, let’s rewrite our equation to the form that will allow us to determine how to calculate the derivatives more easily:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;text-align: left;">y = ReLU(sum(mul(x<span class="s39">0</span>, w<span class="s39">0</span>), mul(x<span class="s39">1</span>, w<span class="s39">1</span>), mul(x<span class="s39">2</span>, w<span class="s39">2</span>), b))</p><p class="s3" style="padding-top: 13pt;padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="p">The above equation contains 3 nested functions: </span>ReLU<span class="p">, a sum of weighted inputs and a bias, and multiplications of the inputs and weights. To calculate the impact of the example weight, </span>w<span class="s39">0</span><span class="p">, on the output, the chain rule tells us to calculate the derivative of </span>ReLU <span class="p">with respect to its parameter, which is the sum, then multiply it with the partial derivative of the sum operation with respect to its </span>mul(x<span class="s39">0</span>, w<span class="s39">0</span>) <span class="p">input, as this input contains the parameter in question. Then, multiply this with the partial derivative of the multiplication operation with respect to the </span>x<span class="s39">0 </span><span class="p">input. Let’s see this in a simplified equation:</span></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 16pt;text-align: left;">For legibility, we did not denote the <i>ReLU</i>() parameter, which is the full sum, and the sum parameters, which are all of the multiplications of inputs and weights. We excluded this because the equation would be longer and harder to read. This equation shows that we have to calculate the derivatives and partial derivatives of all of the atomic operations and multiply them to acquire the impact that x<span class="s49">0 </span>makes on the output. We can then repeat this to calculate all of the other remaining impacts. The derivatives with respect to the weights and a bias will inform us about their impact and will be used to update these weights and bias. The derivatives with respect to inputs are used to chain more layers by passing them to the previous function in the chain.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll have multiple chained layers of neurons in the neural network model, followed by the loss function. We want to know the impact of a given weight or bias on the loss. That means that we will have to calculate the derivative of the loss function (which we’ll do later in this chapter) and apply the chain rule with the derivatives of all activation functions and neurons in all of the consecutive layers. The derivative with respect to the layer’s inputs, as opposed to the derivative</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">with respect to the weights and biases, is not used to update any parameters. Instead, it is used to chain to another layer (which is why we backpropagate to the previous layer in a chain).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">During the backward pass, we’ll calculate the derivative of the loss function, and use it to multiply with the derivative of the activation function of the output layer, then use this result to multiply</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">by the derivative of the output layer, and so on, through all of the hidden layers and activation functions. Inside these layers, the derivative with respect to the weights and biases will form the gradients that we’ll use to update the weights and biases. The derivatives with respect to inputs will form the gradient to chain with the previous layer, This layer can calculate the impact of its weights and biases on the loss and backpropagate gradients on inputs further.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">For this example, let’s assume that our neuron receives a gradient of <i>1 </i>from the next layer. We’re making up this value for demonstration purposes, and a value of <i>1 </i>won’t change the values, which means that we can more easily show all of the processes. We are going to use the color of red for derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.06: <span class="p">Initial gradient (received during backpropagation).</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Recall that the derivative of <i>ReLU() </i>with respect to its input is <i>1, </i>if the input is greater than <i>0, </i>and</p><p class="s3" style="padding-top: 2pt;padding-bottom: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">0 <span class="p">otherwise:</span></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can write that in Python as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">drelu_dz </span>= <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1. </span>if <span style=" color: #231F20;">z </span>&gt; <span style=" color: #7358A5;">0 </span>else <span style=" color: #7358A5;">0.</span><span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">Where the <span class="s22">drelu_dz </span>means the derivative of the <i>ReLU </i>function with respect to <i>z </i>— we used <i>z </i>instead of <i>x </i>from the equation since the equation denotes the <i>max </i>function in general, and we are applying it to the neuron’s output, which is <i>z</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The input value to the <i>ReLU </i>function is <i>6</i>, so the derivative equals <i>1</i>. We have to use the chain rule and multiply this derivative with the derivative received from the next layer, which is <i>1 </i>for the purpose of this example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The derivative from the next layer <span style=" color: #231F20;">dvalue </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative of ReLU and the chain rule </span>drelu_dz <span style=" color: #C71F43;">= </span>dvalue <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">if </span>z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0 </span><span style=" color: #C71F43;">else </span><span style=" color: #7358A5;">0.</span>) <span style=" color: #32A7BD;">print</span>(drelu_dz)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.07: <span class="p">Derivative of the ReLU function and chain rule.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">This results with the derivative of <i>1</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.08: <span class="p">ReLU and chain rule gradient.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Moving backward through our neural network, what is the function that comes immediately before we perform the activation function?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It’s a sum of the weighted inputs and bias. This means that we — want to calculate the partial derivative of the sum function, and then, using the chain rule, multiply this by the partial derivative of the subsequent, outer, function, which is <i>ReLU</i>. We’ll call these results the:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l2"><li><p style="padding-left: 14pt;text-indent: -7pt;line-height: 16pt;text-align: left;"><span class="s22">drelu_dxw0 </span>— the partial <b>d</b>erivative of the <b>ReLU </b>w.r.t. the first weighed input, <i><b>w</b></i><span class="s51">0</span><i><b>x</b></i><span class="s51">0</span>,</p></li><li><p style="padding-left: 14pt;text-indent: -7pt;line-height: 16pt;text-align: left;"><span class="s22">drelu_dxw1 </span>— the partial <b>d</b>erivative of the <b>ReLU </b>w.r.t. the second weighed input, <i><b>w</b></i><span class="s51">1</span><i><b>x</b></i><span class="s51">1</span>,</p></li><li><p style="padding-left: 14pt;text-indent: -7pt;line-height: 16pt;text-align: left;"><span class="s22">drelu_dxw2 </span>— the partial <b>d</b>erivative of the <b>ReLU </b>w.r.t. the third weighed input, <i><b>w</b></i><span class="s51">2</span><i><b>x</b></i><span class="s51">2</span><span class="s22">,</span></p></li><li><p style="padding-left: 7pt;text-indent: 0pt;line-height: 224%;text-align: left;"><span class="s22">drelu_db </span>— the partial <b>d</b>erivative of the <b>ReLU </b>with respect to the bias, <i><b>b</b></i>. The partial derivative of the sum operation is always <i>1</i>, no matter the inputs:</p></li></ul><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The weighted inputs and bias are summed at this stage. So we will calculate the partial derivatives of the sum operation with respect to each of these, multiplied by the partial derivative for the subsequent function (using the chain rule), which is the <i>ReLU </i>function, denoted by <span class="s22">drelu_dz</span><i>.</i></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">For the first partial derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dsum_dxw0 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dxw0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: justify;">To be clear, the <span class="s22">dsum_dxw0 </span>above means the partial <b>d</b>erivative of the <b>sum </b>with respect to the <b>x </b>(input), <b>w</b>eighted, for the <b>0</b>th pair of inputs and weights. <i>1 </i>is the value of this partial derivative, which we multiply, using the chain rule, with the derivative of the subsequent function, which is the <i>ReLU </i>function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Again, we have to apply the chain rule and multiply the derivative of the ReLU function with the partial derivative of the sum, with respect to the first weighted input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The derivative from the next layer <span style=" color: #231F20;">dvalue </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative of ReLU and the chain rule </span>drelu_dz <span style=" color: #C71F43;">= </span>dvalue <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">if </span>z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0 </span><span style=" color: #C71F43;">else </span><span style=" color: #7358A5;">0.</span>) <span style=" color: #32A7BD;">print</span>(drelu_dz)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Partial derivatives of the multiplication, the chain rule <span style=" color: #231F20;">dsum_dxw0 </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 12pt;text-align: left;">drelu_dxw0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0</p><p class="s12" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(drelu_dxw0)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.0</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">1.0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 23pt;text-indent: 0pt;text-align: left;">Fig 9.09: <span class="p">Partial derivative of the sum function w.r.t. the first weighted input; the chain rule.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">This results with a partial derivative of 1 again:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 9.10: <span class="p">The sum and chain rule gradient (for the first weighted input).</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We can then perform the same operation with the next weighed input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dsum_dxw1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dxw1 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw1</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;text-align: left;">Fig 9.11: <span class="p">Partial derivative of the sum function w.r.t. the second weighted input; the chain rule.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Which results with the next calculated partial derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 9.12: <span class="p">The sum and chain rule gradient (for the second weighted input).</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And the last weighted input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dsum_dxw2 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dxw2 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">Fig 9.13: <span class="p">Partial derivative of the sum function w.r.t. the third weighted input; the chain rule.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 9.14: <span class="p">The sum and chain rule gradient (for the third weighted input).</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Then the bias:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dsum_db <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">drelu_db <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_db</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 9.15: <span class="p">Partial derivative of the sum function w.r.t. the bias; the chain rule.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.16: <span class="p">The sum and chain rule gradient (for the bias).</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s add these partial derivatives, with the applied chain rule, to our code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The derivative from the next layer <span style=" color: #231F20;">dvalue </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative of ReLU and the chain rule </span>drelu_dz <span style=" color: #C71F43;">= </span>dvalue <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">if </span>z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0 </span><span style=" color: #C71F43;">else </span><span style=" color: #7358A5;">0.</span>) <span style=" color: #32A7BD;">print</span>(drelu_dz)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Partial derivatives of the multiplication, the chain rule <span style=" color: #231F20;">dsum_dxw0 </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dsum_dxw1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dsum_dxw2 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dsum_db <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">drelu_dxw0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0 drelu_dxw1 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw1 drelu_dxw2 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw2 drelu_db <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_db</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: justify;">print<span style=" color: #231F20;">(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.0</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">1.0 1.0 1.0 1.0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Continuing backward, the function that comes before the sum is the multiplication of weights and inputs. The derivative for a product is whatever the input is being multiplied by. Recall:</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark174"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The partial derivative of <i>f </i>with respect to <i>x </i>equals <i>y</i>. The partial derivative of <i>f </i>with respect to <i>y </i>equals <i>x</i>. Following this rule, the partial derivative of the first <i>weighted input </i>with respect to the <i>input </i>equals the <i>weight </i>(the other input of this function). Then, we have to apply the chain rule and multiply this partial derivative with the partial derivative of the subsequent function, which is the sum (we just calculated its partial derivative earlier in this chapter):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dmul_dx0 <span style=" color: #C71F43;">= </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dxw0 <span style=" color: #C71F43;">* </span>dmul_dx0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">This means that we are calculating the partial derivative with respect to the <i>x</i><span class="s39">0 </span>input, the value of which is <i>w</i><span class="s39">0</span>, and we are applying the chain rule with the derivative of the subsequent function, which is <span class="s22">drelu_dxw0</span>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is a good time to point out that, as we apply the chain rule in this way — working backward by taking the <i>ReLU() </i>derivative, taking the summing operation’s derivative, multiplying both, and so on, this is a process called <b>backpropagation </b>using the <b>chain rule</b>. As the name implies, the resulting output function’s gradients are passed back through the neural network, using multiplication of the gradient of subsequent functions from later layers with the current one.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">Let’s add this partial derivative to the code and show it on the chart:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The derivative from the next layer <span style=" color: #231F20;">dvalue </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative of ReLU and the chain rule </span>drelu_dz <span style=" color: #C71F43;">= </span>dvalue <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">if </span>z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0 </span><span style=" color: #C71F43;">else </span><span style=" color: #7358A5;">0.</span>) <span style=" color: #32A7BD;">print</span>(drelu_dz)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Partial derivatives of the multiplication, the chain rule <span style=" color: #231F20;">dsum_dxw0 </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dsum_dxw1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dsum_dxw2 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dsum_db <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">drelu_dxw0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0 drelu_dxw1 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw1 drelu_dxw2 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw2 drelu_db <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_db</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: justify;">print<span style=" color: #231F20;">(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Partial derivatives of the multiplication, the chain rule </span>dmul_dx0 <span style=" color: #C71F43;">= </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dxw0 <span style=" color: #C71F43;">* </span>dmul_dx0 <span style=" color: #32A7BD;">print</span>(drelu_dx0)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:13pt"><td style="width:48pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:48pt" colspan="2" rowspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:48pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.0</p></td></tr><tr style="height:14pt"><td style="width:48pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.0 1.0</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.0</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.0</p></td></tr><tr style="height:13pt"><td style="width:48pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">3.0</span></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 23pt;text-indent: 0pt;text-align: left;">Fig 9.17: <span class="p">Partial derivative of the multiplication function w.r.t. the first input; the chain rule.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 9.18: <span class="p">The multiplication and chain rule gradient (for the first input).</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We perform the same operation for other inputs and weights:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">x <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.0</span>, <span style=" color: #7358A5;">3.0</span>] <span style=" color: #A5A4A5;"># input values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">w <span style=" color: #C71F43;">= </span>[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #A5A4A5;"># weights </span>b <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0 </span><span style=" color: #A5A4A5;"># bias</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Adding weighted inputs and a bias </span>z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The derivative from the next layer <span style=" color: #231F20;">dvalue </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.0</span></p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative of ReLU and the chain rule </span>drelu_dz <span style=" color: #C71F43;">= </span>dvalue <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">if </span>z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0 </span><span style=" color: #C71F43;">else </span><span style=" color: #7358A5;">0.</span>) <span style=" color: #32A7BD;">print</span>(drelu_dz)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Partial derivatives of the multiplication, the chain rule <span style=" color: #231F20;">dsum_dxw0 </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">dsum_dxw1 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dsum_dxw2 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dsum_db <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">drelu_dxw0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0 drelu_dxw1 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw1 drelu_dxw2 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw2 drelu_db <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_db</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: justify;">print<span style=" color: #231F20;">(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Partial derivatives of the multiplication, the chain rule </span>dmul_dx0 <span style=" color: #C71F43;">= </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">dmul_dx1 <span style=" color: #C71F43;">= </span>w[<span style=" color: #7358A5;">1</span>] dmul_dx2 <span style=" color: #C71F43;">= </span>w[<span style=" color: #7358A5;">2</span>] dmul_dw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] dmul_dw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] dmul_dw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>]</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dxw0 <span style=" color: #C71F43;">* </span>dmul_dx0 drelu_dw0 <span style=" color: #C71F43;">= </span>drelu_dxw0 <span style=" color: #C71F43;">* </span>dmul_dw0 drelu_dx1 <span style=" color: #C71F43;">= </span>drelu_dxw1 <span style=" color: #C71F43;">* </span>dmul_dx1 drelu_dw1 <span style=" color: #C71F43;">= </span>drelu_dxw1 <span style=" color: #C71F43;">* </span>dmul_dw1 drelu_dx2 <span style=" color: #C71F43;">= </span>drelu_dxw2 <span style=" color: #C71F43;">* </span>dmul_dx2 drelu_dw2 <span style=" color: #C71F43;">= </span>drelu_dxw2 <span style=" color: #C71F43;">* </span>dmul_dw2</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: justify;">print<span style=" color: #231F20;">(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.0</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">1.0 1.0 1.0 1.0</p><p class="s11" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">3.0 1.0 </span>-<span style=" color: #7358A5;">1.0 </span>-<span style=" color: #7358A5;">2.0 2.0 3.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 52pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 9.19: <span class="p">Complete backpropagation graph.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/pro" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 9.01-9.19: </a><a href="https://nnfs.io/pro" class="s9" target="_blank">https://nnfs.io/pro</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">That’s the complete set of the activated neuron’s partial derivatives with respect to the inputs, weights and a bias.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Recall the equation from the beginning of this chapter:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we have the complete code and we are applying the chain rule from this equation, let’s see what we can optimize in these calculations. We applied the chain rule to calculate the partial</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">derivative of the ReLU activation function with respect to the first input, <i>x</i><span class="s39">0</span>. In our code, let’s take the related lines of the code and simplify them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dxw0 <span style=" color: #C71F43;">* </span>dmul_dx0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">where:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dmul_dx0 <span style=" color: #C71F43;">= </span>w[<span style=" color: #7358A5;">0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dxw0 <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">where:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dxw0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>dsum_dxw0 <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">where:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dsum_dxw0 <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>1 <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span>drelu_dz <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">where:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">drelu_dz </span>= <span style=" color: #231F20;">dvalue </span>* <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1. </span>if <span style=" color: #231F20;">z </span>&gt; <span style=" color: #7358A5;">0 </span>else <span style=" color: #7358A5;">0.</span><span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">drelu_dx0 <span style=" color: #C71F43;">= </span>dvalue <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">if </span>z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0 </span><span style=" color: #C71F43;">else </span><span style=" color: #7358A5;">0.</span>) <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p style="padding-left: 85pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 33pt;text-indent: 0pt;text-align: left;">Fig 9.20: <span class="p">How to apply the chain rule for the partial derivative of ReLU w.r.t. first input</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 54pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 9.21: <span class="p">The chain rule applied for the partial derivative of ReLU w.r.t. first input</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/com" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 9.20-9.21: </a><a href="https://nnfs.io/com" class="s9" target="_blank">https://nnfs.io/com</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">In this equation, starting from the left-hand side, is the derivative calculated in the next layer, with respect to its inputs — this is the gradient backpropagated to the current layer, which is the derivative of the <i>ReLU </i>function, and the partial derivative of the neuron’s function with respect to the <i>x</i><span class="s39">0 </span>input. This is all multiplied by applying the chain rule to calculate the impact of the input to the neuron on the whole function’s output.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The partial derivative of a neuron’s function, with respect to the weight, is the input related to this weight, and, with respect to the input, is the related weight. The partial derivative of the neuron’s function with respect to the bias is always 1. We multiply them with the derivative of the subsequent function (which was <i>1 </i>in this example) to get the final derivatives. We are going to code all of these derivatives in the Dense layer’s class and the ReLU activation class for the backpropagation step.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">All together, the partial derivatives above, combined into a vector, make up our gradients. Our gradients could be represented as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">dx <span style=" color: #C71F43;">= </span>[drelu_dx0, drelu_dx1, drelu_dx2] <span style=" color: #A5A4A5;"># gradients on inputs </span>dw <span style=" color: #C71F43;">= </span>[drelu_dw0, drelu_dw1, drelu_dw2] <span style=" color: #A5A4A5;"># gradients on weights </span>db <span style=" color: #C71F43;">= </span>drelu_db <span style=" color: #A5A4A5;"># gradient on bias...just 1 bias here</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">For this single neuron example, we also won’t need our <span class="s22">dx</span>. With many layers, we will continue backpropagating to preceding layers with the partial derivative with respect to our inputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Continuing the single neuron example, we can now apply these gradients to the weights to hopefully minimize the output. This is typically the purpose of the <b>optimizer </b>(discussed in the following chapter), but we can show a simplified version of this task by directly applying a negative fraction of the gradient to our weights. We apply a negative fraction to this gradient since we want to decrease the final output value, and the gradient shows the direction of the steepest ascent. For example, our current weights and bias are:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(w, b)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.0</span>, <span style=" color: #7358A5;">2.0</span>] <span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can then apply a fraction of the gradients to these values:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">w[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">+= -</span><span style=" color: #7358A5;">0.001 </span><span style=" color: #C71F43;">* </span>dw[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">w[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">+= -</span><span style=" color: #7358A5;">0.001 </span><span style=" color: #C71F43;">* </span>dw[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">w[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">+= -</span><span style=" color: #7358A5;">0.001 </span><span style=" color: #C71F43;">* </span>dw[<span style=" color: #7358A5;">2</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 227%;text-align: left;">b <span style=" color: #C71F43;">+= -</span><span style=" color: #7358A5;">0.001 </span><span style=" color: #C71F43;">* </span>db <span style=" color: #32A7BD;">print</span>(w, b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3.001</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.998</span>, <span style=" color: #7358A5;">1.997</span>] <span style=" color: #7358A5;">0.999</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, we’ve slightly changed the weights and bias in such a way so as to decrease the output somewhat intelligently. We can see the effects of our tweaks on the output by doing another forward pass:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># Multiplying inputs by weights </span>xw0 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 13pt;text-align: left;">xw1 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">1</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">1</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">xw2 <span style=" color: #C71F43;">= </span>x[<span style=" color: #7358A5;">2</span>] <span style=" color: #C71F43;">* </span>w[<span style=" color: #7358A5;">2</span>]</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adding</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">z <span style=" color: #C71F43;">= </span>xw0 <span style=" color: #C71F43;">+ </span>xw1 <span style=" color: #C71F43;">+ </span>xw2 <span style=" color: #C71F43;">+ </span>b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation function </span>y <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">max</span>(z, <span style=" color: #7358A5;">0</span>)</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;line-height: 13pt;text-align: left;">print<span style=" color: #231F20;">(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">5.985</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ve successfully decreased this neuron’s output from 6.000 to 5.985. Note that it does not make sense to decrease the neuron’s output in a real neural network; we were doing this purely as a simpler exercise than the full network. We want to decrease the loss value, which is the last calculation in the chain of calculations during the forward pass, and it’s the first one to calculate the gradient during the backpropagation. We’ve minimized the ReLU output of a single neuron only for the purpose of this example to show that we actually managed to decrease the value of</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">chained functions intelligently using the derivatives, partial derivatives, and chain rule. Now, we’ll apply the one-neuron example to the list of samples and expand it to an entire layer of neurons.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To begin, let’s set a list of 3 samples for input, where each sample consists of 4 features. For this example, our network will consist of a single hidden layer, containing 3 neurons (lists of 3 weight sets and 3 biases). We’re not going to describe the forward pass again, but the backward pass, in this case, needs further explanation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far, we have performed an example backward pass with a single neuron, which received a singular derivative to apply the chain rule. Let’s consider multiple neurons in the following layer. A single neuron of the current layer connects to all of them — they all receive the output of this neuron. What will happen during backpropagation? Each neuron from the next layer will return a partial derivative of its function with respect to this input. The neuron in the current layer will receive a vector consisting of these derivatives. We need this to be a singular value for a singular neuron. To continue backpropagation, we need to sum this vector.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, let’s replace the current singular neuron with a layer of neurons. As opposed to a single neuron, a layer outputs a vector of values instead of a singular value. Each neuron in a layer connects to all of the neurons in the next layer. During backpropagation, each neuron from the current layer will receive a vector of partial derivatives the same way that we described for a single neuron. With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array. We know that we need to perform a sum, but what should we sum and what is the result supposed to be? Each neuron is going to output a gradient of the partial derivatives with respect to all of its inputs, and all neurons will form a list of these vectors. We need to sum along the inputs — the first input to all of the neurons, the second input, and so on. We’ll have to sum columns.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate the partial derivatives with respect to inputs, we need the weights — the partial derivative with respect to the input equals the related weight. This means that the array of partial derivatives with respect to all of the inputs equals the array of weights. Since this array is transposed, we’ll need to sum its rows instead of columns. To apply the chain rule, we need to multiply them by the gradient from the subsequent function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the code to show this, we take the transposed weights, which are the transposed array of the derivatives with respect to inputs, and multiply them by their respective gradients (related to given neurons) to apply the chain rule. Then we sum along with the inputs. Then we calculate the gradient for the next layer in backpropagation. The “next” layer in backpropagation is the previous layer in the order of creation of the model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># for the purpose of this example we&#39;re going to use # a vector of 1s</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">1.</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We have 3 sets of weights - one set for each neuron # we have 4 inputs, thus 4 weights</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># recall that we keep weights transposed </span>weights <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]).T</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Sum weights related to the given input multiplied by # the gradient related to the given neuron</p><p class="s10" style="padding-left: 86pt;text-indent: -66pt;line-height: 108%;text-align: left;">dx0 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>([weights[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">0</span>], weights[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">1</span>], weights[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">2</span>]])</p><p class="s10" style="padding-left: 86pt;text-indent: -66pt;line-height: 108%;text-align: left;">dx1 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>([weights[<span style=" color: #7358A5;">1</span>][<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">0</span>], weights[<span style=" color: #7358A5;">1</span>][<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">1</span>], weights[<span style=" color: #7358A5;">1</span>][<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">2</span>]])</p><p class="s10" style="padding-left: 86pt;text-indent: -66pt;line-height: 108%;text-align: left;">dx2 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>([weights[<span style=" color: #7358A5;">2</span>][<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">0</span>], weights[<span style=" color: #7358A5;">2</span>][<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">1</span>], weights[<span style=" color: #7358A5;">2</span>][<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">2</span>]])</p><p class="s10" style="padding-left: 86pt;text-indent: -66pt;line-height: 108%;text-align: left;">dx3 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>([weights[<span style=" color: #7358A5;">3</span>][<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">0</span>], weights[<span style=" color: #7358A5;">3</span>][<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">1</span>], weights[<span style=" color: #7358A5;">3</span>][<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>][<span style=" color: #7358A5;">2</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">dinputs <span style=" color: #C71F43;">= </span>np.array([dx0, dx1, dx2, dx3]) <span style=" color: #32A7BD;">print</span>(dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">[ </span>0.44 <span style=" color: #C71F43;">-</span>0.38 <span style=" color: #C71F43;">-</span>0.07 1.37<span style=" color: #231F20;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">dinputs <span class="p">is a gradient of the neuron function with respect to inputs.</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We defined the gradient of the subsequent function (dvalues) as a row vector, which we’ll explain shortly. From NumPy’s perspective, and since both weights and dvalues are NumPy arrays, we can simplify the dx0 to dx3 calculation. Since the weights array is formatted so that the rows contain weights related to each input (weights for all neurons for the given input), we can multiply them by the gradient vector directly:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># for the purpose of this example we&#39;re going to use # a vector of 1s, 3 derivatives - one for each neuron </span>dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">1.</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We have 3 sets of weights - one set for each neuron # we have 4 inputs, thus 4 weights</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># recall that we keep weights transposed </span>weights <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]).T</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Sum weights related to the given input multiplied by # the gradient related to the given neuron</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">dx0 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>(weights[<span style=" color: #7358A5;">0</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>]) dx1 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>(weights[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>]) dx2 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>(weights[<span style=" color: #7358A5;">2</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>]) dx3 <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">sum</span>(weights[<span style=" color: #7358A5;">3</span>]<span style=" color: #C71F43;">*</span>dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">dinputs <span style=" color: #C71F43;">= </span>np.array([dx0, dx1, dx2, dx3]) <span style=" color: #32A7BD;">print</span>(dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">[ </span>0.44 <span style=" color: #C71F43;">-</span>0.38 <span style=" color: #C71F43;">-</span>0.07 1.37<span style=" color: #231F20;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">You might already see where we are going with this — the sum of the multiplication of the elements is the dot product. We can achieve the same result by using the <span class="s22">np.dot </span>function. For this to be possible, we need to match the “inner” shapes and decide the first dimension of the result, which is the first dimension of the first parameter. We want the output of this calculation to be of the shape of the gradient from the subsequent function — recall that we have one partial derivative for each neuron and multiply it by the neuron’s partial derivative with respect to its</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">input. We then want to multiply each of these gradients with each of the partial derivatives that are related to this neuron’s inputs, and we already noticed that they are rows. The dot product takes rows from the first argument and columns from the second to perform multiplication and sum; thus, we need to transpose the weights for this calculation:</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># for the purpose of this example we&#39;re going to use # a vector of 1s</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">1.</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We have 3 sets of weights - one set for each neuron # we have 4 inputs, thus 4 weights</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># recall that we keep weights transposed </span>weights <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]).T</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># sum weights of given input</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and multiply by the passed-in gradient for this neuron </span>dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues[<span style=" color: #7358A5;">0</span>], weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">[ </span>0.44 <span style=" color: #C71F43;">-</span>0.38 <span style=" color: #C71F43;">-</span>0.07 1.37<span style=" color: #231F20;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have to account for one more thing — a batch of samples. So far, we have been using a single sample responsible for a single gradient vector that is backpropagated between layers. The row vector that we created for <span class="s22">dvalues </span>is in preparation for a batch of data. With more samples,</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: justify;">the layer will return a list of gradients, which we <i>almost </i>handle correctly for. Let’s replace the singular gradient <span class="s22">dvalues[</span><span class="s21">0</span><span class="s22">] </span>with a full list of gradients, <span class="s22">dvalues</span>, and add more example gradients to this list:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s17" style="padding-top: 1pt;padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># for the purpose of this example we&#39;re going to use # an array of an incremental gradient values</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:60pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">dvalues <span style=" color: #C71F43;">=</span></p></td><td style="width:91pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">np.array([[<span style=" color: #7358A5;">1.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:91pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">2.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:91pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">3.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We have 3 sets of weights - one set for each neuron # we have 4 inputs, thus 4 weights</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># recall that we keep weights transposed </span>weights <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]).T</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># sum weights of given input</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and multiply by the passed-in gradient for this neuron </span>dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, weights.T)</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[[</p></td><td style="width:30pt"><p class="s26" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.44</p></td><td style="width:36pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">0.38</span></p></td><td style="width:39pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">0.07</span></p></td><td style="width:45pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.37<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:30pt"><p class="s26" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.88</p></td><td style="width:36pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.76</span></p></td><td style="width:39pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.14</span></p></td><td style="width:45pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.74<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[</p></td><td style="width:30pt"><p class="s26" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">1.32</p></td><td style="width:36pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">1.14</span></p></td><td style="width:39pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">0.21</span></p></td><td style="width:45pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">4.11<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">Calculating the gradients with respect to weights is very similar, but, in this case, we’re going to be using gradients to update the weights, so we need to match the shape of weights, not inputs. Since the derivative with respect to the weights equals inputs, weights are transposed, so we need to transpose inputs to receive the derivative of the neuron with respect to weights. Then we use these transposed inputs as the first parameter to the dot product — the dot product is going to multiply rows by inputs, where each row, as it is transposed, contains data for a given input for all of the samples, by the columns of <span class="s22">dvalues</span>. These columns are related to the outputs of singular neurons for all of the samples, so the result will contain an array with the shape of the weights, containing the gradients with respect to the inputs, multiplied with the incoming gradient for all of the samples in the batch:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s17" style="padding-top: 1pt;padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># for the purpose of this example we&#39;re going to use # an array of an incremental gradient values</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">2.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">3.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We have 3 sets of inputs - samples </span>inputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>],</p><p class="s10" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2.</span>, <span style=" color: #7358A5;">5.</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 134pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>, <span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.8</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># sum weights of given input</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and multiply by the passed-in gradient for this neuron </span>dweights <span style=" color: #C71F43;">= </span>np.dot(inputs.T, dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(dweights)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[[ <span style=" color: #7358A5;">0.5</span></p></td><td style="width:30pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.5</p></td><td style="width:42pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.5<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">20.1</span></p></td><td style="width:30pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">20.1</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">20.1<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">10.9</span></p></td><td style="width:30pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">10.9</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10.9<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[ <span style=" color: #7358A5;">4.1</span></p></td><td style="width:30pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">4.1</p></td><td style="width:42pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">4.1<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">This output’s shape matches the shape of weights because we summed the inputs for each weight and then multiplied them by the input gradient. <span class="s22">dweights </span>is a gradient of the neuron function with respect to the weights.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">For the biases and derivatives with respect to them, the derivatives come from the sum operation and always equal 1, multiplied by the incoming gradients to apply the chain rule. Since gradients are a list of gradients (a vector of gradients for each neuron for all samples), we just have to sum them with the neurons, column-wise, along axis 0.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s17" style="padding-top: 1pt;padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># for the purpose of this example we&#39;re going to use # an array of an incremental gradient values</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">2.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">3.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># One bias for each neuron</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># biases are the row vector with a shape (1, neurons) </span>biases <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">0.5</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># dbiases - sum values, do this over samples (first axis), keepdims # since this by default will produce a plain list -</p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># we explained this in the chapter 4</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>) <span style=" color: #32A7BD;">print</span>(dbiases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">6. 6. 6.</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s29" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">keepdims <span class="p">lets us keep the gradient as a row vector — recall the shape of biases array.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The last thing to cover here is the derivative of the ReLU function. It equals <i>1 </i>if the input is greater than <i>0 </i>and <i>0 </i>otherwise. The layer passes its outputs through the <i>ReLU() </i>activation during the forward pass. For the backward pass, <i>ReLU() </i>receives a gradient of the same shape. The derivative of the ReLU function will form an array of the same shape, filled with 1 when the related input is greater than 0, and 0 otherwise. To apply the chain rule, we need to multiply this array with the gradients of the following function:</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Example layer output</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">z <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">7</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:145pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>,</p></td><td style="width:84pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2<span style=" color: #231F20;">, </span>3<span style=" color: #231F20;">, </span>4<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">5</span>,</p></td><td style="width:84pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">6<span style=" color: #231F20;">, </span>7<span style=" color: #231F20;">, </span>8<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:145pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">9</span>,</p></td><td style="width:84pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">10<span style=" color: #231F20;">, </span>11<span style=" color: #231F20;">, </span>12<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation&#39;s derivative </span>drelu <span style=" color: #C71F43;">= </span>np.zeros_like(z) drelu[z <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(drelu)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The chain rule </span>drelu <span style=" color: #C71F43;">*= </span>dvalues</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(drelu)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:13pt"><td style="width:21pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:75pt" colspan="4"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:21pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[[<span style=" color: #7358A5;">1</span></p></td><td style="width:15pt" colspan="2"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1</p></td><td style="width:12pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0</p></td><td style="width:48pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">1</span></p></td><td style="width:15pt" colspan="2"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0</p></td><td style="width:12pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0</p></td><td style="width:48pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">0</span></p></td><td style="width:15pt" colspan="2"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1</p></td><td style="width:12pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1</p></td><td style="width:48pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">]]</span></p></td></tr><tr style="height:16pt"><td style="width:21pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">[[</p></td><td style="width:6pt"><p class="s26" style="text-indent: 0pt;text-align: center;">1</p></td><td style="width:21pt"><p class="s26" style="padding-left: 12pt;text-indent: 0pt;text-align: left;">2</p></td><td style="width:24pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;text-align: right;">0</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;text-align: left;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:21pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[</p></td><td style="width:6pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: center;">5</p></td><td style="width:21pt"><p class="s26" style="padding-left: 12pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0</p></td><td style="width:24pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">8<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:21pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;">[</p></td><td style="width:6pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: center;">0</p></td><td style="width:21pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">10</p></td><td style="width:24pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;">11</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">To calculate the ReLU derivative, we created an array filled with zeros. <span class="s22">np.zeros_like </span>is a NumPy function that creates an array filled with zeros, with the shape of the array from its parameter, the <span class="s22">z </span>array in our case, which is an example output of the neuron. Following the <i>ReLU() </i>derivative, we then set the values related to the inputs greater than <u><i>0 </i></u>as 1. We then</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">print this table to see and compare it to the gradients. In the end, we multiply this array with the gradient of the subsequent function and print the result.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">We can now simplify this operation. Since the <i>ReLU() </i>derivative array is filled with 1s, which do not change the values multiplied by them, and 0s that zero the multiplying value, this means that we can take the gradients of the subsequent function and set to 0 all of the values that correspond to the <i>ReLU() </i>input and are equal to or less than 0:</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Example layer output</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">z <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">7</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">3</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:145pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>,</p></td><td style="width:84pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2<span style=" color: #231F20;">, </span>3<span style=" color: #231F20;">, </span>4<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">5</span>,</p></td><td style="width:84pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">6<span style=" color: #231F20;">, </span>7<span style=" color: #231F20;">, </span>8<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:145pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">9</span>,</p></td><td style="width:84pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">10<span style=" color: #231F20;">, </span>11<span style=" color: #231F20;">, </span>12<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation&#39;s derivative # with the chain rule applied </span>drelu <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">drelu[z <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span><span style=" color: #32A7BD;">print</span>(drelu)</p><p class="s11" style="padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:13pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: center;">1</p></td><td style="width:18pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">2</p></td><td style="width:21pt"><p class="s26" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0</p></td><td style="width:26pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:15pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: center;">5</p></td><td style="width:18pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0</p></td><td style="width:21pt"><p class="s26" style="padding-right: 5pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0</p></td><td style="width:26pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">8<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: center;">0</p></td><td style="width:18pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">10</p></td><td style="width:21pt"><p class="s26" style="padding-right: 5pt;text-indent: 0pt;line-height: 12pt;text-align: right;">11</p></td><td style="width:26pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The copy of <span class="s22">dvalues </span>ensures that we don’t modify it during the ReLU derivative calculation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s combine the forward and backward pass of a single neuron with a full layer and batch-based partial derivatives. We’ll minimize ReLU’s output, once again, only for this example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Passed-in gradient from the next layer</p><p class="s17" style="padding-top: 1pt;padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># for the purpose of this example we&#39;re going to use # an array of an incremental gradient values</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">dvalues <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">1.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">2.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">3.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We have 3 sets of inputs - samples </span>inputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">2.5</span>],</p><p class="s10" style="padding-left: 134pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">2.</span>, <span style=" color: #7358A5;">5.</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 134pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.5</span>, <span style=" color: #7358A5;">2.7</span>, <span style=" color: #7358A5;">3.3</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.8</span>]])</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We have 3 sets of weights - one set for each neuron # we have 4 inputs, thus 4 weights</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># recall that we keep weights transposed </span>weights <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]).T</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># One bias for each neuron</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># biases are the row vector with a shape (1, neurons) </span>biases <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">0.5</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer_outputs <span style=" color: #C71F43;">= </span>np.dot(inputs, weights) <span style=" color: #C71F43;">+ </span>biases <span style=" color: #A5A4A5;"># Dense layer </span>relu_outputs <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, layer_outputs) <span style=" color: #A5A4A5;"># ReLU activation</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Let&#39;s optimize and test backpropagation here</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># ReLU activation - simulates derivative with respect to input values # from next layer passed to current layer during backpropagation </span>drelu <span style=" color: #C71F43;">= </span>relu_outputs.copy()</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">drelu[layer_outputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># dinputs - multiply by weights </span>dinputs <span style=" color: #C71F43;">= </span>np.dot(drelu, weights.T) <span style=" color: #A5A4A5;"># dweights - multiply by inputs </span>dweights <span style=" color: #C71F43;">= </span>np.dot(inputs.T, drelu)</p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># dbiases - sum values, do this over samples (first axis), keepdims # since this by default will produce a plain list -</p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># we explained this in the chapter 4</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">dbiases <span style=" color: #C71F43;">= </span>np.sum(drelu, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s11" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">weights </span>+= -<span style=" color: #7358A5;">0.001 </span>* <span style=" color: #231F20;">dweights biases </span>+= -<span style=" color: #7358A5;">0.001 </span>* <span style=" color: #231F20;">dbiases</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:34pt"><td style="width:88pt"><p class="s53" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">print<span style=" color: #231F20;">(weights)</span></p><p class="s53" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(biases)</span></p></td><td style="width:155pt" colspan="2" rowspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:21pt"><td style="width:88pt"><p class="s25" style="padding-top: 7pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p></td></tr><tr style="height:15pt"><td style="width:88pt"><p class="s54" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[[ <span style=" color: #7358A5;">0.179515</span></p></td><td style="width:70pt"><p class="s55" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.5003665</p></td><td style="width:85pt"><p class="s56" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.262746 </span><span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:88pt"><p class="s54" style="padding-right: 12pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[ <span style=" color: #7358A5;">0.742093</span></p></td><td style="width:70pt"><p class="s56" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.9152577</span></p></td><td style="width:85pt"><p class="s56" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.2758402</span><span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:88pt"><p class="s54" style="padding-right: 12pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.510153</span></p></td><td style="width:70pt"><p class="s55" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.2529017</p></td><td style="width:85pt"><p class="s55" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.1629592<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:88pt"><p class="s54" style="padding-right: 12pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[ <span style=" color: #7358A5;">0.971328</span></p></td><td style="width:70pt"><p class="s56" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">-<span style=" color: #7358A5;">0.5021842</span></p></td><td style="width:85pt"><p class="s55" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.8636583<span style=" color: #231F20;">]]</span></p></td></tr></table><p class="s57" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">1.98489 2.997739 0.497389</span>]]</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">In this code, we replaced the plain Python functions with NumPy variants, created example data, calculated the forward and backward passes, and updated the parameters. Now we will update the dense layer and ReLU activation code with a <span class="s35">backward </span>method (for backpropagation), which we’ll call during the backpropagation phase of our model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">neurons</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(inputs, neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">During the <span class="s35">forward </span>method for our <span class="s35">Layer_Dense </span>class, we will want to remember what the inputs were (recall that we’ll need them when calculating the partial derivative with respect to weights during backpropagation), which can be easily implemented using an object property (<span class="s22">self.inputs</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s14" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, we will add our backward pass (backpropagation) code that we developed previously into a new method in the layer class, which we’ll call <span class="s35">backward</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 113%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>) <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 13pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We then do the same for our ReLU class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 113%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 13pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify the original variable, # let&#39;s make a copy of the values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">By this point, we’ve covered everything we need to perform backpropagation, except for the derivative of the Softmax activation function and the derivative of the cross-entropy loss function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark165">Categorical Cross-Entropy loss derivative</a><a name="bookmark175">&zwnj;</a><a name="bookmark176">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 130%;text-align: left;">If you are not interested in the mathematical derivation of the Categorical Cross-Entropy loss, feel free to skip to the code implementation, as derivatives are known for common loss functions, and you won’t necessarily need to know how to solve them. It is a good exercise if you plan to create custom loss functions, though.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">As we learned in chapter 5, the Categorical Cross-Entropy loss function’s formula is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Where <i>L</i><span class="s39">i </span>denotes sample loss value, <i>i </i>— <i>i</i>-th sample in a set, <i>k </i>— index of the target label (ground-true label), <i>y </i>— target values and <i>y-hat </i>— predicted values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This formula is convenient when calculating the loss value itself, as all we need is the output of the Softmax activation function at the index of the correct class. For the purpose of the derivative calculation, we’ll use the full equation mentioned back in chapter 5:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Where <i>L</i><span class="s39">i </span>denotes sample loss value, <i>i </i>— <i>i</i>-th sample in a set, <i>j </i>— label/output index, <i>y </i>— target values and <i>y-hat </i>— predicted values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll use this full function because our current goal is to calculate the gradient, which is composed of the partial derivatives of the loss function with respect to each of its inputs (being the outputs of the Softmax activation function). This means that we cannot use the equation, which takes just the value at the index of the correct class (the first equation above). To calculate partial derivatives with respect to each of the inputs, we need an equation that takes all of them as parameters, thus the choice to use the full equation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">First, let’s define the gradient equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">We defined the equation here as the partial derivative of the loss function with respect to each of its inputs. We already learned that the derivative of the sum equals the sum of the derivatives. We also learned that we can move constants. An example is <i>y</i><span class="s39">i,j</span>, as it is not what we are calculating the derivative with respect to. Let’s apply these transforms:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we have to solve the derivative of the logarithmic function, which is the reciprocal of its parameter, multiplied (using the chain rule) by the partial derivative of this parameter — using prime (also called Lagrange’s) notation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can solve it further (using Leibniz’s notation in this case):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s apply this derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The partial derivative of a value with respect to this value equals 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we are calculating the partial derivative with respect to the <i>y </i>given <i>j</i>, the sum is being performed over a single element and can be omitted:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The derivative of this loss function with respect to its inputs (predicted values at the i-th sample, since we are interested in a gradient with respect to the predicted values) equals the negative ground-truth vector, divided by the vector of the predicted values (which is also the output vector of the softmax function).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 14pt;padding-left: 8pt;text-indent: 0pt;line-height: 87%;text-align: left;"><a name="bookmark166">Categorical Cross-Entropy loss derivative code implementation</a><a name="bookmark177">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we derived this equation and have found that it solves to a simple division operation of 2 values, we know that, with NumPy, we can extend this operation to the sample-wise vectors of ground truth and predicted values, and further to the batch-wise arrays of them. From the coding perspective, we need to add a backward method to the Loss_CategoricalCrossentropy class. We need to pass the array of predictions and the array of true values into it and calculate the negated division of them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Along with the partial derivative calculation, we are performing two additional operations. First, we’re turning numerical labels into one-hot encoded vectors — prior to this, we need to check how many dimensions y_true consists of. If the shape of the labels returns a single dimension</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">(which means that they are shaped like a list and not like an array), they consist of discrete numbers and need to be converted to a list of one-hot encoded vectors — a two-dimensional array. If that’s the case, we need to turn them into one-hot encoded vectors. We’ll use the <span class="s22">np.eye </span>method which, given a number, <i>n</i>, returns an <i>n</i>x<i>n </i>array filled with ones on the diagonal and zeros everywhere else. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-bottom: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 227%;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np <span style=" color: #32A7BD;">print</span>(np.eye(<span style=" color: #7358A5;">5</span>))</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:72pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:108pt" colspan="4"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:15pt"><td style="width:72pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">array([[<span style=" color: #7358A5;">1.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:35pt"><p class="s26" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;line-height: 13pt;text-align: center;">0.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:72pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">[<span style=" color: #7358A5;">0.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:35pt"><p class="s26" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;line-height: 13pt;text-align: center;">0.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:72pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">[<span style=" color: #7358A5;">0.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:35pt"><p class="s26" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;line-height: 13pt;text-align: center;">0.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:72pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">[<span style=" color: #7358A5;">0.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 13pt;text-align: left;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:35pt"><p class="s26" style="padding-left: 2pt;padding-right: 7pt;text-indent: 0pt;line-height: 13pt;text-align: center;">0.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:72pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:35pt"><p class="s26" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can then index this table with the numerical label to get the one-hot encoded vector that represents it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.eye(<span style=" color: #7358A5;">5</span>)[<span style=" color: #7358A5;">1</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([<span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">1.</span>, <span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">0.</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.eye(<span style=" color: #7358A5;">5</span>)[<span style=" color: #7358A5;">4</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([<span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">0.</span>, <span style=" color: #7358A5;">1.</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">If <i>y_true </i>is already one-hot encoded, we do not perform this step.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The second operation is the gradient normalization. As we’ll learn in the next chapter, optimizers sum all of the gradients related to each weight and bias before multiplying them by the learning rate (or some other factor). What this means, in our case, is that the more samples we have in a dataset, the more gradient sets we’ll receive at this step, and the bigger this sum will become. As a consequence, we’ll have to adjust the learning rate according to each set of samples. To solve this problem, we can divide all of the gradients by the number of samples. A sum of elements divided by a count of them is their mean value (and, as we mentioned, the optimizer will perform the sum) — this way, we’ll effectively normalize the gradients and make their sum’s magnitude invariant to the number of samples.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark167">Softmax activation derivative</a><a name="bookmark178">&zwnj;</a><a name="bookmark179">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The next calculation that we need to perform is the partial derivative of the Softmax function, which is a bit more complicated task than the derivative of the Categorical Cross-Entropy loss. Let’s remind ourselves of the equation of the Softmax activation function and define the derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Where <i>S</i><span class="s39">i,j </span>denotes <i>j</i>-th Softmax’s output of <i>i</i>-th sample, <i>z </i>— input array which is a list of input vectors (output vectors from the previous layer), <i>z</i><span class="s39">i,j </span>— <i>j</i>-th Softmax’s input of <i>i</i>-th sample, <i>L </i>— number of inputs, <i>z</i><span class="s39">i,k </span>— <i>k</i>-th Softmax’s input of <i>i</i>-th sample.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we described in chapter 4, the Softmax function equals the exponentiated input divided by the sum of all exponentiated inputs. In other words, we need to exponentiate all of the values first, then divide each of them by the sum of all of them to perform the normalization. Each input to the Softmax impacts each of the outputs, and we need to calculate the partial derivative of each</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">output with respect to each input. From the programming side of things, if we calculate the impact of one list on the other list, we’ll receive a matrix of values as a result. That’s exactly what we’ll calculate here — we’ll calculate the <b>Jacobian matrix </b>(which we’ll explain later) of the vectors, which we’ll dive deeper into soon.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">To calculate this derivative, we need to first define the derivative of the division operation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In order to calculate the derivative of the division operation, we need to take the derivative of the numerator multiplied by the denominator, subtract the numerator multiplied by the derivative of the denominator from it, and then divide the result by the squared denominator.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We can now start solving the derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s apply the derivative of the division operation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At this step, we have two partial derivatives present in the equation. For the one on the right side of the numerator (right side of the subtraction operator):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">We need to calculate the derivative of the sum of the constant,<i>e </i>(Euler’s number), raised to power <i>z</i><span class="s39">i,l </span>(where <i>l </i>denotes consecutive indices from <i>1 </i>to the number of the Softmax outputs — <i>L</i>) with respect to the <i>z</i><span class="s39">i,k</span>. The derivative of the sum operation is the sum of derivatives, and the derivative of the constant <i>e </i>raised to power <i>n </i>(<i>e</i><i>n</i>) with respect to <i>n </i>equals <i>e</i><i>n</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">It is a special case when the derivative of an exponential function equals this exponential function itself, as its exponent is exactly what we are deriving with respect to, thus its derivative equals <i>1</i>. We also know that the range <i>1...L </i>contains <i>k </i>(<i>k </i>is one of the indices from this range) exactly once and then, in this case, the derivative is going to equal <i>e </i>to the power of the <i>z</i><span class="s39">i,k </span>(as <i>j </i>equals <i>k</i>) and <i>0 </i>otherwise (when <i>j </i>does not equal <i>k </i>as <i>z</i><span class="s39">i,l </span>won’t contain <i>z</i><span class="s39">i,k </span>and will be treated as a constant — The derivative of the constant equals <i>0</i>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">The derivative on the left side of the subtraction operator in the denominator is a slightly different case:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: justify;">It does not contain the sum over all of the elements like the derivative we solved moments ago, so it can become either <i>0 </i>if <i>j≠k </i>or <i>e </i>to the power of the <i>z</i><span class="s39">i,j </span>if <i>j=k</i>. That means, starting from this step, we need to calculate the derivatives separately for both cases. Let’s start with <i>j=k</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">In the case of <i>j=k</i>, the derivative on the left side is going to equal <i>e </i>to the power of the <i>z</i><span class="s39">i,j </span>and the derivative on the right solves to the same value in both cases. Let’s substitute them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 16pt;text-align: justify;">The numerator contains the constant <i>e </i>to the power of <i>z</i><span class="s39">i,j </span>in both the minuend (the value we are subtracting from) and subtrahend (the value we are subtracting from the minuend) of the subtraction operation. Because of this, we can regroup the numerator to contain this value</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">multiplied by the subtraction of their current multipliers. We can also write the denominator as a multiplication of the value instead of using the power of <i>2</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Then let’s split the whole equation into 2 parts:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We moved <i>e </i>from the numerator and the sum from the denominator to its own fraction, and the content of the parentheses in the numerator, and the other sum from the denominator as another fraction, both joined by the multiplication operation. Now we can further split the “right” fraction into two separate fractions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, as it’s a subtraction operation, we separated both values from the numerator, dividing them both by the denominator and applying the subtraction operation between new fractions. If we</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">look closely, the “left” fraction turns into the Softmax function’s equation, as well as the “right” one, with the middle fraction solving to <i>1 </i>as the numerator and the denominator are the same values:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Note that the “left” Softmax function carries the <span class="s59">j </span>parameter, and the “right” one <span class="s59">k </span>— both came from their numerators, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we have to go back and solve the derivative in the case of <i>j≠k</i>. In this case, the “left” derivative of the original equation solves to <i>0 </i>as the whole expression is treated as a constant:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The difference is that now the whole subtrahend solves to <i>0</i>, leaving us with just the minuend in the numerator:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, exactly like before, we can write the denominator as the multiplication of the values instead of using the power of <i>2</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">That lets us to split this fraction into 2 fractions, using the multiplication operation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now both fractions represent the Softmax function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Note that the left Softmax function carries the <span class="s59">j </span>parameter, and the “right” one has <span class="s59">k </span>— both came from their numerators, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">As a summary, the solution of the derivative of the Softmax function with respect to its inputs is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">That’s not the end of the calculation that we can perform here. When left in this form, we’ll have 2 separate equations to code and use in different cases, which isn’t very convenient for the speed of calculations. We can, however, further morph the result of the second case of the derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the first step, we moved the second Softmax along the minus sign into the brackets so we can add a zero inside of them and right before this value. That does not change the solution, but now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Both solutions look very similar, they differ only in a single value. Conveniently, there exists</p><h3 style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Kronecker delta <span class="p">function (which we’ll explain soon) whose equation is:</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can apply it here, simplifying our equation further to:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">That’s the final math solution to the derivative of the Softmax function’s outputs with respect to each of its inputs. To make it a little bit easier to implement in Python using NumPy, let’s transform the equation for the last time:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We basically multiplied <i>S</i><span class="s39">i,j </span>by both sides of the subtraction operation from the parentheses.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 94%;text-align: left;"><a name="bookmark168">Softmax activation derivative code implementation</a><a name="bookmark180">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This lets us code the solution using just two NumPy functions, which we’ll explain now step by step:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s make up a single sample:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">softmax_output <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And shape it as a list of samples:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np softmax_output <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>]</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">softmax_output <span style=" color: #C71F43;">= </span>np.array(softmax_output).reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #32A7BD;">print</span>(softmax_output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([[<span style=" color: #7358A5;">0.7</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.1</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.2</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The left side of the equation is Softmax’s output multiplied by the Kronecker delta. The Kronecker delta equals <i>1 </i>when both inputs are equal, and <i>0 </i>otherwise. If we visualize this as an array, we’ll have an array of zeros with ones on the diagonal — you might remember that we already have implemented such a solution using the <span class="s22">np.eye </span>method:</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.eye(softmax_output.shape[<span style=" color: #7358A5;">0</span>]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:72pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:60pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:72pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">array([[<span style=" color: #7358A5;">1.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:72pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:72pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now we’ll do the multiplication of both of the values from the equation part:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(softmax_output <span style=" color: #C71F43;">* </span>np.eye(softmax_output.shape[<span style=" color: #7358A5;">0</span>]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0. </span>, <span style=" color: #7358A5;">0. </span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0. </span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0. </span>],</p><p class="s10" style="padding-top: 2pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0. </span>, <span style=" color: #7358A5;">0. </span>, <span style=" color: #7358A5;">0.2</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">It turns out that we can gain some speed by replacing this by the <span class="s22">np.diagflat </span>method call, which computes the same solution — the diagflat method creates an array using an input vector as the diagonal:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(np.diagflat(softmax_output))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:78pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:72pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">array([[<span style=" color: #7358A5;">0.7</span>,</p></td><td style="width:30pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0. <span style=" color: #231F20;">,</span></p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 7pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0. <span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0. </span>,</p></td><td style="width:30pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.1<span style=" color: #231F20;">,</span></p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 7pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0. <span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0. </span>,</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0. <span style=" color: #231F20;">,</span></p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.2<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 16pt;text-align: left;">The other part of the equation is <i>S</i><span class="s39">i,j</span><i>S</i><span class="s39">i,k </span>— the multiplication of the Softmax outputs, iterating over the <span class="s59">j </span>and <span class="s59">k </span>indices respectively. Since, for each sample (the <span class="s59">i </span>index), we’ll have to multiply the values from the Softmax function’s output (in all of the combinations), we can use the dot product operation. For this, we’ll just have to transpose the second argument to get its row vector form (as described in chapter 2):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(np.dot(softmax_output, softmax_output.T))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:84pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:84pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">array([[<span style=" color: #7358A5;">0.49</span>,</p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.07<span style=" color: #231F20;">,</span></p></td><td style="width:48pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.14<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:84pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.07</span>,</p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.01<span style=" color: #231F20;">,</span></p></td><td style="width:48pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.02<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.14</span>,</p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.02<span style=" color: #231F20;">,</span></p></td><td style="width:48pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Finally, we can perform the subtraction of both arrays (following the equation):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 56pt;text-indent: -36pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.diagflat(softmax_output) <span style=" color: #C71F43;">- </span>np.dot(softmax_output, softmax_output.T))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:90pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:53pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:90pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">array([[ <span style=" color: #7358A5;">0.21</span>,</p></td><td style="width:43pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.07</span><span style=" color: #231F20;">,</span></p></td><td style="width:53pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.14</span><span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:90pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.07</span>,</p></td><td style="width:43pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.09<span style=" color: #231F20;">,</span></p></td><td style="width:53pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.02</span><span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:90pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.14</span>,</p></td><td style="width:43pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.02</span><span style=" color: #231F20;">,</span></p></td><td style="width:53pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.16<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The matrix result of the equation and the array solution provided by the code is called the <b>Jacobian matrix</b>. In our case, the Jacobian matrix is an array of partial derivatives in all of the combinations of both input vectors. Remember, we are calculating the partial derivatives of every output of the Softmax function with respect to each input separately. We do this because each input influences each output due to the normalization process, which takes the sum of all the exponentiated inputs. The result of this operation, performed on a batch of samples, is a list of the Jacobian matrices, which effectively forms a 3D matrix — you can visualize it as a column whose levels are Jacobian matrices being the sample-wise gradient of the Softmax function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This raises a question — if sample-wise gradients are the Jacobian matrices, how do we perform the chain rule with the gradient back-propagated from the loss function, since it’s a vector for each sample? Also, what do we do with the fact that the previous layer, which is the Dense layer, will expect the gradients to be a 2D array? Currently, we have a 3D array of the partial derivatives</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">— a list of the Jacobian matrices. The derivative of the Softmax function with respect to any of its inputs returns a vector of partial derivatives (a row from the Jacobian matrix), as this input influences all the outputs, thus also influencing the partial derivative for each of them. We need to sum the values from these vectors so that each of the inputs for each of the samples will return a single partial derivative value instead. Because each input influences all of the outputs, the returned vector of the partial derivatives has to be summed up for the final partial derivative with respect to this input. We can perform this operation on each of the Jacobian matrices directly, applying the chain rule at the same time (applying the gradient from the loss function) using <span class="s22">np.dot() </span>— For each sample, it’ll take the row from the Jacobian matrix and multiply it by the</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;">corresponding value from the loss function’s gradient. As a result, the dot product of each of these</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">vectors and values will return a singular value, forming a vector of the partial derivatives sample- wise and a 2D array (a list of the resulting vectors) batch-wise.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s code the solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-top: 1pt;padding-left: 268pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">First, we created an empty array (which will become the resulting gradient array) with the same shape as the gradients that we’re receiving to apply the chain rule. The <span class="s22">np.empty_like </span>method creates an empty and uninitialized array. Uninitialized means that we can expect it to contain meaningless values, but we’ll set all of them shortly anyway, so there’s no need for initialization (for example, with zeros using <span class="s22">np.zeros() </span>instead). In the next step, we’re going to iterate sample-wise over pairs of the outputs and gradients, calculating the partial derivatives as described earlier and calculating the final product (applying the chain rule) of the Jacobian matrix and gradient vector (from the passed-in gradient array), storing the resulting vector as a row in the dinput array. We’re going to store each vector in each row while iterating, forming the output array.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 5pt;padding-left: 8pt;text-indent: 0pt;line-height: 94%;text-align: left;"><a name="bookmark169">Common Categorical Cross-Entropy loss and Softmax activation derivative</a><a name="bookmark181">&zwnj;</a><a name="bookmark182">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At the moment, we have calculated the partial derivatives of the Categorical Cross-Entropy loss and Softmax activation functions, and we can finally use them, but there is still one more step that we can perform to speed the calculations up. Different books and tutorials usually mention the derivative of the loss function with respect to the Softmax inputs, or even weight and biases of the output layer directly and don’t go into the details of the partial derivatives of these functions separately. This is partially because the derivatives of both functions combine to solve a simple equation — the whole code implementation is simpler and faster to execute. When we look at our current code, we perform multiple operations to calculate the gradients and even include a loop in the backward step of the activation function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Let’s apply the chain rule to calculate the partial derivative of the Categorical Cross-Entropy loss function with respect to the Softmax function inputs. First, let’s define this derivative by applying the chain rule:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This partial derivative equals the partial derivative of the loss function with respect to its inputs, multiplied (using the chain rule) by the partial derivative of the activation function with respect to its inputs. Now we need to systematize semantics — we know that the inputs to the loss function, <i>y-hat</i><span class="s39">i,j</span>, are the outputs of the activation function, <i>S</i><span class="s39">i,j</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">That means that we can update the equation to the form of:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we can substitute the equation for the partial derivative of the Categorical Cross-Entropy function, but, since we are calculating the partial derivative with respect to the Softmax inputs, we’ll use the one containing the sum operator over all of the outputs — it will soon become clear why. The derivative:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">After substitution to the combined derivative’s equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, as we calculated before, the partial derivative of the Softmax activation, before applying Kronecker delta to it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s actually do the substitution of the <i>S</i><span class="s39">i,j </span>with <i>y-hat</i><span class="s39">i,j </span>here as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The solution is different depending on if <i>j=k </i>or <i>j≠k</i>. To handle for this situation, we have to split the current partial derivative following these cases — when they both match and when they do not:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">For the <i>j≠k </i>case, we just updated the sum operator to exclude <i>k </i>and that’s the only change:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">For the <i>j=k </i>case, we do not need the sum operator as it will sum only one element, of index <i>k</i>. For</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">the same reason, we also replace <i>j</i>indices with <i>k</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Back to the main equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we can substitute the partial derivatives of the activation function for both cases with the newly-defined solutions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 16pt;text-align: justify;">We can cancel out the <i>y-hat</i><span class="s39">i,k </span>from both sides of the subtraction in the equation — both contain it as part of the multiplication operations and in their denominators. Then on the “right” side of the equation, we can replace 2 minus signs with the plus one and remove the parentheses:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Now let’s multiply the <i>-y</i><span class="s39">i,k </span>with the content of the parentheses on the “left” side of the equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><span class="p">Now let’s look at the sum operation — it adds up </span>y<span class="s39">i,j</span>y-hat<span class="s39">i,k </span><span class="p">over all possible values of index </span>j <span class="p">except for when it equals </span>k<span class="p">. Then, on the left of this part of the equation, we have </span>y<span class="s39">i,k</span>y-hat<span class="s39">i,k</span><span class="p">, which contains </span>y<span class="s39">i,k </span><span class="p">— the exact element that is excluded from the sum. We can then join both expressions:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">Now the sum operator iterates over all of the possible values of <i>j </i>and, since we know that <i>y</i><span class="s39">i,j </span>for each <i>i </i>is the one-hot encoded vector of ground-truth values, the sum of all of its elements equals</p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: justify;">1<span class="p">. In other words, following the earlier explanation in this chapter — this sum will multiply </span>0 <span class="p">by the </span>y-hat<span class="s39">i,k </span><span class="p">except for a single situation, the true label, where it’ll multiply </span>1 <span class="p">by this value. We can then simplify it further to:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">As we can see, when we apply the chain rule to both partial derivatives, the whole equation simplifies significantly to the subtraction of the predicted and ground truth values. It is also multiple times faster to compute.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 14pt;padding-left: 8pt;text-indent: 0pt;line-height: 94%;text-align: justify;"><a name="bookmark170">Common Categorical Cross-Entropy loss and Softmax activation derivative - code implementation</a><a name="bookmark183">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To code this solution, nothing in the forward pass changes — we still need to perform it on the activation function to receive the outputs and then on the loss function to calculate the loss value. For backpropagation, we’ll create the backward step containing the implementation of the new equation, which calculates the combined gradient of the loss and activation functions. We’ll code the solution as a separate class, which initializes both the Softmax activation and the Categorical Cross-Entropy objects, calling their forward methods respectively during the forward pass. Then the new backward pass is going to contain the new code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer&#39;s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">To implement the solution <i>y-hat</i><span class="s39">i,k</span><i>-y</i><span class="s39">i,k</span>, instead of performing the subtraction of the full arrays, we’re taking advantage of the fact that the <i>y </i>being <span class="s22">y_true </span>in the code consists of one-hot encoded vectors, which means that, for each sample, there is only a singular value of <i>1 </i>in these vectors and the remaining positions are filled with zeros.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This means that we can use NumPy to index the prediction array with the sample number and its true value index, subtracting <i>1 </i>from these values. This operation requires discrete true labels instead of one-hot encoded ones, thus the additional code that performs the transformation if needed — If the number of dimensions in the ground-truth array equals <i>2</i>, it means that it’s a</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">matrix consisting of one-hot encoded vectors. We can use <span class="s22">np.argmax()</span>, which returns the index of the maximum value (index for <i>1 </i>in this case), but we need to perform this operation sample- wise to get a vector of indices:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>], [<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>], [<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>]]) <span style=" color: #32A7BD;">print</span>(np.argmax(y_true))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">For the last step, we normalize the gradient in exactly the same way and for the same reason as described along with the Categorical Cross-Entropy gradient normalization.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s summarize the code for each of the classes that we have updated:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 216%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>): <span style=" color: #A5A4A5;"># Number of samples</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true] <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer&#39;s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: justify;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: justify;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>): <span style=" color: #A5A4A5;"># Number of samples</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: justify;">samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can now test if the combined backward step returns the same values compared to when we backpropagate gradients through both of the functions separately. For this example, let’s make up an output of the Softmax function and some target values. Next, let’s backpropagate them using both solutions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">softmax_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class_targets <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">softmax_loss <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy() softmax_loss.backward(softmax_outputs, class_targets)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dvalues1 <span style=" color: #C71F43;">= </span>softmax_loss.dinputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">activation <span style=" color: #C71F43;">= </span>Activation_Softmax() activation.output <span style=" color: #C71F43;">= </span>softmax_outputs loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss.backward(softmax_outputs, class_targets) activation.backward(loss.dinputs)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dvalues2 <span style=" color: #C71F43;">= </span>activation.dinputs</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Gradients: combined loss and activation:&#39;</span>) <span style=" color: #32A7BD;">print</span>(dvalues1)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Gradients: separate loss and activation:&#39;</span>) <span style=" color: #32A7BD;">print</span>(dvalues2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Gradients: combined loss and activation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">[[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.1</span></p></td><td style="width:76pt"><p class="s26" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.03333333</p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.06666667<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[ <span style=" color: #7358A5;">0.03333333</span></p></td><td style="width:76pt"><p class="s25" style="padding-right: 5pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.16666667</span></p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.13333333<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:84pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[ <span style=" color: #7358A5;">0.00666667</span></p></td><td style="width:76pt"><p class="s25" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">-<span style=" color: #7358A5;">0.03333333</span></p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.02666667<span style=" color: #231F20;">]]</span></p></td></tr><tr style="height:30pt"><td style="width:160pt" colspan="2"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">Gradients: separate loss an</p><p class="s24" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.09999999 0.03333334</span></p></td><td style="width:81pt"><p class="s24" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">d activation:</p><p class="s26" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">0.06666667<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[ <span style=" color: #7358A5;">0.03333334</span></p></td><td style="width:76pt"><p class="s25" style="padding-right: 5pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.16666667</span></p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.13333334<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:84pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[ <span style=" color: #7358A5;">0.00666667</span></p></td><td style="width:76pt"><p class="s25" style="padding-right: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;">-<span style=" color: #7358A5;">0.03333333</span></p></td><td style="width:81pt"><p class="s26" style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.02666667<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The results are the same. The small difference between values in both arrays results from the precision of floating-point values in raw Python and NumPy. To answer the question of how many times faster this solution is, we can take advantage of Python’s timeit module, running both solutions multiple times and combining the execution times. A full description of the timeit module and the code used here is outside of the scope of this book, but we include this code purely to show the speed deltas:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">from <span style=" color: #231F20;">timeit </span>import <span style=" color: #231F20;">timeit</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="text-indent: 0pt;text-align: right;">softmax_outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.7</span>, <span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.2</span>],</p><p class="s10" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">[<span style=" color: #7358A5;">0.1</span>, <span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">0.4</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">[<span style=" color: #7358A5;">0.02</span>, <span style=" color: #7358A5;">0.9</span>, <span style=" color: #7358A5;">0.08</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class_targets <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">def <span class="s32">f1</span><span class="s10">():</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">softmax_loss <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy() softmax_loss.backward(softmax_outputs, class_targets)</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">dvalues1 <span style=" color: #C71F43;">= </span>softmax_loss.dinputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">def <span class="s32">f2</span><span class="s10">():</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">activation <span style=" color: #C71F43;">= </span>Activation_Softmax() activation.output <span style=" color: #C71F43;">= </span>softmax_outputs loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss.backward(softmax_outputs, class_targets) activation.backward(loss.dinputs)</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">dvalues2 <span style=" color: #C71F43;">= </span>activation.dinputs</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: justify;">t1 <span style=" color: #C71F43;">= </span>timeit(<span class="s31">lambda</span>: f1(), <span class="s23">number</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">10000</span>) t2 <span style=" color: #C71F43;">= </span>timeit(<span class="s31">lambda</span>: f2(), <span class="s23">number</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">10000</span>) <span style=" color: #32A7BD;">print</span>(t2<span style=" color: #C71F43;">/</span>t1)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">6.922146504409747</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Calculating the gradients separately is about 7 times slower.. This factor can differ from a machine to a machine, but it clearly shows that it was worth putting in additional effort to calculate and code the optimized solution of the combined loss and activation function derivative.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s take the code of the model and initialize the new class of combined accuracy and loss class’ object:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Instead of the previous:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax activation (to be used with Dense layer): </span>activation2 <span style=" color: #C71F43;">= </span>Activation_Softmax()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then replace the forward pass calls over these objects:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p class="s14" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses from output of activation2 (softmax activation) </span>loss <span style=" color: #C71F43;">= </span>loss_function.forward(activation2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">With the forward pass call on the new object:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And finally add the backward step and printing gradients:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print gradients </span>print<span style=" color: #231F20;">(dense1.dweights) </span>print<span style=" color: #231F20;">(dense1.dbiases) </span>print<span style=" color: #231F20;">(dense2.dweights) </span>print<span style=" color: #231F20;">(dense2.dbiases)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s46" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full model code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(loss_activation.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print loss value </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print accuracy </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print gradients </span>print<span style=" color: #231F20;">(dense1.dweights) </span>print<span style=" color: #231F20;">(dense1.dbiases) </span>print<span style=" color: #231F20;">(dense2.dweights) </span>print<span style=" color: #231F20;">(dense2.dbiases)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:13pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[[<span style=" color: #7358A5;">0.33333334</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.33333334</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.33333334<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333316</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333332</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333364<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0.33333287</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.3333329</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333418<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-left: 8pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[<span style=" color: #7358A5;">0.3333326</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.33333263</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33333477<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">0.33333233</span></p></td><td style="width:67pt"><p class="s26" style="padding-left: 2pt;padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.3333324</p></td><td style="width:78pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.33333528<span style=" color: #231F20;">]]</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">loss: <span style=" color: #7358A5;">1.0986104</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">acc: <span style=" color: #7358A5;">0.34</span></p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">[[ </span>1.5766358e-04 7.8368575e-05 4.7324404e-05<span style=" color: #231F20;">] [ </span>1.8161036e-04 1.1045571e-05 <span style=" color: #C71F43;">-</span>3.3096316e-05<span style=" color: #231F20;">]] [[</span><span style=" color: #C71F43;">-</span>3.6055347e-04 9.6611722e-05 <span style=" color: #C71F43;">-</span>1.0367142e-04<span style=" color: #231F20;">]] [[ </span>5.4410957e-05 1.0741142e-04 <span style=" color: #C71F43;">-</span>1.6182236e-04<span style=" color: #231F20;">] [</span><span style=" color: #C71F43;">-</span>4.0791339e-05 <span style=" color: #C71F43;">-</span>7.1678100e-05 1.1246944e-04<span style=" color: #231F20;">]</span></p><p class="s11" style="padding-left: 19pt;text-indent: 6pt;line-height: 118%;text-align: left;"><span style=" color: #231F20;">[</span>-<span style=" color: #7358A5;">5.3011299e-05 8.5817286e-05 </span>-<span style=" color: #7358A5;">3.2805994e-05</span><span style=" color: #231F20;">]] [[</span>-<span style=" color: #7358A5;">1.0732794e-05 </span>-<span style=" color: #7358A5;">9.4590941e-06 2.0027626e-05</span><span style=" color: #231F20;">]]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark171">Full code up to this point:</a><a name="bookmark184">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>): <span style=" color: #A5A4A5;"># Initialize weights and biases</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from input ones, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>) <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let’s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 201pt;text-indent: 0pt;text-align: left;">np.dot(single_output, single_output.T)</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 268pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We’ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer’s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier’s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Let’s see output of the first few samples: </span><span style=" color: #32A7BD;">print</span>(loss_activation.output[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print loss value </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print accuracy </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;line-height: 111%;text-align: left;"><span style=" color: #A5A4A5;"># Print gradients </span>print<span style=" color: #231F20;">(dense1.dweights) </span>print<span style=" color: #231F20;">(dense1.dbiases) </span>print<span style=" color: #231F20;">(dense2.dweights) </span>print<span style=" color: #231F20;">(dense2.dbiases)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At this point, thanks to gradients and backpropagation using the chain rule, we’re able to adjust the weights and biases with the goal of lowering loss, but we’d be doing it in a very rudimentary way. This process of adjusting weights and biases using gradients to decrease loss is the job of the optimizer, which is the subject of the next chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 30pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch9" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch9<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark185">Chapter 10</a><a name="bookmark194">&zwnj;</a><a name="bookmark195">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Optimizers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Once we have calculated the gradient, we can use this information to adjust weights and biases to decrease the measure of loss. In a previous toy example, we showed how we could successfully decrease a neuron’s activation function’s (ReLU) output in this manner. Recall that we subtracted a fraction of the gradient for each weight and bias parameter. While very rudimentary, this is still a commonly used optimizer called <b>Stochastic Gradient Descent (SGD)</b>. As you will soon discover, most optimizers are just variants of SGD.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark186">Stochastic Gradient Descent (SGD)</a><a name="bookmark196">&zwnj;</a><a name="bookmark197">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">There are some naming conventions with this optimizer that can be confusing, so let’s walk through those first. You might hear the following names:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ul id="l3"><li><p style="padding-left: 44pt;text-indent: -18pt;text-align: left;">Stochastic Gradient Descent, SGD</p></li><li><p style="padding-top: 2pt;padding-left: 44pt;text-indent: -18pt;text-align: left;">Vanilla Gradient Descent, Gradient Descent, GD, or Batch Gradient Descent, BGD</p></li><li><p style="padding-top: 2pt;padding-left: 44pt;text-indent: -18pt;text-align: left;">Mini-batch Gradient Descent, MBGD</p></li></ul><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The first name, <b>Stochastic Gradient Descent</b>, historically refers to an optimizer that fits a single sample at a time. The second optimizer, <b>Batch Gradient Descent</b>, is an optimizer used to fit a whole dataset at once. The last optimizer, <b>Mini-batch Gradient Descent</b>, is used to fit slices of a dataset, which we’d call batches in our context. The naming convention can be confusing here for multiple reasons.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, in the context of deep learning and this book, we call slices of data <b>batches</b>, where, historically, the term to refer to slices of data in the context of Stochastic Gradient Descent was <b>mini-batches</b>. In our context, it does not matter if the batch contains a single sample, a slice of the dataset, or the full dataset — as a batch of the data. Additionally, with the current code, we are fitting the full dataset; following this naming convention, we would use <b>Batch Gradient Descent</b>. In a future chapter, we’ll introduce data slices, or <b>batches</b>, so we should start by using the <b>Mini- batch Gradient Descent </b>optimizer. That said, current naming trends and conventions with Stochastic Gradient Descent in use with deep learning today have merged and normalized all of these variants, to the point where we think of the <b>Stochastic Gradient Descent </b>optimizer as one that assumes a batch of data, whether that batch happens to be a single sample, every sample in a dataset, or some subset of the full dataset at a time.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">In the case of Stochastic Gradient Descent, we choose a learning rate, such as <i>1.0</i>. We then subtract the <i>learning_rate · parameter_gradients </i>from the actual parameter values. If our learning rate is 1, then we’re subtracting the exact amount of gradient from our parameters. We’re going to start with 1 to see the results, but we’ll be diving more into the learning rate shortly. Let’s create the SGD optimizer class code. The initialization method will take hyper-parameters, starting with the learning rate, for now, storing them in the class’ properties. The <span class="s35">update_params </span>method, given a layer object, performs the most basic optimization, the same way that we performed it in the previous chapter — it multiplies the gradients stored in the layers by the negated learning rate</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">and adds the result to the layer’s parameters. It seems that, in the previous chapter, we performed SGD optimization without knowing it. The full class so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.0</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.learning_rate <span style=" color: #C71F43;">* </span>layer.dweights layer.biases <span style=" color: #C71F43;">+= -</span>self.learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">To use this, we need to create an optimizer object:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then update our network layer’s parameters after calculating the gradient using:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">optimizer.update_params(dense1) optimizer.update_params(dense2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Recall that the layer object contains its parameters (weights and biases) and also, at this stage, the gradient that is calculated during backpropagation. We store these in the layer’s properties so that the optimizer can make use of them. In our main neural network code, we’d bring the optimization in after backpropagation. Let’s make a 1x64 densely-connected neural network (1 hidden layer with 64 neurons) and use the same dataset as before:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The next step is to create the optimizer’s object:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create optimizer </span>optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then perform a <b>forward pass </b>of our sample data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Let&#39;s print loss value </span><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;loss:&#39;</span>, loss)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;acc:&#39;</span>, accuracy)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we do our <b>backward pass</b>, which is also called <b>backpropagation</b>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 111%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we finally use our optimizer to update weights and biases:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.update_params(dense1) optimizer.update_params(dense2)</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is everything we need to train our model! But why would we only perform this optimization once, when we can perform it lots of times by leveraging Python’s looping capabilities? We will repeatedly perform a forward pass, backward pass, and optimization until we reach some stopping point. Each full pass through all of the training data is called an <b>epoch</b>. In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">to have a perfect model with ideal weights and biases after only one epoch. To add multiple epochs of training into our code, we will initialize our model and run a loop around all the code performing the forward pass, backward pass, and optimization calculations:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create optimizer </span>optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.update_params(dense1) optimizer.update_params(dense2)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This gives us an update of where we are (epochs), the model’s accuracy, and loss every 100 epochs. Initially, we can see consistent improvement:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.360</span>, loss: <span style=" color: #7358A5;">1.099</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.400</span>, loss: <span style=" color: #7358A5;">1.087</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.417</span>, loss: <span style=" color: #7358A5;">1.077</span></p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:27pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p><p class="s26" style="padding-top: 1pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">...</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">1000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.407<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1.058</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.403<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.038</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.447<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.022</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.467<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.023</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.437<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.005</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2400<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.497<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.993</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2500<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.513<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.981</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">9500<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.590<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.865</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">9600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.627<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.863</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">9700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.630<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.830</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">9800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.663<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.844</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">9900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.627<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.820</p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.633</span>, loss: <span style=" color: #7358A5;">0.848</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Additionally, we’ve prepared animations to help visualize the training process and to convey the impact of various optimizers and their hyperparameters. The left part of the animation canvas</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">contains dots, where color represents each of the 3 classes of the data, the coordinates are features, and the background colors show the model prediction areas. Ideally, the points’ colors and the background should match if the model classifies correctly. The surrounding area should also follow the data’s “trend” — which is what we’d call generalization — the ability of the model to correctly predict unseen data. The colorful squares on the right show weights and biases — red</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">for positive and blue for negative values. The matching areas right below the Dense 1 bar and next to the Dense 2 bar show the updates that the optimizer performs to the layers. The updates might look overly strong compared to the weights and biases, but that’s because we’ve visually normalized them to the maximum value, or else they would be almost invisible since the updates are quite small at a time. The other 3 graphs show the loss, accuracy, and current learning rate values in conjunction with the training time, epochs in this case.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.01: <span class="p">Model training with Stochastic Gradient Descent optimizer.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning, there are quick flashing colors in the animation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/pup" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.01: </a><a href="https://nnfs.io/pup" class="s9" target="_blank">https://nnfs.io/pup</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark198">Our neural network mostly stays stuck at around a loss of 1 and later 0.85-0.9, and an accuracy around 0.60. The animation also has a “flashy wiggle” effect, which most likely means we chose too high of a learning rate. Given that loss didn’t decrease much, we can assume that this learning rate, being too high, also caused the model to get stuck in a </a><b>local minimum</b>, which we’ll learn more about soon. Iterating over more epochs doesn’t seem helpful at this point, which tells us that we’re likely stuck with our optimization. Does this mean that this is the most we can get from our optimizer on this dataset?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Recall that we’re adjusting our weights and biases by applying some fraction, in this case, <i>1.0</i>, to the gradient and subtracting this from the weights and biases. This fraction is called the <b>learning rate </b>(LR) and is the primary adjustable parameter for the optimizer as it decreases loss. To gain an intuition for adjusting, planning, or initially setting the learning rate, we should first understand how the learning rate affects the optimizer and output of the loss function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark187">Learning Rate</a><a name="bookmark199">&zwnj;</a><a name="bookmark200">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 17pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far, we have a gradient of a model and the loss function with respect to all of the parameters, and we want to apply a fraction of this gradient to the parameters in order to descend the loss value.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In most cases, we won’t apply the negative gradient as is, as the direction of the function’s steepest descent will be continuously changing, and these values will usually be too big for meaningful model improvements to occur. Instead, we want to perform small steps — calculating the gradient, updating parameters by a negative fraction of this gradient, and repeating this in a loop. Small steps ensure that we are following the direction of the steepest descent, but these steps can also be too small, causing learning stagnation — we’ll explain this shortly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s forget, for a while, that we are performing gradient descent of an n-dimensional function (our loss function), where n is the number parameters (weights and biases) that the model contains, and assume that we have just one dimension to the loss function (a singular input). Our goal for the following images and animations is to visualize some concepts and gain an intuition; thus, we will not use or present certain optimizer settings, and instead will be considering things in more general terms. That said, we’ve used a real SGD optimizer on a real function to prepare all of the following examples. Here’s the function where we want to determine what input to it will result in the lowest possible output:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 73pt;text-indent: 0pt;text-align: left;"><a name="bookmark201"><span/></a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.02: <span class="p">Example function to minimize the output.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can see the <b>global minimum </b>of this function, which is the lowest possible <i>y </i>value that this function can output. This is the goal — to minimize the function’s output to find the global minimum. The values of the axes are not important in this case. The goal is only to show the function and the learning rate concept. Also, remember that this one-dimensional function example is being used merely to aid in visualization. It would be easy to solve this function with simpler math than what is required to solve the much larger n-dimensional loss function for neural networks, where n (which is the number of weights and biases) can be in the millions or even billions (or more). When we have millions of, or more, dimensions, gradient descent is the best- known way to search for a global minimum.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark202">We’ll start descending from the left side of this graph. With an example learning rate:</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.03: <span class="p">Stuck in the first local minimum.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/and" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.03: </a><a href="https://nnfs.io/and" class="s9" target="_blank">https://nnfs.io/and</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The learning rate turned out to be too small. Small updates to the parameters caused stagnation in the model’s learning — the model got stuck in a local minimum. The <b>local minimum </b>is a minimum that is near where we look but isn’t necessarily the global minimum, which is the</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">absolute lowest point for a function. With our example here, as well as with optimizing full neural networks, we do not know where the global minimum is. How do we know if we’ve reached</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">the global minimum or at least gotten close? The loss function measures how far the model is with its predictions to the real target values, so, as long as the loss value is not <i>0 </i>or very close to <i>0</i>, and the model stopped learning, we’re at some local minimum. In reality, we almost never approach a loss of <i>0 </i>for various reasons. One reason for this may be imperfect neural network</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">hyperparameters. Another reason for this may be insufficient data. If you did reach a loss of 0 with a neural network, you should find it suspicious, for reasons we’ll get into later in this book.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We can try to modify the learning rate:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.04: <span class="p">Stuck in the second local minimum.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/xor" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.04: </a><a href="https://nnfs.io/xor" class="s9" target="_blank">https://nnfs.io/xor</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This time, the model escaped this local minimum but got stuck at another one. Let’s see one more example after another learning rate change:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.05: <span class="p">Stuck in the third local minimum, near the global minimum.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/tho" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.05: </a><a href="https://nnfs.io/tho" class="s9" target="_blank">https://nnfs.io/tho</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This time the model got stuck at a local minimum near the global minimum. The model was able to escape the “deeper” local minimums, so it might be counter-intuitive why it is stuck here.</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Remember, the model follows the direction of steepest descent of the loss function, no matter how large or slight the descent is. For this reason, we’ll introduce momentum and the other techniques to prevent such situations.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Momentum, in an optimizer, adds to the gradient what, in the physical world, we could call inertia</p><p style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">— for example, we can throw a ball uphill and, with a small enough hill or big enough applied force, the ball can roll-over to the other side of the hill. Let’s see how this might look with the model in training:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.06: <span class="p">Reached the global minimum, too low learning rate.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/pog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.06: </a><a href="https://nnfs.io/pog" class="s9" target="_blank">https://nnfs.io/pog</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We used a very small learning rate here with a large momentum. The color change from green, through orange to red presents the advancement of the gradient descent process, the steps. We can see that the model achieved the goal and found the global minimum, but this took many steps. Can this be done better?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.07: <span class="p">Reached the global minimum, better learning rate.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/jog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.07: </a><a href="https://nnfs.io/jog" class="s9" target="_blank">https://nnfs.io/jog</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And even further:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 10.08: <span class="p">Reached the global minimum, significantly better learning rate.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/mog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.08: </a><a href="https://nnfs.io/mog" class="s9" target="_blank">https://nnfs.io/mog</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With these examples, we were able to find the global minimum in about 200, 100, and 50 steps, respectively, by modifying the learning rate and the momentum. It’s possible to significantly shorten the training time by adjusting the parameters of the optimizer. However, we have to be careful with these hyper-parameter adjustments, as this won’t necessarily always help the model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 36pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 9pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.09: <span class="p">Unstable model, learning rate too big.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/log" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.09: </a><a href="https://nnfs.io/log" class="s9" target="_blank">https://nnfs.io/log</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">With the learning rate set too high, the model might not be able to find the global minimum. Even, at some point, if it does, further adjustments could cause it to jump out of this minimum. We’ll see this behavior later in this chapter — try to take a close look at results and see if you can find it, as well as the other issues we’ve described, from the different optimizers as we work through them.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, the model was “jumping” around some minimum and what this might mean is that we should try to lower the learning rate, raise the momentum, or possibly apply a learning rate decay (lowering the learning rate during training), which we’ll describe in this chapter. If we set the learning rate far too high:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 9pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.10: <span class="p">Unstable model, learning rate significantly too big.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/sog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.10: </a><a href="https://nnfs.io/sog" class="s9" target="_blank">https://nnfs.io/sog</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark203">In this situation, the model starts “jumping” around, and moves in what we might observe as random directions. This is an example of “overshooting,” with every step — the direction of a change is correct, but the amount of the gradient applied is too large. In an extreme situation, we could cause a </a><b>gradient explosion</b>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.11: <span class="p">Broken model, learning rate critically too big.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.11: </a><a href="https://nnfs.io/bog" class="s9" target="_blank">https://nnfs.io/bog</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A gradient explosion is a situation where the parameter updates cause the function’s output to rise instead of fall, and, with each step, the loss value and gradient become larger. At some point, the floating-point variable limitation causes an overflow as it cannot hold values of this size anymore, and the model is no longer able to train. It’s crucial to recognize this situation forming</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">during training, especially for large models, where the training can take days, weeks, or more. It is possible to tune the model’s hyper-parameters in time to save the model and to continue training.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">When we choose the learning rate and the other hyper-parameters correctly, the learning process can be relatively quick:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.12: <span class="p">Model learned, good learning rate, can be better.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/cog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.12: </a><a href="https://nnfs.io/cog" class="s9" target="_blank">https://nnfs.io/cog</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This time it took significantly less time for the model to find the global minimum, but it can always be better:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.13: <span class="p">An efficient learning example.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/rog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.13: </a><a href="https://nnfs.io/rog" class="s9" target="_blank">https://nnfs.io/rog</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This time the model needed just a few steps to find the global minimum. The challenge is to choose the hyper-parameters correctly, and it is not always an easy task. It is usually best to start with the optimizer defaults, perform a few steps, and observe the training process when tuning different settings. It is not always possible to see meaningful results in a short-enough period</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">of time, and, in this case, it’s good to have the ability to update the optimizer’s settings during training. How you choose the learning rate, and other hyper-parameters, depends on the model, data, including the amount of data, the parameter initialization method, etc. There is no single, best way to set hyper-parameters, but experience usually helps. As we mentioned, one of the challenges during the training of a neural network model is to choose the right settings. The</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 231%;text-align: left;">difference can be anything from a model not learning at all to learning very well. For a summary of learning rates — if we plot the loss along an axis of steps:</p><p style="padding-left: 41pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.14: <span class="p">Graphs of the loss in a function of steps, different rates</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can see various examples of relative learning rates and what loss will ideally look like as a graph over time (steps) of training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Knowing what the learning rate should be to get the most out of your training process isn’t possible, but a good rule is that your initial training will benefit from a larger learning rate to take initial steps faster. If you start with steps that are too small, you might get stuck in a local</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">minimum and be unable to leave it due to not making large enough updates to the parameters. For example, what if we make the learning rate 0.85 rather than 1.0 with the SGD optimizer?</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">.85</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.update_params(dense1) optimizer.update_params(dense2)</span></p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.360</span>, loss: <span style=" color: #7358A5;">1.099</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.403</span>, loss: <span style=" color: #7358A5;">1.091</span></p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2000<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.437<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">1.053</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2100<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.443<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.026</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2200<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.377<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.050</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2300<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.433<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.016</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2400<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.460<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.000</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2500<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.493<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.010</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2600<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.527<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.998</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2700<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.523<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.977</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7100<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.577<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.941</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7200<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.550<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.921</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7300<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.593<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.943</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7400<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.593<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.940</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7500<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.557<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.907</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7600<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.590<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.949</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7700<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.590<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.935</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9100<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.597<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.860</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9200<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.630<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.842</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">10000<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.657<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.816</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 10.15: <span class="p">Model training with SGD optimizer and lowered learning rate.</span></h3><p style="padding-top: 3pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/cup" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.15: </a><a href="https://nnfs.io/cup" class="s9" target="_blank">https://nnfs.io/cup</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, the neural network did slightly better in terms of accuracy, and it achieved a lower loss; lower loss is not always associated with higher accuracy. Remember, even if we desire the best accuracy out of our model, the optimizer’s task is to decrease loss, not raise accuracy directly. Loss is the mean value of all of the sample losses, and some of them could drop significantly, while others might rise just slightly, changing the prediction for them from a correct to an incorrect class at the same time. This would cause a lower mean loss in general, but also more incorrectly predicted samples, which will, at the same time, lower the accuracy. A likely reason for this model’s lower accuracy is that it found another local minimum by chance — the descent path has changed, due to smaller steps. In a direct comparison of these two models in training, different learning rates did not show that the lower this learning rate value is, the better. In most cases, we want to start with a larger learning rate and decrease the learning rate over time/steps.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">A commonly-used solution to keep initial updates large and explore various learning rates during training is to implement a <b>learning rate decay</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark188">Learning Rate Decay</a><a name="bookmark204">&zwnj;</a><a name="bookmark205">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The idea of a <b>learning rate decay </b>is to start with a large learning rate, say 1.0 in our case, and then decrease it during training. There are a few methods for doing this. One is to decrease the learning rate in response to the loss across epochs — for example, if the loss begins to level out/ plateau or starts “jumping” over large deltas. You can either program this behavior-monitoring logically or simply track your loss over time and manually decrease the learning rate when you deem it appropriate. Another option, which we will implement, is to program a <b>Decay Rate</b>, which steadily decays the learning rate per batch or epoch.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s plan to decay per step. This can also be referred to as <b>1/t decaying </b>or <b>exponential decaying</b>. Basically, we’re going to update the learning rate each step by the reciprocal of the step count fraction. This fraction is a new hyper-parameter that we’ll add to the optimizer, called the <b>learning rate decay</b>. How this decaying works is it takes the step and the decaying ratio</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">and multiplies them. The further in training, the bigger the step is, and the bigger result of this multiplication is. We then take its reciprocal (the further in training, the lower the value) and multiply the initial learning rate by it. The added <i>1 </i>makes sure that the resulting algorithm never raises the learning rate. For example, for the first step, we might divide 1 by the learning rate,</p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">0.001 <span class="p">for example, which will result in a current learning rate of </span>1000<span class="p">. That’s definitely not what we wanted. 1 divided by the 1+fraction ensures that the result, a fraction of the starting learning rate, will always be less than or equal to 1, decreasing over time. That’s the desired result — start with the current learning rate and make it smaller with time. The code for determining the current decay rate:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">starting_learning_rate <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">learning_rate_decay <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">learning_rate <span style=" color: #C71F43;">= </span>starting_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 96pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">+ </span>learning_rate_decay <span style=" color: #C71F43;">* </span>step)) <span style=" color: #32A7BD;">print</span>(learning_rate)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.9090909090909091</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In practice, 0.1 would be considered a fairly aggressive decay rate, but this should give you a sense of the concept. If we are on step 20:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">starting_learning_rate <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">learning_rate_decay <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">20</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">learning_rate <span style=" color: #C71F43;">= </span>starting_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 96pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">+ </span>learning_rate_decay <span style=" color: #C71F43;">* </span>step)) <span style=" color: #32A7BD;">print</span>(learning_rate)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0.3333333333333333</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 20pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can also simulate this in a loop, which is more comparable to how we will be applying learning rate decay:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">starting_learning_rate <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1.</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">learning_rate_decay <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>step <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">20</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">learning_rate <span style=" color: #C71F43;">= </span>starting_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 96pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">+ </span>learning_rate_decay <span style=" color: #C71F43;">* </span>step)) <span style=" color: #32A7BD;">print</span>(learning_rate)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1.0</span></p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">0.9090909090909091</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.8333333333333334</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.7692307692307692</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.7142857142857143</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.6666666666666666</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.625</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.588235294117647</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.5555555555555556</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.5263157894736842</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.5</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.47619047619047616</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.45454545454545453</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.4347826086956522</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.41666666666666663</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.4</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.3846153846153846</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.37037037037037035</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.35714285714285715</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.3448275862068965</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This learning rate decay scheme lowers the learning rate each step using the mentioned formula. Initially, the learning rate drops fast, but the change in the learning rate lowers each step, letting the model sit as close as possible to the minimum. The model needs small updates near the end of training to be able to get as close to this point as possible. We can now update our SGD optimizer class to allow for the learning rate decay:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">We’ve updated a few things in the SGD class. First, in the<u> </u><span class="s43">init </span>method, we added handling for the current learning rate, and <span class="s22">self.learning_rate </span>is now the initial learning rate. We</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">also added attributes to track the decay rate and the number of iterations that the optimizer has gone through. Next, we added a new method called <span class="s35">pre_update_params</span>. This method, if we have a decay rate other than 0, will update our <span class="s22">self.current_learning_rate </span>using the prior formula. The <span class="s35">update_params </span>method remains unchanged, but we do have a new <span class="s35">post_ update_params </span>method that will add to our <span class="s22">self.iterations </span>tracking. With our updated SGD optimizer class, we’ve added printing the current learning rate, and added pre and post optimizer method calls. Let’s use a decay rate of 1e-2 (0.01) and train our model again:</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:345pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:345pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">, acc: </span>0.360<span style=" color: #231F20;">, loss: </span>1.099<span style=" color: #231F20;">, lr: </span>1.0</p></td></tr><tr style="height:16pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;text-align: left;">0.403<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;text-align: left;">1.095<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;text-align: left;">0.5025125628140703</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.397<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.084<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.33444816053511706</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.400<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.080<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.2506265664160401</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">400<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.407<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.078<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.2004008016032064</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">500<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.420<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.078<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.1669449081803005</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.420<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.077<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.14306151645207438</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.417<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.077<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.1251564455569462</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.413<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.077<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.11123470522803114</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.410<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.077<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.10010010010010009</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.417<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">1.077<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.09099181073703366</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.420<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">1.076<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.047641734159123386</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">3000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.413<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">1.075<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.03226847370119393</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.407<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">1.075<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.02439619419370578</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">5000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.403<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">1.074<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.019611688566385566</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.400<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">1.073<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.014086491055078181</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:42pt"><p class="s24" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:345pt" colspan="7"><p class="s26" style="padding-top: 2pt;padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10000<span style=" color: #231F20;">, acc: </span>0.397<span style=" color: #231F20;">, loss: </span>1.072<span style=" color: #231F20;">, lr: </span>0.009901970492127933</p></td></tr></table><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 10.16: <span class="p">Model training with SGD optimizer and and learning rate decay set too high.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/zuk" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.16: </a><a href="https://nnfs.io/zuk" class="s9" target="_blank">https://nnfs.io/zuk</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This model definitely got stuck, and the reason is almost certainly because the learning rate decayed far too quickly and became too small, trapping the model in some local minimum. This is most likely why, rather than wiggling, our accuracy and loss stopped changing <i>at all</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can, instead, try to decay a bit slower by making our decay a smaller number. For example, let’s go with 1e-3 (0.001):</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:338pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:338pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0<span style=" color: #231F20;">, acc: </span>0.360<span style=" color: #231F20;">, loss: </span>1.099<span style=" color: #231F20;">, lr: </span>1.0</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:338pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">100<span style=" color: #231F20;">, acc: </span>0.400<span style=" color: #231F20;">, loss: </span>1.088<span style=" color: #231F20;">, lr: </span>0.9099181073703367</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:338pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">, acc: </span>0.423<span style=" color: #231F20;">, loss: </span>1.078<span style=" color: #231F20;">, lr: </span>0.8340283569641367</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:338pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">1700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">0.450<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">1.025<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">0.3705075954057058</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.470<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1.017<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.35727045373347627</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.460<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1.008<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3449465332873405</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.463<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1.000<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.33344448149383127</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">2100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.490<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1.005<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.32268473701193934</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:126pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">3200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.493<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.983<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.23815194093831865</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:126pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">5000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.577<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.900<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.16669444907484582</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:126pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">6000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.633<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.860<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.1428775539362766</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:126pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">8000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.647<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.799<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.11112345816201799</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:126pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">9800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.663<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.773<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.09260116677470137</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">9900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.663<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.772<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.09175153683824203</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:338pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10000<span style=" color: #231F20;">, acc: </span>0.667<span style=" color: #231F20;">, loss: </span>0.771<span style=" color: #231F20;">, lr: </span>0.09091735612328393</p></td></tr></table><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 10.17: <span class="p">Model training with SGD optimizer and more proper learning rate decay.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/muk" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.17: </a><a href="https://nnfs.io/muk" class="s9" target="_blank">https://nnfs.io/muk</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">In this case, we’ve achieved our lowest loss and highest accuracy thus far, but it still should be possible to find parameters that will give us even better results. For example, you may suspect that the initial learning rate is too high. It can make for a great exercise to attempt to find better settings. Feel free to try!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Stochastic Gradient Descent with learning rate decay can do fairly well but is still a fairly basic optimization method that only follows a gradient without any additional logic that could potentially help the model find the <b>global minimum </b>to the loss function. One option for improving the SGD optimizer is to introduce <b>momentum</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark189">Stochastic Gradient Descent with Momentum</a><a name="bookmark206">&zwnj;</a><a name="bookmark207">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Momentum creates a rolling average of gradients over some number of updates and uses this average with the unique gradient at each step. Another way of understanding this is to imagine a ball going down a hill — even if it finds a small hole or hill, momentum will let it go straight through it towards a lower minimum — the bottom of this hill. This can help in cases where</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">you’re stuck in some local minimum (a hole), bouncing back and forth. With momentum, a model is more likely to pass through local minimums, further decreasing loss. Simply put, momentum may still point towards the global gradient descent direction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Recall this situation from the beginning of this chapter:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With regular updates, the SGD optimizer might determine that the next best step is one that keeps the model in a local minimum. Remember that the gradient points toward the current steepest loss ascent for that step — taking the negative of the gradient vector flips it toward the current steepest descent, which may not necessarily follow descent towards the global minimum — the current steepest descent may point towards a local minimum. So this step may decrease loss for that update but might not get us out of the local minimum. We might wind up with a gradient</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">that points in one direction and then the opposite direction in the next update; the gradient could continue to bounce back and forth around a local minimum like this, keeping the optimization of the loss stuck. Instead, momentum uses the previous update’s direction to influence the next update’s direction, minimizing the chances of bouncing around and getting stuck.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Recall another example shown in this chapter:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 86pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We utilize momentum by setting a parameter between 0 and 1, representing the fraction of the previous parameter update to retain, and subtracting (adding the negative) our actual gradient, multiplied by the learning rate (like before), from it. The update contains a portion of the gradient from preceding steps as our momentum (direction of previous changes) and only a portion of the current gradient; together, these portions form the actual change to our parameters and the bigger the role that momentum takes in the update, the slower the update can change the direction. When we set the momentum fraction too high, the model might stop learning at all since the direction of the updates won’t be able to follow the global gradient descent. The code for this is as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 122pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">The hyperparameter, <span class="s22">self.momentum</span>, is chosen at the start and the <span class="s22">layer.weight_ momentums </span>start as all zeros but are altered during training as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">This means that the momentum is always the previous update to the parameters. We will perform the same operations as the above with the biases. We can then update our SGD optimizer class’ <span class="s35">update_params </span>method with the momentum calculation, applying with the parameters, and retaining them for the next steps as an alternative chain of operations to the current code. The difference is that we only calculate the updates and we add these updates with the common code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Making our full SGD optimizer class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Let’s show an example illustrating how adding momentum changes the learning process. Keeping the same starting <b>learning rate </b>(1) and <b>decay </b>(1e-3) from the previous training attempt and using a momentum of 0.5:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.5</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.467pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:332pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:332pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0<span style=" color: #231F20;">, acc: </span>0.360<span style=" color: #231F20;">, loss: </span>1.099<span style=" color: #231F20;">, lr: </span>1.0</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:332pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">100<span style=" color: #231F20;">, acc: </span>0.427<span style=" color: #231F20;">, loss: </span>1.078<span style=" color: #231F20;">, lr: </span>0.9099181073703367</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:332pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">, acc: </span>0.423<span style=" color: #231F20;">, loss: </span>1.075<span style=" color: #231F20;">, lr: </span>0.8340283569641367</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:332pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">1800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">0.483<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">0.978<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">0.35727045373347627</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">1900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.547<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.984<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.3449465332873405</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:120pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">3100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.593<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.883<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.2439619419370578</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">3200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.570<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.878<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.23815194093831865</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">3300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.563<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.863<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.23261223540358225</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">3400<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.607<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.860<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.22732439190725165</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:25pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:120pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">4600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.670<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.761<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.1786033220217896</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">4700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.690<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.749<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">lr:</p></td><td style="width:120pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.1754693805930865</p></td></tr></table><p class="s14" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">6000</span>, acc: <span style=" color: #7358A5;">0.743</span>, loss: <span style=" color: #7358A5;">0.661</span>, lr: <span style=" color: #7358A5;">0.1428775539362766</span></p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">8000</span>, acc: <span style=" color: #7358A5;">0.763</span>, loss: <span style=" color: #7358A5;">0.586</span>, lr: <span style=" color: #7358A5;">0.11112345816201799</span></p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.800</span>, loss: <span style=" color: #7358A5;">0.539</span>, lr: <span style=" color: #7358A5;">0.09091735612328393</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 10.18: <span class="p">Model training with SGD optimizer, learning rate decay and Momentum.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Anim 10.18<a href="https://nnfs.io/ram" class="s82" target="_blank">: </a><a href="https://nnfs.io/ram" class="s9" target="_blank">https://nnfs.io/ram</a></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The model achieved the lowest loss and highest accuracy that we’ve seen so far, but can we do even better? Sure we can! Let’s try to set the momentum to 0.9:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_SGD(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:338pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:338pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">, acc: </span>0.360<span style=" color: #231F20;">, loss: </span>1.099<span style=" color: #231F20;">, lr: </span>1.0</p></td></tr><tr style="height:16pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;text-align: left;">0.443<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;text-align: left;">1.053<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;text-align: left;">0.9099181073703367</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.497<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.999<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.8340283569641367</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.603<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.810<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.7698229407236336</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">400<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.700<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.700<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.7147962830593281</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">500<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.750<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.595<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.66711140760507</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.810<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.496<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.6253908692933083</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.810<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.466<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.5885815185403178</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.847<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.384<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.5558643690939411</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.850<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.364<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.526592943654555</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.877<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.344<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.5002501250625312</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.900<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.242<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.31259768677711786</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.910<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.216<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.25647601949217746</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">3800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.920<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.202<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.20837674515524068</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.930<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.181<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.12347203358439313</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:338pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10000<span style=" color: #231F20;">, acc: </span>0.933<span style=" color: #231F20;">, loss: </span>0.173<span style=" color: #231F20;">, lr: </span>0.09091735612328393</p></td></tr></table><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 10.19: <span class="p">Model training with SGD optimizer, learning rate decay and Momentum (tuned).</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Anim 10.19<a href="https://nnfs.io/map" class="s82" target="_blank">: </a><a href="https://nnfs.io/map" class="s9" target="_blank">https://nnfs.io/map</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is a decent enough example of how momentum can prove useful. The model achieved an accuracy of almost 88% in the first 1000 epochs and improved further, ending with an accuracy of 93.3% and a loss of 0.173. These results are a great improvement. The SGD optimizer with momentum is usually one of 2 main choices for an optimizer in practice next to the Adam optimizer, which we’ll talk about shortly. First, we have 2 other optimizers to talk about. The next modification to Stochastic Gradient Descent is <b>AdaGrad</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark190">AdaGrad</a><a name="bookmark208">&zwnj;</a><a name="bookmark209">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">AdaGrad<span class="p">, short for </span>adaptive gradient<span class="p">, institutes a per-parameter learning rate rather than a globally-shared rate. The idea here is to normalize updates made to the features. During the training process, some weights can rise significantly, while others tend to not change by much.</span></h3><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It is usually better for weights to not rise too high compared to the other weights, and we’ll talk about this with regularization techniques. AdaGrad provides a way to normalize parameter updates by keeping a history of previous updates — the bigger the sum of the updates is, in either direction (positive or negative), the smaller updates are made further in training. This lets less-frequently updated parameters to keep-up with changes, effectively utilizing more neurons for training. The concept of AdaGrad can be contained in the following two lines of code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">cache <span style=" color: #C71F43;">+= </span>parm_gradient <span style=" color: #C71F43;">** </span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">parm_updates <span style=" color: #C71F43;">= </span>learning_rate <span style=" color: #C71F43;">* </span>parm_gradient <span style=" color: #C71F43;">/ </span>(sqrt(cache) <span style=" color: #C71F43;">+ </span>eps)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">The <span class="s22">cache </span>holds a history of squared gradients, and the <span class="s22">parm_updates </span>is a function of the learning rate multiplied by the gradient (basic SGD so far) and then is divided by the square root of the cache plus some <b>epsilon </b>value. The division operation performed with a constantly rising cache might also cause the learning to stall as updates become smaller with time, due to the monotonic nature of updates. That’s why this optimizer is not widely used, except for some specific applications. The <b>epsilon </b>is a <b>hyperparameter </b>(pre-training control knob setting)</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">preventing division by 0. The epsilon value is usually a small value, such as <i>1e-7</i>, which we’ll be defaulting to. You might also notice that we are summing the squared value, only to calculate the square root later, which might look counter-intuitive as to why we do this. We are adding squared values and taking the square root, which is not the same as just adding the value, for example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The resulting cache value grows slower, and in a different way, taking care of the negative numbers (we would not want to divide the update by the negative number and flip its sign). Overall, the impact is the learning rates for parameters with smaller gradients are decreased slowly, while the parameters with larger gradients have their learning rates decreased faster.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To implement AdaGrad, we start by copying and pasting our SGD optimizer class, changing the name, adding a property for <b>epsilon </b>with a default of 1e-7 to the<u> </u><span class="s43">init </span>method, and</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">removing the momentum. Next, inside the <span class="s35">update_params </span>method, we’ll replace the momentum code with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We added the cache and its updates, then added dividing the updates by the square root of the cache. Full code for the AdaGrad optimizer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Testing this optimizer now with decaying set to <i>1e-4 </i>as well as <i>1e-5 </i>works better than <i>1e-3</i>, which we have used previously. This optimizer with our dataset works better with lesser decaying:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;">#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9) </span>optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adagrad(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-4</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.360</span>, loss: <span style=" color: #7358A5;">1.099</span>, lr: <span style=" color: #7358A5;">1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.467pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.457<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">1.012<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td><td style="width:114pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.9901970492127933</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.527<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.936<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:114pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.9804882831650161</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.600<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.874<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td><td style="width:114pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.9709680551509855</p></td></tr></table><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1200<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.700<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.640<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.892936869363336</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1700<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.750<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.579<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.8547739123001966</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4700<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.800<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.464<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.6803183890060548</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">5100<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.810<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.454<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.6622955162593549</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">6700<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.820<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.426<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.5988382537876519</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7500<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.830<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.412<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.5714612263557918</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:39pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:117pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9900<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.847<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.381<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.5025378159706518</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:39pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10000<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.847<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.379<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:117pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.5000250012500626</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.20: <span class="p">Model training with AdaGrad optimizer.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bop" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.20: </a><a href="https://nnfs.io/bop" class="s9" target="_blank">https://nnfs.io/bop</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark191">AdaGrad worked quite well here, but not as good as SGD with momentum, and we can see that loss consistently fell throughout the entire training process. It is interesting to note that AdaGrad initially took a few more epochs to reach similar results to Stochastic Gradient Descent with momentum.</a><a name="bookmark210">&zwnj;</a><a name="bookmark211">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">RMSProp</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Continuing with Stochastic Gradient Descent adaptations, we reach <b>RMSProp</b>, short for <b>Root Mean Square Propagation</b>. Similar to AdaGrad, RMSProp calculates an adaptive learning rate per parameter; it’s just calculated in a different way than AdaGrad.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Where AdaGrad calculates the cache as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">cache <span style=" color: #C71F43;">+= </span>gradient <span style=" color: #C71F43;">** </span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">RMSProp calculates the cache as:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">cache <span style=" color: #C71F43;">= </span>rho <span style=" color: #C71F43;">* </span>cache <span style=" color: #C71F43;">+ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>rho) <span style=" color: #C71F43;">* </span>gradient <span style=" color: #C71F43;">** </span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Note that this is similar to both momentum with the SGD optimizer and cache with the AdaGrad. RMSProp adds a mechanism similar to momentum but also adds a per-parameter adaptive learning rate, so the learning rate changes are smoother. This helps to retain the global direction of changes and slows changes in direction. Instead of continually adding squared gradients to a cache (like in Adagrad), it uses a moving average of the cache. Each update to the cache retains a part of the cache and updates it with a fraction of the new, squared, gradients. In this way, cache contents “move” with data in time, and learning does not stall. In the case of this optimizer, the per- parameter learning rate can either fall or rise, depending on the last updates and current gradient. RMSProp applies the cache in the same way as AdaGrad does.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The new hyperparameter here is <i>rho</i>. <i>Rho </i>is the cache memory decay rate. Because this optimizer, with default values, carries over so much momentum of gradient and the adaptive learning rate updates, even small gradient updates are enough to keep it going; therefore, a default learning rate of <i>1 </i>is far too large and causes instant model instability. A learning rate that becomes stable again and gives fast enough updates is around <i>0.001 </i>(that’s also the default value for this optimizer used in well-known machine learning frameworks). That’s what we’ll use as default from now on too. The following is the full code for RMSProp optimizer class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Changing the optimizer used in our main neural network testing code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_RMSprop(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-4</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And running this code gives us:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.360</span>, loss: <span style=" color: #7358A5;">1.099</span>, lr: <span style=" color: #7358A5;">0.001</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.417<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">1.077<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.0009901970492127933</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.457<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.072<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009804882831650162</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.480<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">1.062<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009709680551509856</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.597<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.961<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.0009091735612328393</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.703<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.767<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.0006757213325224677</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">5800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.713<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.744<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.0006329514526235838</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.720<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.718<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.0005848295221942804</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.730</span>, loss: <span style=" color: #7358A5;">0.668</span>, lr: <span style=" color: #7358A5;">0.0005000250012500625</span></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.21: <span class="p">Model training with RMSProp optimizer.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/pun" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.21: </a><a href="https://nnfs.io/pun" class="s9" target="_blank">https://nnfs.io/pun</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The results are not the greatest, but we can slightly tweak the hyperparameters:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 7pt;text-indent: 0pt;text-align: center;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_RMSprop(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.02</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-5</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 46pt;text-indent: 0pt;text-align: center;">rho<span class="s11">=</span><span class="s14">0.999</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:345pt" colspan="7"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:345pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0<span style=" color: #231F20;">, acc: </span>0.360<span style=" color: #231F20;">, loss: </span>1.099<span style=" color: #231F20;">, lr: </span>0.02</p></td></tr><tr style="height:16pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;text-align: left;">0.467<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;text-align: left;">1.014<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;text-align: left;">0.01998021958261321</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.530<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.959<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.019960279044701046</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.623<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.762<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:136pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.019880913329158343</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.710<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.634<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.019802176259170884</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.810<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.475<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.01964655841412981</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">3800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.850<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.351<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.01926800836231563</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">6200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.870<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.286<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018832569044906263</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">6600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.903<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.262<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018761902081633034</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.900<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.274<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018674310684506857</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:136pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9500<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.890<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.244<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018265006986365174</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.893<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.241<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018248341681949654</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.743<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.794<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018231706761228456</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.917<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.213<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018215102141185255</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">9900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: right;">0.907<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: right;">0.225<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td><td style="width:136pt"><p class="s26" style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.018198527739105907</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:345pt" colspan="7"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10000<span style=" color: #231F20;">, acc: </span>0.910<span style=" color: #231F20;">, loss: </span>0.221<span style=" color: #231F20;">, lr: </span>0.018181983472577025</p></td></tr></table><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.22: <span class="p">Model training with RMSProp optimizer (tuned).</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/not" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.22: </a><a href="https://nnfs.io/not" class="s9" target="_blank">https://nnfs.io/not</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Pretty good result, close to SGD with momentum but not as good. We still have one final adaptation to stochastic gradient descent to cover.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark192">Adam</a><a name="bookmark212">&zwnj;</a><a name="bookmark213">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Adam<span class="p">, short for </span>Adaptive Momentum<span class="p">, is currently the most widely-used optimizer and is built atop RMSProp, with the momentum concept from SGD added back in. This means that, instead of applying current gradients, we’re going to apply momentums like in the SGD optimizer with momentum, then apply a per-weight adaptive learning rate with the cache as done in RMSProp.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">The Adam optimizer additionally adds a bias correction mechanism. Do not confuse this with the layer’s bias. The bias correction mechanism is applied to the cache and momentum, compensating for the initial zeroed values before they warm up with initial steps. To achieve this correction, both momentum and caches are divided by </span>1-beta<span class="s41">step</span><span class="p">. As step raises, </span>beta<span class="s41">step</span> <span class="p">approaches </span>0 <span class="p">(a fraction to the power of a rising value decreases), solving this whole expression to a fraction during the first steps and approaching </span>1 <span class="p">as training progresses. For example, </span>beta 1<span class="p">, a fraction of momentum to apply, defaults to 0.9. This means that, during the first step, the correction value equals:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">With training progression, as step count rises:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The same applies to the cache and the <i>beta 2 </i>— in this case, the starting value is 0.001 and also approaches <i>1</i>. These values divide the momentums and the cache, respectively. Division by a fraction causes them to be multiple times bigger, significantly speeding up training in the initial stages before both tables warm up during multiple initial steps. We also previously mentioned that both of these bias-correcting coefficients go towards a value of <i>1 </i>as training progresses and return parameter updates to their typical values for the later training steps. To get parameter updates, we divide the scaled momentum by the scaled square-rooted cache.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The code for the Adam Optimizer is based on the RMSProp optimizer. It adds the cache seen from the SGD along with the <i>beta 1 </i>hyper-parameter. Next, it introduces the bias correction mechanism for both the momentum and the cache. We’ve also modified the way the parameter updates are calculated — using corrected momentums and corrected caches, instead of gradients and caches. The full list of changes made from RMSProp are posted after the following code:</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get corrected cache</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The following changes were made from copying the RMSProp class code:</p><ol id="l4"><li><p style="padding-top: 2pt;padding-left: 44pt;text-indent: -18pt;text-align: left;">renamed class from <span class="s35">Optimizer_RMSprop </span>to <span class="s35">Optimizer_Adam</span></p></li><li><p style="padding-top: 1pt;padding-left: 44pt;text-indent: -18pt;text-align: left;">renamed the <span class="s29">rho </span>hyperparameter and property to <span class="s29">beta_2 </span>in <span class="s43">init</span><span class="s61">  </span></p></li><li><p style="padding-top: 1pt;padding-left: 43pt;text-indent: -18pt;text-align: left;">added <span class="s29">beta_1 </span>hyperparameter and property in <span class="s43">init</span><span class="s61">  </span></p></li><li><p style="padding-top: 1pt;padding-left: 43pt;text-indent: -18pt;text-align: left;">added <i>momentum </i>array creation in <i>update_params()</i></p></li><li><p style="padding-top: 2pt;padding-left: 43pt;text-indent: -18pt;text-align: left;">added <i>momentum </i>calculation</p></li><li><p style="padding-top: 2pt;padding-left: 43pt;text-indent: -18pt;text-align: left;">renamed <span class="s22">self.rho </span>to <span class="s22">self.beta_2 </span>with cache calculation code in <span class="s35">update_params</span></p></li><li><p style="padding-top: 1pt;padding-left: 43pt;text-indent: -18pt;text-align: left;">added <span class="s22">*_corrected </span>variables as corrected momentums and weights</p></li><li><p class="s22" style="padding-top: 1pt;padding-left: 43pt;text-indent: -18pt;line-height: 112%;text-align: left;"><span class="p">replaced </span>layer.dweights<span class="s3">, </span>layer.dbiases<span class="s3">, </span>layer.weight_cache<span class="p">, and </span>layer. bias_cache <span class="p">with corrected arrays of values in parameter updates with momentum arrays</span></p></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Back to our main neural network code. We can now set our optimizer to Adam, run the code, and see what impact these changes had:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.02</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-5</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">With our default settings, we end with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.360</span>, loss: <span style=" color: #7358A5;">1.099</span>, lr: <span style=" color: #7358A5;">0.02</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.683<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.772<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 1pt;padding-right: 7pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.01998021958261321</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">epoch:</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: center;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: center;">0.793<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: center;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 13pt;text-align: right;">0.560<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 13pt;text-align: right;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 13pt;text-align: center;">0.019960279044701046</p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:30pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.850<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:42pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.458<span style=" color: #231F20;">,</span></p></td><td style="width:25pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:126pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.019940378268975763</p></td></tr></table><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">400<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.873<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.374<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.01992051713662487</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">500<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.897<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.321<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.01990069552930875</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.893<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.286<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.019880913329158343</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.900<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.260<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: left;">0.019861170418772778</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">1700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: right;">0.930<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: right;">0.164<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">0.019665876753950384</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">2600<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: right;">0.950<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 13pt;text-align: right;">0.132<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 13pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 13pt;text-align: right;">0.019493367381748363</p></td></tr><tr style="height:15pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 13pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">9900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.967<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.078<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018198527739105907</p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.963</span>, loss: <span style=" color: #7358A5;">0.079</span>, lr: <span style=" color: #7358A5;">0.018181983472577025</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.23: <span class="p">Model training with Adam optimizer.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Epilepsy Warning (quick flashing colors)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/you" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.23: </a><a href="https://nnfs.io/you" class="s9" target="_blank">https://nnfs.io/you</a></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is the best result so far, but let’s adjust the learning rate to be a bit higher, to <i>0.05 </i>and change decay to <i>5e-7</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.05</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">In this case, loss and accuracy slightly improved, ending on:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.360</span>, loss: <span style=" color: #7358A5;">1.099</span>, lr: <span style=" color: #7358A5;">0.05</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">100<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.713<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.684<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.04999752512250644</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">200<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.827<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.511<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.04999502549496326</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">700<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.907<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.264<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.049982531105378675</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.897<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.278<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.04998003297682575</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.923<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.230<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td><td style="width:129pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.049977535097973466</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.930<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.170<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04995007490013731</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">3300<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.950<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.136<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04991766081847992</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7800<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.973<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.089<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04980578235171948</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">7900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.970<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.089<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04980330185930667</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">8000<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.980<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.088<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04980082161395499</p></td></tr><tr style="height:14pt"><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:33pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:30pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:43pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:129pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:42pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch:</p></td><td style="width:33pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">9900<span style=" color: #231F20;">,</span></p></td><td style="width:30pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">acc:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: right;">0.983<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:43pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: right;">0.074<span style=" color: #231F20;">,</span></p></td><td style="width:24pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td><td style="width:129pt"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.049753743844839965</p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.983</span>, loss: <span style=" color: #7358A5;">0.074</span>, lr: <span style=" color: #7358A5;">0.04975126853296942</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 10.24: <span class="p">Model training with Adam optimizer (tuned).</span></h3><p style="padding-top: 3pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a name="bookmark193">Epilepsy Warning (quick flashing colors)</a><a name="bookmark214">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/car" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 10.24 : </a><a href="https://nnfs.io/car" class="s9" target="_blank">https://nnfs.io/car</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It doesn’t get much better, both for accuracy and loss. While Adam has performed the best here and is usually the best optimizer of those shown, that’s not always the case. It’s usually a good idea to try the Adam optimizer first but to also try the others, especially if you’re not getting the results you hoped for. Sometimes simple SGD or SGD + momentum performs better than Adam. Reasons why will vary, but keep this in mind.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We will cover choosing various hyperparameters (such as the learning rate) when training, but a general starting learning rate for SGD is 1.0, with a decay down to 0.1. For Adam, a good starting LR is 0.001 (1e-3), decaying down to 0.0001 (1e-4). Different problems may require different values here, but these are decent to start.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We achieved 98.3% accuracy on the generated dataset in this section, and a loss approaching perfection (0). Rather than being excited, you will soon learn to fear results this good, or at least approach them cautiously. There are cases where you can truly achieve valid results as good as these, but, in this case, we’ve been ignoring a major concept in machine learning: out-of-sample testing data (which can shed light on over-fitting), which is the subject of the next section.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full code up to this point:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>): <span style=" color: #A5A4A5;"># Initialize weights and biases</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>) <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let’s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn’t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get corrected cache</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We’ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer’s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier’s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.05</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-top: 4pt;padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch10" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch10<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark215">Chapter 11</a><a name="bookmark216">&zwnj;</a><a name="bookmark217">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;line-height: 130%;text-align: left;">Testing with Out-of-Sample Data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Up to this point, we’ve created a model that is seemingly 98% accurate at predicting the testing dataset that we’ve generated. These generated data are created based on a very clear set of rules outlined in the <i>spiral_data </i>function. The expectation is that a well-trained neural network can learn a representation of these rules and use this representation to predict classes of additional generated data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Imagine that you’ve trained a neural network model to read license plates on vehicles. The expectation for a well-trained model, in this case, would be that it could see future examples of license plates and still accurately predict them (a prediction, in this case, would be correctly identifying the characters on the license plate).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The complexity of neural networks is their biggest issue and strength. By having a massive amount of tunable parameters, they are exceptional at “fitting” to data. This is a gift, a curse,</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark218">and something that we must constantly try to balance. With enough neurons, a model can easily memorize a dataset; however, it can not generalize the data with too few. This is one reason why we do not simply solve problems with neural networks by using the most neurons or biggest models possible.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At the moment, we’re uncertain whether our latest neural network’s 98% accuracy is due to learning to meaningfully represent the underlying data-generation function or instead <b>overfitting </b>the data. So far, we have only tuned hyper-parameters to achieve the highest possible accuracy on the training data, and have never tried to challenge the model with the previously unseen data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Overfitting <span class="p">is effectively just memorizing the data without any understanding of it. An overfit model will do very well predicting the data that it has already seen, but often significantly worse on unseen data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 11.01: <span class="p">Good generalization (left) and overfitting (right) on the data</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The left image shows an example of generalization. In this example, the model learned to separate red and blue data points, even if some of them will be predicted incorrectly. One reason for this might be the data that contains some “confusing” samples. When you look at the image, you</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">can see that, for example, some of these blue dots might not be there, which would raise the data quality and make it easier to fit. A good dataset is one of the biggest challenges with neural</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">networks. The image on the right shows the model that memorized the data, fitting them perfectly and ruining generalization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Without knowing if a model overfits the training data, we cannot trust the model’s results. For this reason, it’s essential to have both <b>training </b>and <b>testing data </b>as separate sets for different purposes.</p><h3 style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark219">Training </a><span class="p">data should only be used to train a model. The </span>testing, <span class="p">or </span>out-of-sample <span class="p">data, should only be used to validate a model’s performance after training (we are using the testing data during training later in this chapter for demonstration purposes only). The idea is that some data are reserved and withheld from the training data for testing the model’s performance.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In many cases, one can take a random sampling of available data to train with and make the remaining data the testing dataset. You still need to be very careful about information leaking through. One common area where this can be problematic is in time-series data. Consider a scenario where you have data from sensors collected every second. You might have millions of observations collected, and randomly selecting your data for the <b>testing </b>data might result in</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">samples in your <b>testing </b>dataset that are only a second in time apart from your <b>training </b>data, thus are very similar. This means overfitting can spill into your testing data, and the model can achieve good results on both the training and the testing data, which won’t mean it generalized well.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Randomly allocating time-series data as testing data may be very similar to training data. Both datasets must differ enough to prove the model’s ability to generalize. In time-series data, a better approach is to take multiple slices of your data, entire blocks of time, and reserve those for testing.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Other biases like these can sneak into your testing dataset, and this is something you must be vigilant about, carefully considering if data leakage has occurred and how to truly isolate <b>out-of- sample </b>data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In our case, we can use our data-generating function to create new data that will serve as out-of- sample/testing data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create test dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Given what was just said about overfitting, it may look wrong to only generate more data, as the testing data could look similar to the training data. Intuition and experience are both important to spot potential issues with out-of-sample data. By looking at the image representation of the data, we can see that another set of data generated by the same function will be adequate. This is just about as safe as it gets for out-of-sample data as the classes are partially mixing at the edges (also, we’re quite literally using the “underlying function” to make more data).</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark220">With these data, we evaluate the model’s performance by doing a forward pass and calculating loss and accuracy the same as before:</a></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 28pt;text-align: left;"># Validate the model # Create test dataset</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 11pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our testing data through this layer <span style=" color: #231F20;">dense1.forward(X_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y_test.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 108%;text-align: left;">y_test <span style=" color: #C71F43;">= </span>np.argmax(y_test, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9800</span>, acc: <span style=" color: #7358A5;">0.983</span>, loss: <span style=" color: #7358A5;">0.075</span>, lr: <span style=" color: #7358A5;">0.04975621940303483</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.983</span>, loss: <span style=" color: #7358A5;">0.074</span>, lr: <span style=" color: #7358A5;">0.049753743844839965</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.983</span>, loss: <span style=" color: #7358A5;">0.074</span>, lr: <span style=" color: #7358A5;">0.04975126853296942</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.803</span>, loss: <span style=" color: #7358A5;">0.858</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While 80.3% accuracy and a loss of <i>0.858 </i>is not terrible, this contrasts with our training data that achieved 98% accuracy and a loss of 0.074. This is evidence of over-fitting. In the following image, the training data is dimmed, and validation data points are shown on top of it at the same positions for both the well-generalized (on the left) and overfitted (on the right) models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 211pt;text-indent: -187pt;line-height: 130%;text-align: left;">Fig 11.02: <span class="p">Left - prediction with well-generalized model; right - prediction mistakes with an overfit model.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can recognize overfitting when testing data results begin to diverge in trend from training data. It will usually be the case that performance against your training data is better, but having training loss differ from test performance by over 10% approximately is a common sign of serious overfitting from our anecdotal experience. Ideally, both datasets would have identical performance. Even a small difference means that the model did not correctly predict some testing samples, implying slight overfitting of training data. In most cases, modest overfitting is not a serious problem, but something we hope to minimize.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s see the training process of this model once again, but with the training data, training accuracy, and loss plots dimmed. We add the test data and its loss and accuracy plotted on top of the training counterparts to show this model overfitting:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 11.03: <span class="p">Prediction issues on the testing data — overfitted model.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/zog" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 11.03: </a><a href="https://nnfs.io/zog" class="s9" target="_blank">https://nnfs.io/zog</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is a classic example of overfitting — the validation loss falls down, then starts rising once the model starts overfitting. The dots representing classes in the validation data can be spotted over areas of effect of other classes. Previously, we weren’t aware that this was happening; we were just seeing very good training results. That’s why we usually should use the testing data to test the model after training. The model is currently tuned to achieve the best possible score on the training data, and most likely the learning rate is too high, there are too many training epochs, or the model is too big. There are other possible causes and ways to fix this, but this is the topic of the following chapters. In general, the goal is to have the testing loss identical to the training loss, even if that means higher loss and lower accuracy on the training data. Similar performance on both datasets means that model generalized instead of overfitting on the training data.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As mentioned, one option to prevent overfitting is to change the model’s size. If a model is not learning at all, one solution might be to try a larger model. If your model is learning, but there’s a divergence between the training and testing data, it could mean that you should try a smaller model. One general rule to follow when selecting initial model hyperparameters is to find the</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">smallest model possible that still learns. Other possible ways to avoid overfitting are regularization techniques we’ll discuss in chapter 14, and the <i>Dropout </i>layer explained in chapter 15. Often the divergence of the training and testing data can take a long time to occur. The process of trying different model settings is called hyperparameter searching. Initially, you can very quickly (usually within minutes) try different settings (e.g., layer sizes) to see if the models are learning <i>something</i>. If they are, train the models fully — or at least significantly longer — and compare results to pick the best set of hyperparameters. Another possibility is to create a list of different hyperparameter sets and train the model in a loop using each of those sets at a time to pick the best set at the end. The reasoning here is that the fewer neurons you have, the less chance you have that the model is memorizing the data. Fewer neurons can mean it’s easier for a neural network to generalize (actually learn the meaning of the data) compared to memorizing the data. With enough neurons, it’s easier for a neural network to memorize the data. Remember that the neural network wants to decrease training loss and follows the path of least resistance to meet that objective. Our job as the programmer is to make the path to generalization the easiest path. This can often mean our job is actually to make the path to lowering loss for the model more challenging!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 28pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch11" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch11<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark221">Chapter 12</a><a name="bookmark222">&zwnj;</a><a name="bookmark223">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Validation Data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the chapter on optimization, we used hyperparameter tuning to select hyperparameters that lead to better results, but one more thing requires clarification. We <i>should not </i>check different hyperparameters using the test dataset; if we do that, we’re going to be manually optimizing the model to the test dataset, biasing it towards overfitting these data, and these data are supposed to be used only to perform the last check if the model trains and generalizes well. In other words, if</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">we’re tuning our network’s parameters to fit the testing data, then we’re essentially optimizing our network on the testing data, which is another way for overfitting on these data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Thus, hyperparameter tuning using the test dataset is a mistake. The test dataset should only be used as unseen data, not informing the model in any way, which hyperparameter tuning is, other than to test performance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Hyperparameter tuning can be performed using yet another dataset called <b>validation data</b>. The test dataset needs to contain real out-of-sample data, but with a validation dataset, we have more freedom with choosing data. If we have a lot of training data and can afford to use some for validation purposes, we can take it as an out-of-sample dataset, similar to a test dataset. We can now search for parameters that work best using this new validation dataset and test our model</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark224">at the end using the test dataset to see if we really tuned the model or just overfitted it to the validation data.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">There are situations when we’ll be short on data and cannot afford to create yet another dataset from the training data. In those situations, we have two options:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The first is to temporarily split the training data into a smaller training dataset and validation dataset for hyperparameter tuning. Afterward, with the final hyperparameter set, train the model on all the training data. We allow ourselves to do that as we tune the model to the part of training data that we put aside as validation data. Keep in mind that we still have a test dataset to check the model’s performance after training.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The second possibility in situations where we are short on data is a process called <b>cross- validation</b>. Cross-validation is primarily used when we have a small training dataset and cannot afford any data for validation purposes. How it works is we split the training dataset into a given number of parts, let’s say 5. We now train the model on the first 4 chunks and validate it on the last. So far, this is similar to the case described previously — we are also only using the training dataset and can validate on data that was not used for training. What makes cross-validation different is that we then swap samples. For example, if we have 5 chunks, we can call them chunks A, B, C, D, and E. We may first train on A, B, C, and D, then validate on E. We’ll then train on A, B, C, E, and validate on D, doing this until we’ve validated on each of the 5 sample groups. This way, we do not lose any training data. We validate using the data that was not used for training during any given iteration and validate on more data than if we just temporarily split the training dataset and train on all of the samples. This validation method is often called k-fold cross-validation; here, our k is 5. Here’s an example of 2 steps of cross-validation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 12.01: <span class="p">Cross-validation, first step.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 58pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 12.02: <span class="p">Cross-validation, third step.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/lho" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 12.01-12.02: </a><a href="https://nnfs.io/lho" class="s9" target="_blank">https://nnfs.io/lho</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">When using a validation dataset and cross-validation, it is common to loop over different hyperparameter sets, leaving the code to run training multiple times, applying different settings each run, and reviewing the results to choose the best set of hyperparameters. In general, we should not loop over <i>all </i>possible setting combinations that we would like to check unless training is exceptionally fast. It’s usually better to check some settings that we suspect will work well, pick the best combination of those settings, tweak them to create the next list of setting sets, and train the model on new sets. We can repeat this process as many times as we’d like.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-top: 4pt;padding-left: 103pt;text-indent: 27pt;line-height: 130%;text-align: left;"><a href="https://nnfs.io/ch12" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch12<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark225">Chapter 13</a><a name="bookmark226">&zwnj;</a><a name="bookmark227">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Training Dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we are talking about datasets and testing, it’s worth mentioning a few things about the training dataset and operations that we can perform on it; this technique is referred to as</p><h3 style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">preprocessing<span class="p">. However, it’s important to remember that any preprocessing we do to our training data also needs to be done to our validation and testing data and later done to the prediction data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Neural networks usually perform best on data consisting of numbers in a range of 0 to 1 or -1 to 1, with the latter being preferable. Centering data on the value of 0 can help with model training as it attenuates weight biasing in some direction. Models can work fine with data in the range of 0 to 1 in most cases, but sometimes we’re going to need to rescale them to a range of -1 to 1 to get training to behave or achieve better results.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Speaking of the data range, the values do not have to strictly be in the range of -1 and 1 — the model will perform well with data slightly outside of this range or with just some values being many times bigger. The case here is that when we multiply data by a weight and sum the results with a bias, we’re usually passing the resulting output to an activation function. Many activation functions behave properly within this described range. For example, <i>softmax </i>outputs a vector of probabilities containing numbers in the range of 0 to 1; <i>sigmoid </i>also has an output range of 0 to 1,</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark228">but </a><i>tanh </i>outputs a range from -1 to 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Another reason why this scaling is ideal is a neural network’s reliance on many multiplication operations. If we multiply by numbers above 1 or below -1, the resulting value is larger in scale than the original one. Within the -1 to 1 range, the result becomes a fraction, a smaller value.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Multiplying big numbers from our training data with weights might cause floating-point overflow or instability — weights growing too fast. It’s easier to control the training process with smaller numbers.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">There are many terms related to data <b>preprocessing</b>: standardization, scaling, variance scaling, mean removal (as mentioned above), non-linear transformations, scaling to outliers, etc., but they are out of the scope of this book. We’re only going to scale data to a range by simply dividing</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">all of the numbers by the maximum of their absolute values. For the example of an image that consists of numbers in the range between <i>0 </i>and <i>255</i>, we divide the whole dataset by <i>255 </i>and return data in the range from <i>0 </i>to <i>1</i>. We can also subtract <i>127.5 </i>(to get a range from <i>-127.5 </i>to <i>127.5</i>) and divide by 127.5, returning data in the range from -1 to 1.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We need to ensure identical scaling for all the datasets (same scale parameters). For example, we can find the maximum for training data and divide training, validation and testing data by this number. In general, we should prepare a scaler of our choice and use its instance on every dataset. It is important to remember that once we train our model and want to predict using new samples, we need to scale those new samples by using the same scaler instance we used on the training, validation, and testing data. In most cases, when we are working with data (e.g., sensor data), we will need to save the scaler object along with the model and use it during prediction as well; otherwise, results are likely to vary as the model might not effectively recognize these data without being scaled. It is usually fine to scale datasets that consist of larger numbers than</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">the training data using a scaler prepared on the training data. If the resulting numbers are slightly outside of the <i>-1 </i>to <i>1 </i>range, it does not affect validation or testing negatively, since we do not train on these data. Additionally, for linear scaling, we can use different datasets to find the maximum as well, but be aware that non-linear scaling can leak the information from other datasets to the training dataset and, in this case, the scaler should be prepared on the training data only.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">In cases where we do not have many training samples, we could use <b>data augmentation</b>. One easy way to understand augmentation is in the case of images. Let’s imagine that our model’s goal is to detect rotten fruits — apples, for example. We will take a photo of an apple from different angles and predict whether it’s rotten. We should get more pictures in this case, but let’s assume that we cannot. What we could do is to take photos that we have, rotate, crop, and save those as worthy data too. This way, we have added more samples to the dataset, which can help with model generalization. In general, if we use augmentation, then it’s only useful if the augmentations that we make are similar to variations that we could see in reality. For example, we may refrain from using a rotation when creating a model to detect road signs as they are not being rotated in real- life scenarios (in most cases, anyway). The case of a rotated road sign, however, is one you better</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-top: 5pt;padding-left: 296pt;text-indent: 0pt;text-align: left;">Chapter 13 | Training Dataset | <span class="s63">333</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">consider if you’re making a self-driving car. Just because a bolt came loose on a stop sign, flipping it over, doesn’t mean you no longer need to stop there!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">How many samples do we need to train the model? There is no single answer to this question — one model might require just a few per class, and another may require a few million or billion. Usually, a few thousand per class will be necessary, and a few tens of thousands should be preferable to start. The difference depends on the data complexity and model size. If the model has to predict sensor data with 2 simple classes, for example, if an image contains a dark area or does not, hundreds of samples per class might be enough. To train on data with many features and several classes, tens of thousands of samples are what you should start with. If you’re attempting to train a chatbot the intricacies of written language, then you’re going to likely want at least millions of samples.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch13" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch13<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark229">Chapter 14</a><a name="bookmark232">&zwnj;</a><a name="bookmark233">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">L1 and L2 Regularization</p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Regularization methods <span class="p">are those which reduce generalization error. The first forms of regularization that we’ll address are </span>L1 <span class="p">and </span>L2 regularization<span class="p">. L1 and L2 regularization are used to calculate a number (called a </span>penalty<span class="p">) added to the loss value to penalize the model for large weights and biases. Large weights might indicate that a neuron is attempting to memorize a data element; generally, it is believed that it would be better to have many neurons contributing to a model’s output, rather than a select few.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark230">Forward Pass</a><a name="bookmark234">&zwnj;</a><a name="bookmark235">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">L1 regularization’s penalty is the sum of all the absolute values for the weights and biases. This is a linear penalty as regularization loss returned by this function is directly proportional to parameter values. L2 regularization’s penalty is the sum of the squared weights and biases.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This non-linear approach penalizes larger weights and biases more than smaller ones because of the square function used to calculate the result. In other words, L2 regularization is commonly used as it does not affect small parameter values substantially and does not allow the model to grow weights too large by heavily penalizing relatively big values. L1 regularization, because of its linear nature, penalizes small weights more than L2 regularization, causing the model</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">to start being invariant to small inputs and variant only to the bigger ones. That’s why L1 regularization is rarely used alone and usually combined with L2 regularization if it’s even used at all. Regularization functions of this type drive the sum of weights and the sum of parameters towards <i>0</i>, which can also help in cases of exploding gradients (model instability, which might cause weights to become very large values). Beyond this, we also want to dictate how much of an impact we want this regularization penalty to carry. We use a value referred to as <b>lambda </b>in this equation — where a higher value means a more significant penalty.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">L1 weight regularization:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">L1 bias regularization:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">L2 weight regularization:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">L2 bias regularization:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Overall loss:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Using code notation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">l1w <span style=" color: #C71F43;">= </span>lambda_l1w <span style=" color: #C71F43;">* </span><span style=" color: #32A7BD;">sum</span>(<span style=" color: #32A7BD;">abs</span>(weights)) l1b <span style=" color: #C71F43;">= </span>lambda_l1b <span style=" color: #C71F43;">* </span><span style=" color: #32A7BD;">sum</span>(<span style=" color: #32A7BD;">abs</span>(biases)) l2w <span style=" color: #C71F43;">= </span>lambda_l2w <span style=" color: #C71F43;">* </span><span style=" color: #32A7BD;">sum</span>(weights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>) l2b <span style=" color: #C71F43;">= </span>lambda_l2b <span style=" color: #C71F43;">* </span><span style=" color: #32A7BD;">sum</span>(biases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>)</p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;line-height: 14pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>l1w <span style=" color: #C71F43;">+ </span>l1b <span style=" color: #C71F43;">+ </span>l2w <span style=" color: #C71F43;">+ </span>l2b</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Regularization losses are calculated separately, then summed with the data loss, to form the overall loss. Parameter <i>m </i>is an arbitrary iterator over all of the weights in a model, parameter <i>n </i>is the bias equivalent of this iterator, w<span class="s49">m </span>is the given weight, and b<span class="s49">n </span>is the given bias.</p><p style="padding-top: 12pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">To implement regularization in our neural network code, we’ll start with the<u> </u><span class="s43">init </span>method of the <span class="s35">Dense </span>layer’s class, which will house the <b>lambda </b>regularization strength hyperparameters, since these can be set separately for every layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(inputs, neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This method sets the lambda hyperparameters. Now we update our loss class to include the additional penalty if we choose to set the lambda hyperparameter for any of the regularizers in the layer’s initialization. We will implement this code into the <i>Loss </i>class as it is common for the hidden layers. What’s more, the regularization calculation is the same, regardless of</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the type of loss used. It’s only a penalty that is summed with the data loss value resulting in a final, overall loss value. For this reason, we’re going to add a new method to a general loss class, which is inherited by all of our specific loss functions (such as our existing</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="s35">Loss_CategoricalCrossentropy</span>). For the code of this method, we’ll create the layer’s regularization loss variable. We’ll add to it each of the atomic regularization losses if its corresponding lambda value is greater than <i>0</i>. To perform these calculations, we read the lambda hyperparameters, weights, and biases from the passed-in layer object. For our general loss class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we’ll calculate the regularization loss and add it to our calculated loss in the training loop:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate loss from output of activation2 so softmax activation </span>data_loss <span style=" color: #C71F43;">= </span>loss_function.forward(activation2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate regularization penalty</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">= </span>loss_function.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;">loss_function.regularization_loss(dense2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">We created a new <span class="s22">regularization_loss </span>variable and added all layer’s regularization losses to it. This completes the forward pass for regularization, but this also means our overall loss has changed since part of the calculation can include regularization, which must be accounted for in the backpropagation of the gradients. Thus, we will now cover the partial derivatives for both L1 and L2 regularization.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark231">Backward pass</a><a name="bookmark236">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of L2 regularization is relatively simple:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This might look complicated, but is one of the simpler derivative calculations that we have to derive in this book. Lambda is a constant, so we can move it outside of the derivative term. We can remove the sum operator since we calculate the partial derivative with respect to the given parameter only, and the sum of one element equals this element. So, we only need to calculate the derivative of <i>w</i><i>2</i>, which we know is <i>2w</i>. From the coding perspective, we will multiply all of the weights by <i>2λ</i>. We’ll implement this with NumPy directly as it’s just a simple multiplication operation.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">L1 regularization’s derivative, on the other hand, requires more explanation. In the case of L1 regularization, we must calculate the derivative of the absolute value piecewise function, which effectively multiplies a value by -1 if it is less than 0; otherwise, it’s multiplied by 1. This is because the absolute value function is linear for positive values, and we know that a linear function’s derivative is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">For negative values, it negates the sign of the value to make it positive. In other words, it multiplies values by -1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">When we combine that:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And the complete partial derivative of L1 regularization with respect to given weight:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Like L2 regularization, lambda is a constant, and we calculate the partial derivative of this regularization with respect to the specific input. The partial derivative, in this case, equals 1 or -1 depending on the w<span class="s49">m </span>(weight) value.</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">We are calculating this derivative with respect to weights, and the resulting gradient, which has</p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">the same shape as the weights, is what we’ll use to update the weights. To put this into pure Python code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>] <span style=" color: #A5A4A5;"># weights of one neuron</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">dL1 <span style=" color: #C71F43;">= </span>[] <span style=" color: #A5A4A5;"># array of partial derivatives of L1 regularization </span><span style=" color: #C71F43;">for </span>weight <span style=" color: #C71F43;">in </span>weights:</p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span>weight <span style=" color: #C71F43;">&gt;= </span><span style=" color: #7358A5;">0</span>: dL1.append(<span style=" color: #7358A5;">1</span>)</p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 48pt;line-height: 108%;text-align: left;">dL1.append(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>) <span style=" color: #32A7BD;">print</span>(dL1)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">You may have noticed that we’re using <span class="s15">&gt;= </span><span class="s21">0 </span>in the code where the equation above clearly depicts</p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;"><i>&gt; 0</i>. If we picture the <span class="s22">np.abs </span>function, it’s a line going down and “bouncing” at the value <i>0</i>, like a saw tooth. At the pointed end (i.e., the value of <i>0</i>), the derivative of the <span class="s22">np.abs </span>function is</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">undefined, but we cannot code it this way, so we need to handle this situation and break this rule a bit.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Now let’s try to modify this L1 derivative to work with multiple neurons in a layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>], <span style=" color: #A5A4A5;"># now we have 3 sets of weights</span></p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 86pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">dL1 <span style=" color: #C71F43;">= </span>[] <span style=" color: #A5A4A5;"># array of partial derivatives of L1 regularization </span><span style=" color: #C71F43;">for </span>neuron <span style=" color: #C71F43;">in </span>weights:</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">neuron_dL1 <span style=" color: #C71F43;">= </span>[] <span style=" color: #A5A4A5;"># derivatives related to one neuron </span><span style=" color: #C71F43;">for </span>weight <span style=" color: #C71F43;">in </span>neuron:</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span>weight <span style=" color: #C71F43;">&gt;= </span><span style=" color: #7358A5;">0</span>: neuron_dL1.append(<span style=" color: #7358A5;">1</span>)</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 48pt;line-height: 108%;text-align: left;">neuron_dL1.append(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>) dL1.append(neuron_dL1)</p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(dL1)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>], [<span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>], [<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">That’s the vanilla Python version, now for the NumPy version. With NumPy, we’re going to use conditions and binary masks. We’ll create the gradient as an array filled with values of <i>1 </i>and shaped like weights, using <span class="s22">np.ones_like(weights)</span>. Next, the condition <span class="s22">weights </span><span class="s15">&lt; </span><span class="s21">0 </span>returns an array of the same shape as <span class="s22">dL1</span>, containing <span class="s21">0 </span>where the condition is false and <span class="s21">1 </span>where it’s true. We’re using this as a binary mask to <span class="s22">dL1 </span>to set values to -<i>1 </i>only where the condition is true (where weight values are less than 0):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">weights <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">0.2</span>, <span style=" color: #7358A5;">0.8</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>, <span style=" color: #7358A5;">1</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0.5</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.91</span>, <span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.5</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.26</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.27</span>, <span style=" color: #7358A5;">0.17</span>, <span style=" color: #7358A5;">0.87</span>]])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">dL1 <span style=" color: #C71F43;">= </span>np.ones_like(weights) dL1[weights <span style=" color: #C71F43;">&lt; </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= -</span><span style=" color: #7358A5;">1</span></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:55pt"><td style="width:78pt"><p class="s53" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">print<span style=" color: #231F20;">(dL1)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s25" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p></td><td style="width:102pt" colspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">array([[ <span style=" color: #7358A5;">1.</span>,</p></td><td style="width:30pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">1.</span><span style=" color: #231F20;">,</span></p></td><td style="width:41pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:15pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[ <span style=" color: #7358A5;">1.</span>,</p></td><td style="width:30pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">1.</span><span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:41pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">1.</span><span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:78pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.</span>,</p></td><td style="width:30pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">1.</span><span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.<span style=" color: #231F20;">,</span></p></td><td style="width:41pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This returned an array of the same shape containing values of 1 and -1 — the partial gradient of the <span class="s22">np.abs </span>function (we still have to multiply it by the lambda hyperparameter). We can now</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">take these and update the backward pass method for the dense layer object. For L1 regularization, we’ll take the code above and multiply it by <i>λ </i>for weights and perform the same operation for biases. For L2 regularization, as discussed at the beginning of this chapter, all we need to do is take the weights/biases, multiply them by <i>2λ</i>, and add that product to the gradients:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense Layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With this, we can update our print to include new information — regularization loss and overall loss:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 56pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 56pt;text-indent: 0pt;line-height: 119%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we can add weight and bias regularizer parameters when defining a layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 3 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We usually add regularization terms to the hidden layers only. Even if we are calling the regularization method on the output layer as well, it won’t modify gradients if we do not set the lambda hyperparameters to values other than <i>0</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full code up to this point:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get corrected cache</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.weights)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer&#39;s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 189pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.02</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>data_loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate regularization penalty </span>regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss_activation.loss.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\ loss_activation.loss.regularization_loss(dense2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Validate the model</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create test dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our testing data through this layer <span style=" color: #231F20;">dense1.forward(X_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y_test.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 108%;text-align: left;">y_test <span style=" color: #C71F43;">= </span>np.argmax(y_test, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.947</span>, loss: <span style=" color: #7358A5;">0.217 </span>(data_loss: <span style=" color: #7358A5;">0.157</span>, reg_loss: <span style=" color: #7358A5;">0.060</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.019900507413187767</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.830</span>, loss: <span style=" color: #7358A5;">0.435</span></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 7pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 14.01: <span class="p">Training with regularization</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/abc" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 14.01: </a><a href="https://nnfs.io/abc" class="s9" target="_blank">https://nnfs.io/abc</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This animation shows the training data in the background (dimmed dots) and the validation data in the foreground. After adding the L2 regularization term to the hidden layer, we achieved a lower validation loss (0.858 before adding regularization in, 0.435 now) and higher accuracy (0.803 before, 0.830 now). We can also take a moment to exemplify how a simple increase in data for training can make a large difference. If we grow from 100 samples to 1,000 samples:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1000</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And run the code again:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.895</span>, loss: <span style=" color: #7358A5;">0.357 </span>(data_loss: <span style=" color: #7358A5;">0.293</span>, reg_loss: <span style=" color: #7358A5;">0.063</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.019900507413187767</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.873</span>, loss: <span style=" color: #7358A5;">0.332</span></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 14.02: <span class="p">Training with regularization and more training data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/bcd" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 14.02: </a><a href="https://nnfs.io/bcd" class="s9" target="_blank">https://nnfs.io/bcd</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can see that this change alone also had a considerable impact on both validation accuracy in general, as well as the delta between the validation and training accuracies — lower accuracy and higher training loss suggest that the capacity of the model might be too low. A large delta earlier and a small one now suggests that the model was most likely overfitting previously. In theory, this regularization allows us to create much larger models without fear of overfitting (or memorization). We can test this by increasing the number of neurons per layer. Going with 128 or 256 neurons per layer helps with the training accuracy but not that much with the validation accuracy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 256 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">256</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 256 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">256</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.924</span>, loss: <span style=" color: #7358A5;">0.253 </span>(data_loss: <span style=" color: #7358A5;">0.210</span>, reg_loss: <span style=" color: #7358A5;">0.043</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.019900507413187767</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.887</span>, loss: <span style=" color: #7358A5;">0.331</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This didn’t produce much of a change in results, but raising this number again to 512 did improve validation accuracy and loss as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 512 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">512</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 512 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">512</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.918</span>, loss: <span style=" color: #7358A5;">0.253 </span>(data_loss: <span style=" color: #7358A5;">0.210</span>, reg_loss: <span style=" color: #7358A5;">0.043</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.019900507413187767</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.920</span>, loss: <span style=" color: #7358A5;">0.256</span></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 14.03: <span class="p">Training with regularization and more training data (tuned).</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/cde" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 14.03: </a><a href="https://nnfs.io/cde" class="s9" target="_blank">https://nnfs.io/cde</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">In this case, we see that the accuracies and losses for in-sample and out-of-sample data are almost identical. From here, we could add either more layers and neurons or both. Feel free to tinker with this to try to improve it. Next, we’re going to cover another regularization method: <b>dropout</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch14" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch14<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark237">Chapter 15</a><a name="bookmark241">&zwnj;</a><a name="bookmark242">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Dropout</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Another option for neural network regularization is adding a <b>dropout layer</b>. This type of layer disables some neurons, while the others pass through unchanged. The idea here, similarly to regularization, is to prevent a neural network from becoming too dependent on any neuron or</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">for any neuron to be relied upon entirely in a specific instance (which can be common if a model overfits the training data). Another problem dropout can help with is <b>co-adoption</b>, which happens when neurons depend on the output values of other neurons and do not learn the underlying function on their own. Dropout can also help with <b>noise </b>and other perturbations in the training data as more neurons working together mean that the model can learn more complex functions.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The Dropout function works by randomly disabling neurons at a given rate during every forward pass, forcing the network to learn how to make accurate predictions with only a random part of neurons remaining. Dropout forces the model to use more neurons for the same purpose, resulting in a higher chance of learning the underlying function that describes the data. For example, if we disable one half of the neurons during the current step, and the other half during the next step,</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">we are forcing more neurons to learn the data, as only a part of them “sees” the data and gets updates in a given pass. These alternating halves of neurons are an example, and in reality, we’ll use a hyperparameter to inform the dropout layer of the number of neurons to disable randomly.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark238">Also, since active neurons are changing, dropout helps prevent overfitting, as the model can’t use specific neurons to memorize certain samples. It’s also worth mentioning that the dropout layer does not truly disable neurons, but instead zeroes their outputs. In other words, dropout does not decrease the number of neurons used, nor does it make the training process twice as fast when half the neurons are disabled.</a><a name="bookmark243">&zwnj;</a><a name="bookmark244">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Forward Pass</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">In the code, we will “turn off” neurons with a filter that is an array with the same shape as the layer output but filled with numbers drawn from a Bernoulli distribution. A <b>Bernoulli</b></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;"><b>distribution </b>is a binary (or discrete) probability distribution where we can get a value of <i>1 </i>with a probability of <i>p </i>and value of <i>0 </i>with a probability of <i>q</i>. Let’s take some random value from this distribution, <i>r</i><span class="s39">i</span>, then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">What this means is that the probability of this value being <i>1 </i>is <i>p</i>. The probability of it being <i>0 </i>is <i>q</i></p><p class="s3" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">= 1 - p<span class="p">, therefore:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">This means that the given <i>r</i><span class="s39">i </span>is an equivalent of a value from the Bernoulli distribution with a probability <i>p </i>for this value to be <i>1</i>. If <i>r</i><span class="s39">i </span>is a single value from this distribution, a draw from this distribution, reshaped to match the shape of the layer outputs, can be used as a mask to these outputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">We are returned an array filled with values of <i>1 </i>with a probability of <i>p </i>and otherwise values of <i>0</i>. We then apply this filter to the output of a layer we want to add dropout to.</p><p style="padding-left: 120pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 15.01: <span class="p">Example model with no dropout applied.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 120pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 15.02: <span class="p">Example model with 0.5 dropout.</span></h3><p style="padding-left: 118pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 15.03: <span class="p">Example model with 0.9 dropout.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/def" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 15.01-15.03: </a><a href="https://nnfs.io/def" class="s9" target="_blank">https://nnfs.io/def</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">With the code, we have one hyperparameter for a dropout layer. This is a value for the percentage of neurons to disable in that layer. For example, if you chose 0.10 for the dropout parameter, 10% of the neurons will be disabled at random during each forward pass. Before we use NumPy, we’ll demonstrate this with an example in pure Python:</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 28pt;text-align: left;">import <span style=" color: #231F20;">random dropout_rate </span>= <span style=" color: #7358A5;">0.5</span></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 11pt;text-align: left;"># Example output containing 10 values</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">example_output <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">0.27</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.03</span>, <span style=" color: #7358A5;">0.67</span>, <span style=" color: #7358A5;">0.99</span>, <span style=" color: #7358A5;">0.05</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.37<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>2.01<span style=" color: #231F20;">, </span>1.13<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.07<span style=" color: #231F20;">, </span>0.73<span style=" color: #231F20;">]</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Repeat as long as necessary <span style=" color: #C71F43;">while </span><span style=" color: #7358A5;">True</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Randomly choose index and set value to 0</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">index <span style=" color: #C71F43;">= </span>random.randint(<span style=" color: #7358A5;">0</span>, <span style=" color: #32A7BD;">len</span>(example_output) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1</span>) example_output[index] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># We might set an index that already is zeroed</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># There are different ways of overcoming this problem, # for simplicity we count values that are exactly 0</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># while it&#39;s extremely rare in real model that weights # are exactly 0, this is not the best method for sure <span style=" color: #231F20;">dropped_out </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s11" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;">for <span style=" color: #231F20;">value </span>in <span style=" color: #231F20;">example_output: </span>if <span style=" color: #231F20;">value </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">dropped_out <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If required number of outputs is zeroed - leave the loop </span>if <span style=" color: #231F20;">dropped_out </span>/ <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(example_output) </span>&gt;= <span style=" color: #231F20;">dropout_rate:</span></p><p class="s11" style="padding-left: 19pt;text-indent: 48pt;line-height: 216%;text-align: left;">break <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(example_output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.03</span>, <span style=" color: #7358A5;">0.67</span>, <span style=" color: #7358A5;">0.99</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.37</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0.73</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The code is relatively rudimentary, but the idea is to keep zeroing neuron outputs (setting them to 0) randomly until we’ve disabled whatever target % of neurons we require. If we consider a Bernoulli distribution as a special case of a Binomial distribution with <i>n=1 </i>and look at a list of available methods in NumPy, it turns out that there’s a much cleaner way to do this using <i>numpy.</i></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">random.binomial<span class="p">. A binomial distribution differs from Bernoulli distribution in one way, as it adds a parameter, </span>n, <span class="p">which is the number of concurrent experiments (instead of just one) and returns the number of successes from these </span>n <span class="p">experiments.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: justify;"><span class="s20">np.random.binomial() </span>works by taking the already discussed parameters <i>n </i>(number of experiments) and <i>p </i>(probability of the true value of the experiment) as well as an additional parameter <i>size</i>: <span class="s22">np.random.binomial(n, p, size)</span>.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The function itself can be thought of like a coin toss, where the result will be 0 or 1. The <i>n </i>is how many tosses of the coin do you want to do. The <i>p </i>is the probability for the toss result to be a 1.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The overall result is a sum of all toss results. The <i>size </i>is how many of these “tests” to run, and the return is a list of overall results. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">np.random.binomial(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">0.5</span>, <span class="s23">size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">10</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This will produce an array that is of size 10, where each element will be the sum of 2 coin tosses, where the probability of 1 will be 0.5, or 50%. The resulting array:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">array([<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">2</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can use this to create our dropout layer. Our goal here is to create a filter where the intended dropout % is represented as 0, with everything else as 1. For example, let’s say we have a dropout layer that we’ll add after a layer that consists of 5 neurons, and we wish to have a 20% dropout.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">An example of a dropout layer might look like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, ⅕ of that list is a 0. This is an example of the filter we’re going to apply to the output of the dense layer. If we multiplied a neural network’s layer output by this, we’d be effectively disabling the neuron at the same index as the 0.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">We can mimic that with <span class="s20">np.random.binomial() </span>by doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dropout_rate <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.20</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.random.binomial(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span>dropout_rate, <span class="s23">size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">array([<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is based on probabilities, so there will be times when it does not look like the above array. There could be times no neurons zero out, or all neurons zero out. On average, these random draws will tend toward the probability we desire. Also, this was an example using a very small layer (5 neurons). On a realistically sized layer, you should find the probability more consistently matches your intended value.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Assume a neural network layer’s output is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">example_output <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">0.27</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.03</span>, <span style=" color: #7358A5;">0.67</span>, <span style=" color: #7358A5;">0.99</span>, <span style=" color: #7358A5;">0.05</span>,</p><p class="s14" style="padding-top: 2pt;padding-left: 183pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.37<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>2.01<span style=" color: #231F20;">, </span>1.13<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.07<span style=" color: #231F20;">, </span>0.73<span style=" color: #231F20;">])</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, let’s assume our target dropout rate is 0.3, or 30%. We apply a dropout layer:</p><p class="s11" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np dropout_rate </span>= <span style=" color: #7358A5;">0.3</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">example_output <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">0.27</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.03</span>, <span style=" color: #7358A5;">0.67</span>, <span style=" color: #7358A5;">0.99</span>, <span style=" color: #7358A5;">0.05</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.37<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>2.01<span style=" color: #231F20;">, </span>1.13<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.07<span style=" color: #231F20;">, </span>0.73<span style=" color: #231F20;">])</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">example_output <span style=" color: #C71F43;">*= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span>dropout_rate,</p><p class="s10" style="padding-top: 1pt;padding-left: 243pt;text-indent: 0pt;text-align: left;">example_output.shape)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(example_output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">[ </span>0.27 <span style=" color: #C71F43;">-</span>1.03 0.00 0.99 0. <span style=" color: #C71F43;">-</span>0.37 <span style=" color: #C71F43;">-</span>2.01 1.13 <span style=" color: #C71F43;">-</span>0.07 0. <span style=" color: #231F20;">]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Note that our dropout rate is the ratio of neurons we intend to <i>disable (q)</i>. Sometimes, the implementation of dropout will include a rate parameter that instead means the fraction of neurons you intend to <i>keep (p)</i><a href="http://www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" class="s82" target="_blank">. At the time of writing this, the dropout parameter in deep learning frameworks, TensorFlow and Keras, represents the neurons you intend to disable. On the other hand, the dropout parameter in PyTorch and the original paper on dropout (</a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">http://www.cs.toronto.</span><a href="http://www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank"> </a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">edu/~rsalakhu/papers/srivastava14a.pdf</span>) signal the ratio of neurons you intend to keep.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The way it’s implemented is not important. What <i>is </i>important is that you know which method you’re using!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While dropout helps a neural network generalize and is helpful for training, it’s not something we want to utilize when predicting. It’s not as simple as only omitting it because the magnitude of inputs to the next neurons can be dramatically different. If you have a dropout of 50%, for example, this would suggest that, on average, your inputs to the next layer neurons will be 50% smaller when summed, assuming they are fully-connected. What that means is that we used</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">dropout during training, and, in this example, a random 50% of neurons output a value of 0 at each of the steps. Neurons in the next layer multiply inputs by weights, sum them, and receive values</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">of 0 for half of their inputs. If we don’t use dropout during prediction, all neurons will output their values, and this state won’t match the state seen during training, since the sums will be statistically about twice as big. To handle this, during prediction, we might multiply all of the outputs by</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the dropout fraction, but that’d add another step for the forward pass, and there is a better way to achieve this. Instead, we want to scale the data back up after a dropout, during the training</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">phase, to mimic the mean of the sum when all of the neurons output their values. <i>Example_output</i></p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">becomes:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">example_output <span style=" color: #C71F43;">*= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span>dropout_rate,</p><p class="s10" style="padding-top: 1pt;padding-left: 243pt;text-indent: 0pt;text-align: left;">example_output.shape) <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 2pt;padding-left: 128pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span>dropout_rate)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Notice that we added the division of the dropout’s result by the dropout rate. Since this rate is a fraction, it makes the resulting values larger, accounting for the value lost because a fraction of the neuron outputs being zeroed out. This way, we don’t have to worry about the prediction and can simply omit the dropout during prediction. In any specific example, you will find that scaling doesn’t equal the same sum as before because we’re randomly dropping neurons. That said, after enough samples, the scaling will average out overall. To prove this:</p><p class="s11" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np dropout_rate </span>= <span style=" color: #7358A5;">0.2</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 11pt;text-align: left;">example_output <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">0.27</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.03</span>, <span style=" color: #7358A5;">0.67</span>, <span style=" color: #7358A5;">0.99</span>, <span style=" color: #7358A5;">0.05</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.37<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>2.01<span style=" color: #231F20;">, </span>1.13<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.07<span style=" color: #231F20;">, </span>0.73<span style=" color: #231F20;">])</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;sum initial </span>{<span style=" color: #32A7BD;">sum</span>(example_output)}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">sums <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>i <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10000</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">example_output2 <span style=" color: #C71F43;">= </span>example_output <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">np.random.binomial(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span>dropout_rate, example_output.shape) <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1</span><span style=" color: #C71F43;">-</span>dropout_rate)</p><p class="s10" style="padding-left: 20pt;text-indent: 24pt;line-height: 216%;text-align: left;">sums.append(<span style=" color: #32A7BD;">sum</span>(example_output2)) <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;mean sum: </span>{np.mean(sums)}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">sum initial <span style=" color: #7358A5;">0.36000000000000015</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">mean sum: <span style=" color: #7358A5;">0.36282000000000014</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">It’s not exact yet, but you should get the idea.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark239">Backward Pass</a><a name="bookmark245">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The last missing piece to implement dropout as a layer is a backward pass method. As before, we need to calculate the partial derivative of the dropout operation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">When the value of element <i>r</i><span class="s39">i </span>equals <i>1</i>, its function and derivative becomes the neuron’s output, <i>z</i>, compensated for the loss value by <i>1-q</i>, where <i>q </i>is the dropout rate, as we just described:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><span class="p">That’s because the derivative with respect to </span>z <span class="p">of </span>z <span class="p">is 1, and we treat the rest as a constant. When</span>r<span class="s39">i</span>=0<span class="p">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">And that’s because we are zeroing this element of the dropout filter, and the derivative of any constant value (including <i>0</i>) is <i>0</i>. Let’s combine both cases and denote <i>Dropout </i>as <i>Dr</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">i <span class="p">denotes the index of the given input (and the layer output). When we write a derivative of the dropout function this way, we can simplify it to a value from the Bernoulli distribution divided by </span>1-q<span class="p">, which is identical to our scaled mask, the function the dropout applies during the forward</span></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">pass, as it’s also either <i>1 </i>divided by <i>1-q</i>, or <i>0</i>. Thus, we can save this mask during the forward pass and use it with the chain rule as the gradient of this function.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark240">The Code</a><a name="bookmark246">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can now implement this concept in a new layer type, the dropout layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Save input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s take this new dropout layer, and add it between our two dense layers. First defining it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 189pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dropout layer </span>dropout1 <span style=" color: #C71F43;">= </span>Layer_Dropout(<span style=" color: #7358A5;">0.1</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">During the forward pass, add in the dropout:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through Dropout layer <span style=" color: #231F20;">dropout1.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 118%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(dropout1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And of course in the backward pass:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 118%;text-align: left;">dropout1.backward(dense2.dinputs) activation1.backward(dropout1.dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s also raise the learning rate a bit, from 0.02 to 0.05 and raise the learning rate decaying from 5e-7 to 5e-5 as these parameters work better with our model and dropout layer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full code up to now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Save input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get corrected cache</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: -42pt;line-height: 108%;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">* </span>\ layer.weights)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer&#39;s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1000</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 189pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dropout layer </span>dropout1 <span style=" color: #C71F43;">= </span>Layer_Dropout(<span style=" color: #7358A5;">0.1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 3 output values (output values)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Softmax classifier&#39;s combined loss and activation </span>loss_activation <span style=" color: #C71F43;">= </span>Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.05</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-5</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through Dropout layer <span style=" color: #231F20;">dropout1.forward(activation1.output)</span></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(dropout1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>data_loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate regularization penalty </span>regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss_activation.loss.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\ loss_activation.loss.regularization_loss(dense2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>np.argmax(y, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_activation.backward(loss_activation.output, y) dense2.backward(loss_activation.dinputs) dropout1.backward(dense2.dinputs) activation1.backward(dropout1.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Validate the model</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create test dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our testing data through this layer <span style=" color: #231F20;">dense1.forward(X_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform a forward pass through the activation/loss function # takes the output of second dense layer here and returns loss </span>loss <span style=" color: #C71F43;">= </span>loss_activation.forward(dense2.output, y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # calculate values along first axis</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>np.argmax(loss_activation.output, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) <span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">len</span>(y_test.shape) <span style=" color: #C71F43;">== </span><span style=" color: #7358A5;">2</span>:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 108%;text-align: left;">y_test <span style=" color: #C71F43;">= </span>np.argmax(y_test, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>) accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.668</span>, loss: <span style=" color: #7358A5;">0.733 </span>(data_loss: <span style=" color: #7358A5;">0.717</span>, reg_loss: <span style=" color: #7358A5;">0.016</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0334459346466437</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.688</span>, loss: <span style=" color: #7358A5;">0.727 </span>(data_loss: <span style=" color: #7358A5;">0.711</span>, reg_loss: <span style=" color: #7358A5;">0.016</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.03333444448148271</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.757</span>, loss: <span style=" color: #7358A5;">0.712</span></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 15.04: <span class="p">Model trained with dropout.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/efg" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 15.04: </a><a href="https://nnfs.io/efg" class="s9" target="_blank">https://nnfs.io/efg</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While our accuracy and loss have suffered considerably, we’ve found a scenario where our validation set performs <i>better </i>than our in-sample set (because we do not apply dropout when testing so you don’t disable some of the connections). Further tweaking would likely fix the accuracy issue; for example, due to our regularization tactics, we can change our layer sizes to 512:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 512 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">512</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dropout layer </span>dropout1 <span style=" color: #C71F43;">= </span>Layer_Dropout(<span style=" color: #7358A5;">0.1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 512 input features # and 3 output values</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">512</span>, <span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Adding more neurons ends with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.373</span>, loss: <span style=" color: #7358A5;">1.099 </span>(data_loss: <span style=" color: #7358A5;">1.099</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.05</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.719</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.735</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.672<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: left;">0.063<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04975371909050202</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.782</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.627</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.548<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.079<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.049507401356502806</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.800</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.603</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.521<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.082<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0492635105177595</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.802</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.595</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.513<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.082<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04902201088288642</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">500</span>, acc: <span style=" color: #7358A5;">0.809</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.562</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.482<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.079<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.048782867456949125</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">600</span>, acc: <span style=" color: #7358A5;">0.836</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.521</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.445<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.076<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04854604592455945</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">700</span>, acc: <span style=" color: #7358A5;">0.816</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.532</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.457<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.076<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.048311512633460556</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">800</span>, acc: <span style=" color: #7358A5;">0.839</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.515</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.442<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.073<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.04807923457858551</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">900</span>, acc: <span style=" color: #7358A5;">0.842</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.499</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.426<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: left;">0.072<span style=" color: #231F20;">),</span></p></td><td style="width:21pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: left;">lr:</p></td></tr><tr style="height:13pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.04784917938657352</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:21pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">1000</span>, acc: <span style=" color: #7358A5;">0.837</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;text-align: right;">0.480</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;text-align: right;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;text-align: right;">0.408<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:69pt" colspan="2"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">0.071<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">lr: <span style=" color: #7358A5;">0.04762131530072861</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:69pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:69pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">9800</span>, acc: <span style=" color: #7358A5;">0.848</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.443</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.391<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:69pt" colspan="2"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.052<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">lr: <span style=" color: #7358A5;">0.033558173093056816</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:69pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.841</span>,</p></td><td style="width:36pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.468</p></td><td style="width:73pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: right;">0.416<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:69pt" colspan="2"><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.052<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:13pt"><td style="width:148pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0334459346466437</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:69pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.859</span>, loss: <span style=" color: #7358A5;">0.468 </span>(data_loss: <span style=" color: #7358A5;">0.417</span>, reg_loss: <span style=" color: #7358A5;">0.051</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.03333444448148271</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.857</span>, loss: <span style=" color: #7358A5;">0.397</span></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 15.05: <span class="p">Model trained with dropout and bigger hidden layer.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/fgh" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 15.05: </a><a href="https://nnfs.io/fgh" class="s9" target="_blank">https://nnfs.io/fgh</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Pretty good result, but worse compared to the “no dropout” model. Interestingly, validation accuracy is close to the training accuracy with dropout — usually validation accuracy will be higher, so we might suspect these as signs of overfitting here (validation loss is lower than expected).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch15" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch15<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark247">Chapter 16</a><a name="bookmark256">&zwnj;</a><a name="bookmark257">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Binary Logistic Regression</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we’ve learned how to create and train a neural network, let’s consider an alternative output layer for a neural network. Until now, we’ve used an output layer that is a probability distribution, where all of the values represent a confidence level of a given class being the correct class, and where these confidences sum to <i>1</i>. We’re now going to cover an alternate output layer option, where each neuron separately represents two classes <span style=" color: #212121;">— </span>0 for one of the classes, and a 1 for the other. A model with this type of output layer is called <b>binary logistic regression</b>. This single neuron could represent two classes like <i>cat </i>vs. <i>dog</i>, but it could also represent <i>cat </i>vs. <i>not cat </i>or any combination of 2 classes, and you could have many of these. For example, a model may have two binary output neurons. One of these neurons could be distinguishing between <i>person/not person</i>, and the other neuron could be deciding between <i>indoors/outdoors</i>. Binary logistic regression is a regressor type of algorithm, which will differ as we’ll use a <b>sigmoid </b>activation function for the output layer rather than <b>softmax</b>, and <b>binary cross-entropy </b>rather than <b>categorical cross-entropy </b>for calculating loss.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark248">Sigmoid Activation Function</a><a name="bookmark258">&zwnj;</a><a name="bookmark259">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The sigmoid activation function is used with regressors because it “squishes” a range of outputs from negative infinity to positive infinity to be between 0 and 1. The bounds represent the two possible classes. The sigmoid equation is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">For the purpose of neural networks, we’ll use our common notation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;">The denominator of the <b>Sigmoid </b>function contains <i>e </i>raised to the power of <i>z</i><span class="s39">i,j</span>, where <i>z</i>, given indices, means a singular output value of the layer that this activation function takes as input. The index <i>i </i>means the current sample, and the index <i>j </i>means the current output in this sample.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">If we plot the sigmoid function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 66pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 16.1: <span class="p">The sigmoid function graph.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 130%;text-align: left;">Note the output from this function averages at <i>0.5</i>, and squishes down to a flat line as it approaches <i>0 </i>or <i>1</i>. The sigmoid function approaches both maximum and minimum values exponentially fast. For example, for an input of <i>2</i>, the output is <i>~0.88</i>, which is already pretty close to <i>1</i>. With an input of <i>3</i>, the output is <i>~0.95</i>, and so on. It’s also similar for negative values: σ(-2) ≈ 0.12 and σ(-3) ≈ 0.05. This property makes the sigmoid activation function a good candidate to apply to the final layer’s output with a binary logistic regression model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 7pt;text-indent: 0pt;line-height: 16pt;text-align: left;"><span class="p">For commonly-used functions, such as the sigmoid function, the derivatives are almost always public knowledge. Unless you’re inventing a function, you won’t need to calculate derivatives by hand, but it can still be a good exercise. The sigmoid function’s derivative solves to </span>σ<span class="s39">i,j</span>(1-σ<span class="s39">i,j</span>)<span class="p">. If you would like to leverage this fact without diving into the mathematical derivation, feel free to skip to the next section.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark249">Sigmoid Function Derivative</a><a name="bookmark260">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s define the derivative of the <b>Sigmoid </b>function with respect to its input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At this point, we might start calculating the derivative of the division operation, but, since the numerator contains just the value <i>1</i>, the whole fraction is effectively just a reciprocal of its denominator and can be represented as its negative power:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It’s easier to calculate the derivative of the power operation than the derivative of the division operation, so let’s update our equation to follow this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now, we can calculate the derivative of the expression raised to the power of the <i>-1</i>, which equals this exponent multiplied by the expression itself, raised to the power lowered by <i>1</i>. Then, following the chain rule, we have to calculate the derivative of the expression itself:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">As we already learned, the derivative of the sum operation is the sum of derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The derivative of <i>1 </i>with respect to <i>z</i><span class="s39">i,j </span>equals <i>0</i>, as the derivative of a constant is always <i>0</i>. The</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;">derivative of the constant <i>e </i>raised to the power <i>-z</i><span class="s39">i,j </span>equals this value multiplied by the derivative of the exponent:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">The derivative of the <i>-z</i><span class="s39">i,j </span>with respect to <i>z</i><span class="s39">i,j </span>equals <i>-1 </i>as <i>-1 </i>is a constant and can be moved outside of the derivative, leaving us with the derivative of <i>z</i><span class="s39">i,j </span>with respect to <i>z</i><span class="s39">i,j </span>which, as we know, equals <i>1</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Now we can move the minus sign outside of the parentheses and cancel out the other minus:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Let’s rewrite the resulting equation — the expression raised to the power of <i>-2 </i>can be written as its reciprocal raised to the power of <i>2</i>, then the multiplier (the value we multiply by) from the equation can become the numerator of the resulting fraction:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The denominator of this fraction can be written as the multiplication of the expression by itself instead of raising it to the power of <i>2</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 16pt;text-align: left;">Now we can split this fraction into two separate ones — one containing <i>1 </i>in the numerator and the other one <i>e </i>to the power of <i>-z</i><span class="s39">i,j</span>, both having each of the expressions that are separated by the multiplication operator in the denominator in their respective denominators. We can do this as we are performing the multiplication operation between both fractions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you remember the equation of the sigmoid function, you might already see where we are going with this — the multiplicand (the value that is being multiplied by the multiplier) is the equation of the sigmoid function. Let’s work on this equation further — it’d be ideal if the numerator of</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the multiplicator could be represented as some sort of equation containing the sigmoid function’s equation as well. What we can do is add <i>1 </i>and remove <i>1 </i>from it as it won’t change its value:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">What this allows us to do is split the multiplicator into two separate fractions by the minus sign in the multiplicator:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The minuend (the value we are subtracting from) of the multiplicator equals <i>1 </i>as the numerator, and the denominator of the fraction are equal, and the subtrahend (the value we are subtracting from the minuend) is actually the equation of the sigmoid function as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It turns out that the derivative of the sigmoid function equals this function multiplied by the difference of <i>1 </i>and this function as well. That allows us to easily write this derivative in the code.</p><p class="s46" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark250">Sigmoid Function Code</a><a name="bookmark261">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As with other activation functions, we’ll write a forward pass method and a backward pass method. For the forward pass, we’ll take the inputs and apply the sigmoid function. For the backward pass, we’ll leverage the sigmoid function’s derivative, which, as we figured out during derivation of the sigmoid function’s derivative, equals the sigmoid output from the forward pass multiplied by the difference of 1 and this output.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Save input and calculate/save output # of the sigmoid function</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.output </span>= <span style=" color: #7358A5;">1 </span>/ <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1 </span>+ <span style=" color: #231F20;">np.exp(</span>-<span style=" color: #231F20;">inputs))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative - calculates from output of the sigmoid function </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.output) <span style=" color: #C71F43;">* </span>self.output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we have the new activation function, we need to code our new calculation for the binary cross-entropy loss.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark251">Binary Cross-Entropy Loss</a><a name="bookmark262">&zwnj;</a><a name="bookmark263">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 17pt;padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate binary cross-entropy loss, we will continue to use the negative log concept from categorical cross-entropy loss. Rather than only calculating this on the target class, we will sum the log-likelihoods of the correct and incorrect classes for each neuron separately. Because class values are either <i>0 </i>or <i>1</i>, we can simplify the incorrect class to be <i>1-correct class </i>as this inverts the value. We can then calculate the negative log-likelihood of the correct and incorrect classes, adding them together. We are presenting two forms of the equation — the first is following the description just given, then the optimized version differentiating only in the minus signs being moved over and redundant parentheses removed:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In code, this will start as (but will be modified shortly, so do not commit this to your codebase yet):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since a model can contain multiple binary outputs, and each of them, unlike in the cross-entropy loss, outputs its own prediction, loss calculated on a single output is going to be a vector of losses containing one value for each output. What we need is a sample loss and, to achieve that, we need to calculate a mean of all of these losses from a single sample:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Where index <i>i </i>means the current sample, the index <i>j </i>means the current output in this sample, and the <i>J </i>means the number of outputs. Since we are operating on a set of samples (the output is an array containing the set of loss vectors), we can use NumPy to perform this operation on a single call:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 62pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">The last parameter, <span class="s29">axis</span><span class="s15">=-</span><span class="s21">1</span>, informs NumPy to calculate the mean value along the last dimension. To make it easier to visualize, let’s use a simple example. Assume that this is an output of the model containing 3 neurons in the output layer, and it’s passed through the binary cross- entropy loss function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">outputs <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">2</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">6<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">0</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">5<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">10<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">11</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">12<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">13<span style=" color: #231F20;">],</span></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[<span style=" color: #7358A5;">5</span>,</p></td><td style="width:24pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">10<span style=" color: #231F20;">,</span></p></td><td style="width:36pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">15<span style=" color: #231F20;">]])</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">These numbers are completely made up for this example. We want to take each of the output vectors, </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>] <span class="p">for example, and calculate a mean value from the numbers they hold, putting the result on the output vector. We then want to repeat this for the other vectors and return the resulting vector, which will be a one-dimensional array. Using NumPy:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.mean(outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">array([ <span style=" color: #7358A5;">2.</span>, <span style=" color: #7358A5;">4.</span>, <span style=" color: #7358A5;">5.</span>, <span style=" color: #7358A5;">12.</span>, <span style=" color: #7358A5;">10.</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If we calculate the mean value of the first output, it’s indeed <i>2</i>, the mean value of the second output is indeed <i>4</i>, and so on.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We are also going to inherit from the <b>Loss </b>class, so the overall loss calculation will be handled by the <span class="s35">calculate </span>method that we already created for the categorical cross-entropy loss class.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark252">Binary Cross-Entropy Loss Derivative</a><a name="bookmark264">&zwnj;</a><a name="bookmark265">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate the gradient from here, we already know that the derivative for the natural logarithm is <i>1/x </i>and that the derivative of <i>1-x </i>is <i>-1</i>. In simplified form, this gives us <i>-(y_true / y + (1 - y_ true) / (1 - y)) · (-1)</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate the partial derivative of this loss function with respect to the predicted input, we’ll use the latter version of the loss equation. It doesn’t really matter in this case which one we use:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 11pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The expression that we have to calculate the partial derivative of consists of two sub-expressions, which are components of the sum operation. We can write that as the sum of derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Both components contain <i>y</i><span class="s39">i,j </span>(the target value) inside of their derivatives, which are the constants that we are deriving with respect to <i>y-hat</i><span class="s39">i,j </span>(the predicted value, which is a different variable), so we can move them outside of the derivative along with the other constants and minus sign:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Now, like in the <b>Categorical Cross-Entropy </b>loss’ derivative, we have to calculate the derivative of the logarithmic function, which equals the reciprocal of its parameter multiplied (following the chain rule) by the derivative of this parameter. Let’s apply that to both of the partial derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Now the first partial derivative equals <i>1</i>, since the value we derive, and the value we derive with respect to, are the same values. The second partial derivative can be written as the difference of</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">the derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">From the two new derivatives, the first one equals <i>0 </i>as the derivative of the constant always equals <i>0</i>, then the second derivative equals <i>1 </i>as the value we derive, and the value we derive with respect to, are the same values:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can finally clean up to get the resulting equation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The partial derivative of the <b>Binary Cross-Entropy </b>loss solves to a pretty simple equation that will be easy to implement in code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This partial derivative is a derivative of the single output’s loss and, with any type of output, we always need to calculate it with respect to a sample loss, not an atomic output loss, since we</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">have to calculate the mean value of all output losses in a sample to form a <i>sample loss </i>during the forward pass:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">For backpropagation, we have to calculate the partial derivative of the <i>sample loss </i>with respect to each input:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have just calculated the second derivative, the partial derivative of the single output loss with respect to the related prediction. We have to calculate the partial derivative of the sample loss with respect to the single output loss:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l5"><li><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">divided by <i>J </i>(the number of outputs), is a constant and can be moved outside of the derivative. Since we are calculating the derivative with respect to a given output, <i>j</i>, the sum of one element equals this element:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The remaining derivative equals <i>1 </i>as the derivative of a variable with respect to the same variable equals <i>1</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark253">Now we can update the equation of the partial derivative of a sample loss with respect to a single output loss by applying the chain rule:</a><a name="bookmark266">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We have to perform this normalization since each output returns its own derivative, and without normalization, each additional input will raise gradients and require changing other hyperparameters, including the learning rate.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Binary Cross-Entropy Code</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">In our code, this will be:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 2pt;padding-left: 171pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Similar to what we did in the categorical cross-entropy loss, we need to normalize gradient so it’ll become invariant to the number of samples we calculate it for:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize gradient</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Finally, we need to address the numerical instability of the logarithmic function. The sigmoid activation can return a value in the range of <i>0 </i>to <i>1 </i>(inclusive), but the <i>log(0) </i>presents a slight issue due to how it’s calculated and will return <i>negative infinity</i>. This alone isn’t necessarily a big deal, but any list with <i>-inf </i>in it will have a mean of <i>-inf</i>, which is the same for any list with positive infinity averaging to infinity.</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 227%;text-align: left;"><span style=" color: #C71F43;">import </span>numpy <span style=" color: #C71F43;">as </span>np <span style=" color: #32A7BD;">print</span>(np.log(<span style=" color: #7358A5;">0</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><u>  </u>main<u> </u>:<span style=" color: #7358A5;">1</span>: <span class="s31">RuntimeWarning</span>: divide by zero encountered in log</p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #231F20;">inf</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(np.mean([<span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">4</span>, np.log(<span style=" color: #7358A5;">0</span>)]))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><u>  </u>main<u> </u>:<span style=" color: #7358A5;">1</span>: <span class="s31">RuntimeWarning</span>: divide by zero encountered in log</p><p class="s11" style="padding-top: 3pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #231F20;">inf</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is a similar issue to the one we discussed earlier regarding categorical cross-entropy loss in chapter 5. To prevent this issue, we’ll add clipping on the batch of values:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We now will use these clipped values for the forward pass, rather than the originals:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we perform the division operation during the derivative calculation, the gradient passed in may contain both values, <i>0 </i>and <i>1</i>. Either of these values will cause a problem in either the</p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;line-height: 13pt;text-align: left;">y_true <span style=" color: #C71F43;">/ </span>dvalues <span class="p">or </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>dvalues) <span class="p">parts respectively (</span><span class="s3">0 </span><span class="p">in the first</span></p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;line-height: 110%;text-align: left;">and <i>1-1=0 </i>in the second case will also cause division by <i>0</i>), so we need to clip this gradient as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now, similar to the forward pass, we can use these clipped values:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The full code now for the binary cross-entropy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Binary cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_BinaryCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 108pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped)) sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 102pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we have this new activation function and loss calculation, we’ll make edits to our existing softmax classifier to implement the binary logistic regression model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 11pt;padding-left: 8pt;text-indent: 0pt;line-height: 94%;text-align: left;"><a name="bookmark254">Implementing Binary Logistic Regression and Binary Cross-Entropy Loss</a><a name="bookmark267">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">With these new classes, our code changes will be in the execution of actual code (instead of modifying the classes). The first change is to make the <span class="s22">spiral_data </span>object output 2 classes, rather than 3, like so:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we’ll reshape our labels, as they’re not sparse anymore. They’re binary, <i>0 </i>or <i>1</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Reshape labels to be a list of lists</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Inner list contains one output (either 0 or 1) # per each output neuron, 1 in this case</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>y.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Consider the difference here. Initially, the <i>y </i>output from the <span class="s22">spiral_data </span>function would look something like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">2</span>) <span style=" color: #32A7BD;">print</span>(y[:<span style=" color: #7358A5;">5</span>])</p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 0 0 0 0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we reshape it here for binary logistic regression:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">y <span style=" color: #C71F43;">= </span>y.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #32A7BD;">print</span>(y[:<span style=" color: #7358A5;">5</span>])</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">&gt;&gt;&gt; </span>[[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-left: 26pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0</span>]]</p><p class="s22" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;"><span class="p">Why have we done this? Initially, with the softmax classifier, the values from </span>spiral_data <span class="p">could be used directly as the target labels, as they contain the correct class labels in numerical form — an index of the correct class, where each neuron in the output layer is a separate class, for example </span>[<span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">0</span>, <span style=" color: #7358A5;">1</span>]<span class="p">. In this case, however, we’re trying to represent some binary outputs, where each neuron represents 2 possible classes on its own. For the example we’re currently working on, we have a single output neuron so the output from our neural network should be a tensor (array), containing one value, of a target value of either </span><span class="s3">0 </span><span class="p">or </span><span class="s3">1</span><span class="p">, for example, </span>[[<span style=" color: #7358A5;">0</span>], [<span style=" color: #7358A5;">1</span>], [<span style=" color: #7358A5;">1</span>], [<span style=" color: #7358A5;">0</span>], [<span style=" color: #7358A5;">1</span>]]<span class="p">. The </span>.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span class="p">means to reshape the data into</span></p></li><li><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">dimensions, where the second dimension contains a single element, and the first dimension contains how many elements the result will contain (<span class="s15">-</span><span class="s21">1</span>) following other conditions. You are allowed to use <span class="s15">-</span><span class="s21">1 </span>only once in a shape with NumPy, letting you have that dimension be variable. Thanks to this ability, we do not always need the same number of samples every time, and NumPy can handle the calculation for us. In the case above, they’re all <i>0 </i>because the <span class="s22">spiral_data </span>function makes the dataset one class at a time, starting with <i>0</i>. We will also need to reshape the</p></li></ol><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">y-testing data in the same way.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s create our layers and use the appropriate activation functions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span style=" color: #7358A5;">100</span>, <span style=" color: #7358A5;">2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Reshape labels to be a list of lists</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Inner list contains one output (either 0 or 1) # per each output neuron, 1 in this case</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>y.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 189pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 1 output value</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create Sigmoid activation:</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">activation2 <span style=" color: #C71F43;">= </span>Activation_Sigmoid()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 114%;text-align: left;">Notice that we’re still using the rectified linear activation for the hidden layer. The hidden layer activation functions don’t necessarily need to change, even though we’re effectively building a different type of classifier. You should also notice that because this is now a binary classifier, the <span class="s22">dense2 </span>object has only 1 output. Its output represents exactly 2 classes (<i>0 </i>or <i>1</i>) being mapped to one neuron. We can now select a loss function and optimizer. For the <b>Adam </b>optimizer settings, we</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">are going to use the default learning rate and the decaying of <i>5e-7</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_BinaryCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While we require a different calculation for loss (since we use a different activation function for the output layer), we can still use the same optimizer as in the softmax classifier. Another</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">small change is how we measure predictions. With probability distributions, we use <i>argmax </i>and determine which index is associated with the largest value, which becomes the classification result. With a binary classifier, we are determining if the output is closer to <i>0 </i>or to <i>1</i>. To do this, we simplify the output to:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>(activation2.output <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">This results in <i>True</i>/<i>False </i>evaluations to the statement that the output is above <i>0.5 </i>for all values. <i>True </i>and <i>False</i>, when treated as numbers, are <i>1 </i>and <i>0</i>, respectively. For example, if we execute: <span class="s64">int</span><span class="s22">(</span><span class="s21">True</span><span class="s22">)</span>, the result will be <span class="s21">1 </span>and <span class="s64">int</span><span class="s22">(</span><span class="s21">False</span><span class="s22">) </span>will be <span class="s21">0</span>. If we want to convert a list of <i>True/ False </i>boolean values to numbers, we can’t just wrap the list in <span class="s64">int</span><span class="s22">()</span>. However, we <i>can </i>perform math operations directly on an array of boolean values and return the arithmetic answer. For example, we can run:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">a <span style=" color: #C71F43;">= </span>np.array([<span style=" color: #7358A5;">True</span>, <span style=" color: #7358A5;">False</span>, <span style=" color: #7358A5;">True</span>]) <span style=" color: #32A7BD;">print</span>(a)</p><p class="s11" style="padding-top: 7pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[ <span style=" color: #7358A5;">True False True</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">b <span style=" color: #C71F43;">= </span>a<span style=" color: #C71F43;">*</span><span style=" color: #7358A5;">1 </span><span style=" color: #32A7BD;">print</span>(b)</p><p class="s11" style="padding-top: 8pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1 0 1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Thus, to evaluate predictive accuracy, we can do the following in our code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>(activation2.output <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1 </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions<span style=" color: #C71F43;">==</span>y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">The <span class="s15">* </span><span class="s21">1 </span>multiplication turns an array of boolean True/False values into numerical 1/0 values, respectively. We will need to implement this accuracy calculation for validation data too.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark255">Full code up to this point:</a><a name="bookmark268">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Save input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output </span>jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 268pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Save input and calculate/save output # of the sigmoid function</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.output </span>= <span style=" color: #7358A5;">1 </span>/ <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1 </span>+ <span style=" color: #231F20;">np.exp(</span>-<span style=" color: #231F20;">inputs))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative - calculates from output of the sigmoid function </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.output) <span style=" color: #C71F43;">* </span>self.output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get corrected cache</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer&#39;s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Binary cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_BinaryCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 108pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped)) sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 102pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Reshape labels to be a list of lists</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Inner list contains one output (either 0 or 1) # per each output neuron, 1 in this case</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>y.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 2 input features and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-left: 189pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 1 output value</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create Sigmoid activation:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">activation2 <span style=" color: #C71F43;">= </span>Activation_Sigmoid()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_BinaryCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through second Dense layer # takes outputs of activation function</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate the data loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation2.output, y)</p><p class="s10" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate regularization penalty </span>regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss_function.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\ loss_function.regularization_loss(dense2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate accuracy from output of activation2 and targets</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Part in the brackets returns a binary mask - array consisting</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># of True/False values, multiplying it by 1 changes it into array # of 1s and 0s</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>(activation2.output <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1 </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39;</span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_function.backward(activation2.output, y) activation2.backward(loss_function.dinputs) dense2.backward(activation2.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 28pt;text-align: left;"># Validate the model # Create test dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Reshape labels to be a list of lists</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Inner list contains one output (either 0 or 1) # per each output neuron, 1 in this case</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">y_test <span style=" color: #C71F43;">= </span>y_test.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our testing data through this layer <span style=" color: #231F20;">dense1.forward(X_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through second Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Calculate the data loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation2.output, y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Calculate accuracy from output of activation2 and targets</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Part in the brackets returns a binary mask - array consisting of # True/False values, multiplying it by 1 changes it into array</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># of 1s and 0s</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>(activation2.output <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1 </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(predictions <span style=" color: #C71F43;">== </span>y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.500</span>, loss: <span style=" color: #7358A5;">0.693 </span>(data_loss: <span style=" color: #7358A5;">0.693</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.001</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:145pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.630</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.674</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.673<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.001<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">lr:</p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009999505024501287</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.625</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.669</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.668<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.001<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009999005098992651</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.650</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.664</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.663<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.002<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.000999850522346909</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.650</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.659</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.657<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.002<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009998005397923115</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">500</span>, acc: <span style=" color: #7358A5;">0.675</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.647</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.644<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.004<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009997505622347225</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">epoch: <span style=" color: #7358A5;">600</span>, acc: <span style=" color: #7358A5;">0.720</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.632</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.625<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.006<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr:</p></td></tr><tr style="height:14pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009997005896733929</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:145pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">1500</span>, acc: <span style=" color: #7358A5;">0.805</span>, loss: <span style=" color: #7358A5;">0.503 </span>(data_loss: <span style=" color: #7358A5;">0.464</span>, reg_loss: <span style=" color: #7358A5;">0.039</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009992510613295335</span></p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">2500</span>, acc: <span style=" color: #7358A5;">0.855</span>, loss: <span style=" color: #7358A5;">0.430 </span>(data_loss: <span style=" color: #7358A5;">0.379</span>, reg_loss: <span style=" color: #7358A5;">0.052</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009987520593019025</span></p><p class="s14" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">4500</span>, acc: <span style=" color: #7358A5;">0.910</span>, loss: <span style=" color: #7358A5;">0.346 </span>(data_loss: <span style=" color: #7358A5;">0.285</span>, reg_loss: <span style=" color: #7358A5;">0.061</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009977555488927658</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.511pt" cellspacing="0"><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">epoch: <span style=" color: #7358A5;">4600</span>, acc: <span style=" color: #7358A5;">0.905</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.340</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.278<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:47pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.062<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr: <span style=" color: #7358A5;">0.000997705775569079</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:47pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">epoch: <span style=" color: #7358A5;">4700</span>, acc: <span style=" color: #7358A5;">0.910</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.330</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.268<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:47pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.062<span style=" color: #231F20;">),</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009976560072110577</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">4800</span>, acc: <span style=" color: #7358A5;">0.920</span>, loss: <span style=" color: #7358A5;">0.326 </span>(data_loss: <span style=" color: #7358A5;">0.263</span>, reg_loss: <span style=" color: #7358A5;">0.063</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009976062438179587</span></p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">6100</span>, acc: <span style=" color: #7358A5;">0.940</span>, loss: <span style=" color: #7358A5;">0.291 </span>(data_loss: <span style=" color: #7358A5;">0.223</span>, reg_loss: <span style=" color: #7358A5;">0.069</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009969597711777935</span></p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5552pt" cellspacing="0"><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">epoch: <span style=" color: #7358A5;">6600</span>, acc: <span style=" color: #7358A5;">0.950</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.279</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.211<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.068<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr: <span style=" color: #7358A5;">0.000996711350897713</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">epoch: <span style=" color: #7358A5;">6700</span>, acc: <span style=" color: #7358A5;">0.955</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.272</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.203<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.069<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:187pt" colspan="2"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009966616816971556</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">epoch: <span style=" color: #7358A5;">6800</span>, acc: <span style=" color: #7358A5;">0.955</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">0.269</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">0.200<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">0.069<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 7pt;text-indent: 0pt;line-height: 12pt;text-align: center;">lr: <span style=" color: #7358A5;">0.00099661201744669</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:151pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">epoch: <span style=" color: #7358A5;">6900</span>, acc: <span style=" color: #7358A5;">0.960</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.266</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.197<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.069<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:14pt"><td style="width:187pt" colspan="2"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009965623581455767</span></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9800</span>, acc: <span style=" color: #7358A5;">0.965</span>, loss: <span style=" color: #7358A5;">0.222 </span>(data_loss: <span style=" color: #7358A5;">0.158</span>, reg_loss: <span style=" color: #7358A5;">0.063</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009951243880606966</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.965</span>, loss: <span style=" color: #7358A5;">0.221 </span>(data_loss: <span style=" color: #7358A5;">0.157</span>, reg_loss: <span style=" color: #7358A5;">0.063</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009950748768967994</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.965</span>, loss: <span style=" color: #7358A5;">0.219 </span>(data_loss: <span style=" color: #7358A5;">0.156</span>, reg_loss: <span style=" color: #7358A5;">0.063</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009950253706593885</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.945</span>, loss: <span style=" color: #7358A5;">0.207</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">The model performed quite well here! You should have some intuition about tweaking the output layer to better fit the problem you’re attempting to solve while keeping your hidden layers mostly the same. In the next chapter, we’re going to work on regression, where our intended output is not a classification at all, but rather to predict a scalar value, like the price of a house.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch16" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch16<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark269">Chapter 17</a><a name="bookmark280">&zwnj;</a><a name="bookmark281">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Regression</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Up until this point, we’ve been working with classification models, where we try to determine <i>what </i>something is. Now we’re curious about determining a <i>specific </i>value based on an input. For example, you might want to use a neural network to predict what the temperature will be tomorrow or what the price of a car should be. For a task like this, we need something with a</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">much more granular output. This also means that we require a new way to measure loss, as well as a new output layer activation function! It also means our data are different. We need training data that have target scalar values, not classes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">sine_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">plt.plot(X, y) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The data above will produce a graph like:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 82pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.01: <span class="p">The sine data graph.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark270">Linear Activation</a><a name="bookmark282">&zwnj;</a><a name="bookmark283">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we’re no longer using classification labels and want to predict a scalar value, we’re going to use a linear activation function for the output layer. This linear function does not modify its input and passes it to the output: <i>y=x</i>. For the backward pass, we already know the derivative of <i>f(x)=x </i>is <i>1</i>; thus, the full class for our new linear activation function is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Linear activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Linear</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Just remember values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># derivative is 1, 1 * dvalues = dvalues - the chain rule </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This might raise a question — why do we even write some code that does nothing? We just pass inputs to outputs for the forward pass and do the same with gradients during the backward pass since, to apply the chain rule, we multiply incoming gradients by the derivative, which is <i>1</i>. We do this only for completeness and clarity to see the activation function of the output layer in the model definition code. From a computational time point of view, this adds almost nothing to the processing time, at least not enough to noticeably impact training times.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now we just need to figure out loss!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark271">Mean Squared Error Loss</a><a name="bookmark284">&zwnj;</a><a name="bookmark285">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we aren’t working with classification labels anymore, we cannot calculate cross-entropy. Instead, we need some new methods. The two main methods for calculating error in regression are <b>mean squared error </b>(MSE) and <b>mean absolute error </b>(MAE).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With <b>mean squared error</b>, you square the difference between the predicted and true values of single outputs (as the model can have multiple regression outputs) and average those squared values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Where <i>y </i>means the target value, <i>y-hat </i>means predicted value, index <i>i </i>means the current sample, index <i>j </i>means the current output in this sample, and the <i>J </i>means the number of outputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The idea here is to penalize more harshly the further away we get from the intended target.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark272">Mean Squared Error Loss Derivative</a><a name="bookmark286">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The partial derivative of squared error with respect to the predicted value is:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">1 <span class="p">divided by </span>J <span class="p">(the number of outputs) is a constant and can be moved outside of the derivative. Since we are calculating the derivative with respect to the given output, </span>j<span class="p">, the sum of one element equals this element:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">To calculate the partial derivative of an expression to the power of some value, we need to multiply this exponent by the expression, subtract 1 from the exponent, and multiply this by the partial derivative of the inner function:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">The partial derivative of the subtraction equals the subtraction of the partial derivatives:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The partial derivative of the ground truth value with respect to the predicted value equals <i>0 </i>since we treat other variables as constants. The partial derivative of the predicted value with respect to itself equals <i>1</i>, which results in <i>0-1=-1</i>. This is multiplied by the rest of the equation and forms the solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a name="bookmark273">Full solution:</a><a name="bookmark287">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">The partial derivative equals <i>-2</i>, multiplied by the subtraction of the true and predicted values, and then divided by the number of outputs to normalize the gradients, making their magnitude invariant to the number of outputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">Mean Squared Error (MSE) Loss Code</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">The code for MSE includes an implementation of the equation to calculate the sample loss from multiple outputs. <span class="s29">axis</span><span class="s15">=-</span><span class="s21">1 </span>with the mean calculation was explained in the previous chapter in detail and, in short words, it informs NumPy to calculate mean across outputs, for each sample separately. For the backward pass, we implemented the derivative equation, which results in</p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">-2 <span class="p">multiplied by the difference of true and predicted values, and normalized by the number of outputs. Similarly to the other loss function implementations, we also normalize gradients by the number of samples to make them invariant to the batch size, or the number of samples in general:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Mean Squared Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanSquaredError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L2 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean((y_true <span style=" color: #C71F43;">- </span>y_pred)<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><a name="bookmark274"># Return losses </a><span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span><a name="bookmark288">&zwnj;</a><a name="bookmark289">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">self.dinputs </span>= -<span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">(y_true </span>- <span style=" color: #231F20;">dvalues) </span>/ <span style=" color: #231F20;">outputs </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Mean Absolute Error Loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With <b>mean absolute error</b>, you take the absolute difference between the predicted and true values in a single output and average those absolute values.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Where <i>y </i>means the target value, <i>y-hat </i>means predicted value, index <i>i </i>means the current sample, index <i>j </i>means the current output in this sample, and the <i>J </i>means the number of outputs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This function, used as a loss, penalizes the error linearly. It produces sparser results and is robust to outliers, which can be both advantageous and disadvantageous. In reality, L1 (MAE) loss is used less frequently than L2 (MSE) loss.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark275">Mean Absolute Error Loss Derivative</a><a name="bookmark290">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-bottom: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The partial derivative for absolute error with respect to the predicted values is:</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="padding-bottom: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">1 <span class="p">divided by </span>J <span class="p">(the number of outputs) is a constant, and can be moved outside of the derivative. Since we are calculating the derivative with respect to the given output, </span>j<span class="p">, the sum of one element equals this element:</span></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We already calculated the partial derivative of an absolute value for the L1 regularization, which is similar to the L1 loss. The derivative of an absolute value equals <i>1 </i>if this value is greater than <i>0</i>, or <i>-1 </i>if it’s less than <i>0</i>. The derivative does not exist for a value of <i>0</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full solution:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark276">Mean Absolute Error Loss Code</a><a name="bookmark291">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">The code for mean absolute error is very similar to the mean squared error. The forward pass includes NumPy’s <span class="s22">np.abs() </span>to calculate absolute values before calculating the mean. For the backward pass, we’ll use <span class="s22">np.sign(), </span>which returns 1 or -1 given the sign of the input and <i>0 </i>if the parameter equals <i>0</i>, then normalize gradients by the number of samples to make them invariant to the batch size, or number of samples in general:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Mean Absolute Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanAbsoluteError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L1 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean(np.abs(y_true <span style=" color: #C71F43;">- </span>y_pred), <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.sign(y_true <span style=" color: #C71F43;">- </span>dvalues) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark277">Accuracy in Regression</a><a name="bookmark292">&zwnj;</a><a name="bookmark293">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we’ve got data, an activation function, and a loss calculation for regression, we’d like to measure performance.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With cross-entropy, we were able to count the number of matches (situations where the prediction equals the ground truth target), and then divide it by the number of samples to measure the model’s accuracy. With a regression model, we have two problems: the first problem is that</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">each output neuron in the model (there might be many) is a separate output <span style=" color: #212121;">— </span>like in a binary regression model and unlike in a classifier, where all outputs contribute toward a common prediction. The second problem is that the prediction is a float value, and we can’t simply check</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">if the output value equals the ground truth one, as it most likely won’t <span style=" color: #212121;">— </span>if it differs even slightly, the accuracy will be a 0. For example, if your model predicts home prices and one of the samples has the target price of $192,500, and the predicted value is $192,495, then a pure “is it equal” assessment would return False. We’d likely consider the predicted price to be correct or “close enough” in this scenario, given the magnitude of the numbers in consideration. There’s no perfect way to show accuracy with regression. Still, it is preferable to have some accuracy metric.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">For example, Keras, a popular deep learning framework, shows both accuracy and loss for regression models, and we’ll also make our own accuracy metric.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, we need some “limit” value, which we’ll call “precision.” To calculate this precision, we’ll calculate the standard deviation from the ground truth target values and then divide it by <i>250</i>. This value can certainly vary depending on your goals. The larger the number you divide by, the more “strict” the accuracy metric will be. <i>250 </i>is our value of choice. Code to represent this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">accuracy_precision <span style=" color: #C71F43;">= </span>np.std(y) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Then we could use this precision value as a sort of “cushion allowance” for regression outputs when comparing targets and predicted values for accuracy. We perform the comparison by applying the absolute value on the difference between the ground truth values and the predictions. Then we check if the difference is smaller than our previously calculated precision:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>activation2.output</p><p class="s10" style="padding-top: 1pt;padding-left: 110pt;text-indent: -66pt;line-height: 108%;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>np.mean(np.absolute(predictions <span style=" color: #C71F43;">- </span>y) <span style=" color: #C71F43;">&lt; </span>accuracy_precision)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark278">Regression Model Training</a><a name="bookmark294">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">With this new activation function, loss, and way of calculating accuracy, we now create our model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 1 input feature and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 1 output value</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Create Linear activation:</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">activation2 <span style=" color: #C71F43;">= </span>Activation_Linear()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_MeanSquaredError()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create optimizer </span>optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;"># Accuracy precision for accuracy calculation</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"># There are no really accuracy factor for regression problem,</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># but we can simulate/approximate it. We&#39;ll calculate it by checking # how many values have a difference to their ground truth equivalent # less than given precision</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># We&#39;ll calculate this precision as a fraction of standard deviation # of al the ground truth values</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;">accuracy_precision <span style=" color: #C71F43;">= </span>np.std(y) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through second Dense layer # takes outputs of activation function</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate the data loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation2.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate regularization penalty </span>regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss_function.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\ loss_function.regularization_loss(dense2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # To calculate it we&#39;re taking absolute difference between</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># predictions and ground truth values and compare if differences # are lower than given precision value</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>activation2.output</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>np.mean(np.absolute(predictions <span style=" color: #C71F43;">- </span>y) <span style=" color: #C71F43;">&lt;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 159pt;text-indent: 0pt;text-align: left;">accuracy_precision)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_function.backward(activation2.output, y) activation2.backward(loss_function.dinputs) dense2.backward(activation2.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.002</span>, loss: <span style=" color: #7358A5;">0.500 </span>(data_loss: <span style=" color: #7358A5;">0.500</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.001</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.346 </span>(data_loss: <span style=" color: #7358A5;">0.346</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.001</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.145 </span>(data_loss: <span style=" color: #7358A5;">0.145</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.001</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.004</span>, loss: <span style=" color: #7358A5;">0.145 </span>(data_loss: <span style=" color: #7358A5;">0.145</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.001</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Training didn’t work out here very well! Let’s add an ability to draw the testing data and let’s also do a forward pass on the testing data, drawing output data on the same plot as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt X_test, y_test </span>= <span style=" color: #231F20;">sine_data()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">dense1.forward(X_test) activation1.forward(dense1.output) dense2.forward(activation1.output) activation2.forward(dense2.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">plt.plot(X_test, y_test) plt.plot(X_test, activation2.output) plt.show()</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, we are importing matplotlib, then creating a new set of data. Next, we have 4 lines of the code that are the same as the forward pass from our code above. We could call it a prediction or, in the context of what we are going to do, validation. We’ll cover both topics and explain what validation and prediction are in the future chapters. For now, it’s enough to know that what we are doing here is predicting on the same feature-set that we’ve used to train the model in order to see what the model learned and returns for our data — seeing how close outputs are to the</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">training ground-true values. We are then plotting the training data, which are obviously a sine, and prediction data, what we’d hope to form a sine as well. Let’s run this code again and take a look at the generated image:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.02: <span class="p">Model prediction - could not fit the sine data.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Animation of the training process:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.03: <span class="p">Model stoppped training immediately.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/ghi" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.03: </a><a href="https://nnfs.io/ghi" class="s9" target="_blank">https://nnfs.io/ghi</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Recall the rectified linear activation function and how its nonlinear behavior allowed us to map nonlinear functions, but we also needed two or more hidden layers. In this case, we have only 1 hidden layer followed by the output layer. As we should know by now, this is simply not enough!</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">If we add just one more layer:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 1 input feature and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 64 output values</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation2 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create third Dense layer with 64 input features (as we take output # of previous layer here) and 1 output value</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">dense3 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create Linear activation:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">activation3 <span style=" color: #C71F43;">= </span>Activation_Linear()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_MeanSquaredError()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create optimizer </span>optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;"># Accuracy precision for accuracy calculation</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"># There are no really accuracy factor for regression problem,</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># but we can simulate/approximate it. We&#39;ll calculate it by checking # how many values have a difference to their ground truth equivalent # less than given precision</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># We&#39;ll calculate this precision as a fraction of standard deviation # of al the ground truth values</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;">accuracy_precision <span style=" color: #C71F43;">= </span>np.std(y) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through second Dense layer # takes outputs of activation function</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through third Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of second layer as inputs <span style=" color: #231F20;">dense3.forward(activation2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of third dense layer here <span style=" color: #231F20;">activation3.forward(dense3.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate the data loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation3.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate regularization penalty </span>regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;">loss_function.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\ loss_function.regularization_loss(dense2) <span style=" color: #C71F43;">+ </span>\ loss_function.regularization_loss(dense3)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # To calculate it we&#39;re taking absolute difference between</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># predictions and ground truth values and compare if differences # are lower than given precision value</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>activation3.output</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>np.mean(np.absolute(predictions <span style=" color: #C71F43;">- </span>y) <span style=" color: #C71F43;">&lt;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">accuracy_precision)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_function.backward(activation3.output, y) activation3.backward(loss_function.dinputs) dense3.backward(activation3.dinputs) activation2.backward(dense3.dinputs) dense2.backward(activation2.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.update_params(dense3) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt X_test, y_test </span>= <span style=" color: #231F20;">sine_data()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">dense1.forward(X_test) activation1.forward(dense1.output) dense2.forward(activation1.output) activation2.forward(dense2.output) dense3.forward(activation2.output) activation3.forward(dense3.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">plt.plot(X_test, y_test) plt.plot(X_test, activation3.output) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.002</span>, loss: <span style=" color: #7358A5;">0.500 </span>(data_loss: <span style=" color: #7358A5;">0.500</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.001</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.187 </span>(data_loss: <span style=" color: #7358A5;">0.187</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.001</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.617</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.001</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.620</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.001</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.04: <span class="p">Model prediction - better fit to the data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.05: <span class="p">Model trainined to better fit the sine data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/hij" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.05: </a><a href="https://nnfs.io/hij" class="s9" target="_blank">https://nnfs.io/hij</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Our model’s accuracy is not very good, and loss seems stuck at a pretty high level for this model. The image shows us why this is the case, the model has some trouble fitting our data, and it looks like it might be stuck in a local minimum. As we have already learned, to try to help the model with being stuck at a local minimum, we might use a higher learning rate and add a learning rate decay. In the previous model, we have used the default learning rate, which is <i>0.001</i>. Let’s set it to</p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">0.01 <span class="p">and add learning rate decaying:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.01</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.002</span>, loss: <span style=" color: #7358A5;">0.500 </span>(data_loss: <span style=" color: #7358A5;">0.500</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.01</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.027</span>, loss: <span style=" color: #7358A5;">0.061 </span>(data_loss: <span style=" color: #7358A5;">0.061</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.009099181073703368</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.565</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009175153683824203</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.564</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009091735612328393</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.06: <span class="p">Model prediction - similar fit to the data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.07: <span class="p">Model trainined to fit the sine data, similar fit.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/ijk" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.07: </a><a href="https://nnfs.io/ijk" class="s9" target="_blank">https://nnfs.io/ijk</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Our model seems to still be stuck with even lower accuracy this time. Let’s try to use an even bigger learning rate then:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.05</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.002</span>, loss: <span style=" color: #7358A5;">0.500 </span>(data_loss: <span style=" color: #7358A5;">0.500</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.05</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.087</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.04549590536851684</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.275</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004587576841912101</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.229</span>, loss: <span style=" color: #7358A5;">0.031 </span>(data_loss: <span style=" color: #7358A5;">0.031</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0045458678061641965</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.08: <span class="p">Model prediction - similar fit to the data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.09: <span class="p">Model trainined to fit the sine data, similar fit.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/jkl" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.09: </a><a href="https://nnfs.io/jkl" class="s9" target="_blank">https://nnfs.io/jkl</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It’s getting even worse. Accuracy drops significantly, and we can observe the lower part of the sine being of a worse shape as well. It seems like we are not able to make this model learn the data, but after multiple tests and tuning hyperparameters, we could find a learning rate of 0.005:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.005</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.002</span>, loss: <span style=" color: #7358A5;">0.500 </span>(data_loss: <span style=" color: #7358A5;">0.500</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.005</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.007</span>, loss: <span style=" color: #7358A5;">0.084 </span>(data_loss: <span style=" color: #7358A5;">0.084</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.004549590536851684</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.964</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.00045875768419121016</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.970</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.00045458678061641964</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.10: <span class="p">Model prediction - good fit to the data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 17.11: <span class="p">Model trainined to fit the sine data.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/klm" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.11: </a><a href="https://nnfs.io/klm" class="s9" target="_blank">https://nnfs.io/klm</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">This time model has learned pretty well, but the curious part is that both lower and higher learning rates than what we used here initially caused accuracy to be pretty low and loss be stuck at the same value when the learning rate in between them actually worked.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Debugging such a problem is usually a pretty hard task and out of the scope of this book. The accuracy and loss suggest that updates to the parameters are not big enough, but the rising learning rate makes things only worse, and there is just this single spot that we were able to find that lets our model learn. You might recall that, back in chapter 3, we were discussing parameter initialization methods and why it’s important to initialize them wisely. It turns out that, in the current case, we can help the model learn by changing the factor of 0.01 to 0.1 in the Dense layer’s weight initialization. But then you might ask — since the learning rate is being used to decide how much of a gradient to apply to the parameters, why does changing these initial values help instead? As you may recall, the back-propagated gradient is calculated using weights, and the learning rate does not affect it. That’s why it’s important to use right weight initialization, and so far, we have been using the same values for each model.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">If we, for example, take a look at the source code of Keras, a neural network framework, we can learn that:</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">glorot_uniform</span>(<span class="s23">seed</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">&quot;&quot;&quot;Glorot uniform initializer, also called Xavier uniform initializer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">It draws samples from a uniform distribution within [-limit, limit] where `limit` is `sqrt(6 / (fan_in + fan_out))`</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">where `fan_in` is the number of input units in the weight tensor and `fan_out` is the number of output units in the weight tensor.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Arguments</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">seed: A Python integer. Used to seed the random generator.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Returns</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">An initializer.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># References</p><p style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" class="s65" target="_blank">Glorot &amp; Bengio, AISTATS 2010 http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf</a></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">&quot;&quot;&quot;</p><p class="s11" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">VarianceScaling(</span><span class="s23">scale</span>=<span style=" color: #7358A5;">1.</span><span style=" color: #231F20;">,</span></p><p class="s23" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;line-height: 108%;text-align: left;">mode<span class="s11">=</span><span class="s38">&#39;fan_avg&#39;</span><span class="s10">, </span>distribution<span class="s11">=</span><span class="s38">&#39;uniform&#39;</span><span class="s10">, </span>seed<span class="s11">=</span><span class="s10">seed)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This code is part of the Keras 2 library. The important part of the above is actually the comment section, which describes how it initializes weights. We can find there are important pieces of information to remember — the fraction that multiplies the draw from the uniform distribution depends on the number of inputs and the number of neurons and is not constant like in our case. This method of initialization is called <b>Glorot uniform</b>. We (the authors of this book) actually have had a very similar problem in one of our projects, and changing the way weights were initialized changed the model from not learning at all to a learning state.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">For the purposes of this model, let’s change the factor multiplying the draw from the normal distribution in the weight initialization of the <i>Dense </i>layer to 0.1 and re-run all four of the above attempts to compare results:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.weights </span>= <span style=" color: #7358A5;">0.1 </span>* <span style=" color: #231F20;">np.random.randn(n_inputs, n_neurons)</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">And all above tests re-ran:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s18" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.496 </span>(data_loss: <span style=" color: #7358A5;">0.496</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.001</p><p class="s18" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.005</span>, loss: <span style=" color: #7358A5;">0.114 </span>(data_loss: <span style=" color: #7358A5;">0.114</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.001</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s18" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.869</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s18" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.001</span></p><p class="s18" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.883</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s18" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.001</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Fig 17.12: <span class="p">Model prediction - good fit to the data with different weight initialization.</span></h3><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 17.13: <span class="p">Model trained to fit the sine data after replacing weight initialization.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/lmn" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.13: </a><a href="https://nnfs.io/lmn" class="s9" target="_blank">https://nnfs.io/lmn</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This model was previously stuck and has now achieved high accuracy. There are some visible imperfections like at the bottom side of this sine, but the overall result is better.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.01</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.496 </span>(data_loss: <span style=" color: #7358A5;">0.496</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.01</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.065</span>, loss: <span style=" color: #7358A5;">0.011 </span>(data_loss: <span style=" color: #7358A5;">0.011</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.009099181073703368</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.958</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009175153683824203</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.949</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009091735612328393</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Fig 17.14: <span class="p">Model prediction - good fit to the data with different weight initialization.</span></h3><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 17.15: <span class="p">Model trained to fit the sine data after replacing weight initialization.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/mno" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.15: </a><a href="https://nnfs.io/mno" class="s9" target="_blank">https://nnfs.io/mno</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Another previously stuck model has trained very well this time, achieving very good accuracy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.05</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.496 </span>(data_loss: <span style=" color: #7358A5;">0.496</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.05</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.016</span>, loss: <span style=" color: #7358A5;">0.008 </span>(data_loss: <span style=" color: #7358A5;">0.008</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.04549590536851684</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9000</span>, acc: <span style=" color: #7358A5;">0.802</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.005000500050005001</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9100</span>, acc: <span style=" color: #7358A5;">0.233</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004950985246063967</span></p><table style="border-collapse:collapse" cellspacing="0"><tr style="height:20pt"><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:28pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 3pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:28pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 3pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:28pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 2pt;text-indent: 0pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:28pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:28pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:28pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 3pt;text-indent: 0pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr><tr style="height:20pt"><td style="width:36pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000</p></td><td style="width:72pt"><p class="s24" style="padding-top: 6pt;padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:43pt"><p class="s26" style="padding-top: 6pt;padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.000<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-top: 6pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"/><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9200</span>, acc: <span style=" color: #7358A5;">0.434</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004902441415825081</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9300</span>, acc: <span style=" color: #7358A5;">0.838</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0048548402757549285</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9400</span>, acc: <span style=" color: #7358A5;">0.309</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004808154630252909</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9500</span>, acc: <span style=" color: #7358A5;">0.253</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004762358319839985</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9600</span>, acc: <span style=" color: #7358A5;">0.795</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004717426172280404</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9700</span>, acc: <span style=" color: #7358A5;">0.802</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004673333956444528</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9800</span>, acc: <span style=" color: #7358A5;">0.141</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004630058338735069</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.221</span>, loss:</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.004587576841912101</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.631</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0045458678061641965</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 40pt;text-indent: 0pt;text-align: left;">Fig 17.16: <span class="p">Model prediction - good fit to the data with different weight initialization.</span></h3><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 17.17: <span class="p">Model trained to fit the sine data after replacing weight initialization.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/nop" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.17: </a><a href="https://nnfs.io/nop" class="s9" target="_blank">https://nnfs.io/nop</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">The “jumping” accuracy in the case of this set of the optimizer settings shows that the learning rate is way too big, but even then, the model learned the shape of the sine function considerably well.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.005</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.003</span>, loss: <span style=" color: #7358A5;">0.496 </span>(data_loss: <span style=" color: #7358A5;">0.496</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.005</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.017</span>, loss: <span style=" color: #7358A5;">0.048 </span>(data_loss: <span style=" color: #7358A5;">0.048</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.004549590536851684</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.242</span>, loss: <span style=" color: #7358A5;">0.001 </span>(data_loss: <span style=" color: #7358A5;">0.001</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.004170141784820684</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.786</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.003849114703618168</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.885</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0035739814152966403</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.982</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.00045875768419121016</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.981</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.00045458678061641964</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 29pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 43pt;text-indent: 0pt;text-align: left;">Fig 17.18: <span class="p">Model prediction - best fit to the data with different weight initialization.</span></h3><p style="padding-left: 18pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">Fig 17.19: <span class="p">Model trained to best fit the sine data after replacing weight initialization.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 197pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/opq" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Anim 17.19: </a><a href="https://nnfs.io/opq" class="s9" target="_blank">https://nnfs.io/opq</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">These hyperparameters yielded the best results again, but not by much.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we can see, this time, our model learned in all cases, using different learning rates, and did not get stuck if any of them. That’s how much changing weight initialization can impact the training process.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark279">Full code up to this point:</a><a name="bookmark295">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">sine_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.1 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Save input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output </span>jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 268pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Save input and calculate/save output # of the sigmoid function</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.output </span>= <span style=" color: #7358A5;">1 </span>/ <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1 </span>+ <span style=" color: #231F20;">np.exp(</span>-<span style=" color: #231F20;">inputs))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative - calculates from output of the sigmoid function </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.output) <span style=" color: #C71F43;">* </span>self.output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Linear activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Linear</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): <span style=" color: #A5A4A5;"># Just remember values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># derivative is 1, 1 * dvalues = dvalues - the chain rule </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get corrected cache</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: -42pt;line-height: 108%;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">* </span>\ layer.weights)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 231pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 274pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return loss <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Creates activation and loss function objects</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.activation <span style=" color: #C71F43;">= </span>Activation_Softmax() self.loss <span style=" color: #C71F43;">= </span>Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">y_true</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Output layer&#39;s activation function <span style=" color: #231F20;">self.activation.forward(inputs)</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Set the output</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>self.activation.output <span style=" color: #A5A4A5;"># Calculate and return loss value</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.loss.calculate(self.output, y_true)</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Binary cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_BinaryCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 108pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped)) sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Squared Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanSquaredError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L2 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean((y_true <span style=" color: #C71F43;">- </span>y_pred)<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">self.dinputs </span>= -<span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">(y_true </span>- <span style=" color: #231F20;">dvalues) </span>/ <span style=" color: #231F20;">outputs </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Absolute Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanAbsoluteError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L1 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 41pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean(np.abs(y_true <span style=" color: #C71F43;">- </span>y_pred), <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.sign(y_true <span style=" color: #C71F43;">- </span>dvalues) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create Dense layer with 1 input feature and 64 output values </span>dense1 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation1 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create second Dense layer with 64 input features (as we take output # of previous layer here) and 64 output values</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense2 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create ReLU activation (to be used with Dense layer): </span>activation2 <span style=" color: #C71F43;">= </span>Activation_ReLU()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create third Dense layer with 64 input features (as we take output # of previous layer here) and 1 output value</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">dense3 <span style=" color: #C71F43;">= </span>Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create Linear activation:</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">activation3 <span style=" color: #C71F43;">= </span>Activation_Linear()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create loss function</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">loss_function <span style=" color: #C71F43;">= </span>Loss_MeanSquaredError()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">optimizer <span style=" color: #C71F43;">= </span>Optimizer_Adam(<span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.005</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-3</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"># Accuracy precision for accuracy calculation</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"># There are no really accuracy factor for regression problem,</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># but we can simulate/approximate it. We&#39;ll calculate it by checking # how many values have a difference to their ground truth equivalent # less than given precision</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># We&#39;ll calculate this precision as a fraction of standard deviation # of all the ground truth values</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;">accuracy_precision <span style=" color: #C71F43;">= </span>np.std(y) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: justify;"># Train in loop</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: justify;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">10001</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass of our training data through this layer <span style=" color: #231F20;">dense1.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of first dense layer here <span style=" color: #231F20;">activation1.forward(dense1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through second Dense layer # takes outputs of activation function</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># of first layer as inputs <span style=" color: #231F20;">dense2.forward(activation1.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of second dense layer here <span style=" color: #231F20;">activation2.forward(dense2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Perform a forward pass through third Dense layer</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># takes outputs of activation function of second layer as inputs <span style=" color: #231F20;">dense3.forward(activation2.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform a forward pass through activation function # takes the output of third dense layer here <span style=" color: #231F20;">activation3.forward(dense3.output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate the data loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>loss_function.calculate(activation3.output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate regularization penalty </span>regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;">loss_function.regularization_loss(dense1) <span style=" color: #C71F43;">+ </span>\ loss_function.regularization_loss(dense2) <span style=" color: #C71F43;">+ </span>\ loss_function.regularization_loss(dense3)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate overall loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate accuracy from output of activation2 and targets # To calculate it we&#39;re taking absolute difference between</p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># predictions and ground truth values and compare if differences # are lower than given precision value</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>activation3.output</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>np.mean(np.absolute(predictions <span style=" color: #C71F43;">- </span>y) <span style=" color: #C71F43;">&lt;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 159pt;text-indent: 0pt;text-align: left;">accuracy_precision)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span><span style=" color: #7358A5;">100</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span>&#39;data_loss: <span style=" color: #231F20;">{data_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>, &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{optimizer.current_learning_rate}</span>&#39;)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Backward pass <span style=" color: #231F20;">loss_function.backward(activation3.output, y) activation3.backward(loss_function.dinputs) dense3.backward(activation3.dinputs) activation2.backward(dense3.dinputs) dense2.backward(activation2.dinputs) activation1.backward(dense2.dinputs) dense1.backward(activation1.dinputs)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update weights and biases <span style=" color: #231F20;">optimizer.pre_update_params() optimizer.update_params(dense1) optimizer.update_params(dense2) optimizer.update_params(dense3) optimizer.post_update_params()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt X_test, y_test </span>= <span style=" color: #231F20;">sine_data()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">dense1.forward(X_test) activation1.forward(dense1.output) dense2.forward(activation1.output) activation2.forward(dense2.output) dense3.forward(activation2.output) activation3.forward(dense3.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">plt.plot(X_test, y_test) plt.plot(X_test, activation3.output) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-top: 4pt;padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch17" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch17<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark296">Chapter 18</a><a name="bookmark298">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Model Object</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We built a model that can perform the forward pass, backward pass, and ancillary tasks like measuring accuracy. We have built all this by writing a fair bit of code and making modifications in some decently-sized blocks of code. It’s beginning to make more sense to make our model an object itself, especially since we will want to do things like save and load this object to use for future prediction tasks. We will also use this object to cut down on some of the more common lines of code, making it easier to work with our current code base and build new models. To do this model object conversion, we’ll use the last model we were working on, the regression model with sine data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">sine_data X, y </span>= <span style=" color: #231F20;">sine_data()</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Once we have the data, our first step for the model class is to add in the various layers we want. Thus, we can begin our model class by doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">This allows us to use the <span class="s35">add </span>method of the model object to add layers. This alone will help with legibility considerably. Let’s add some layers:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 110%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)) model.add(Activation_Linear())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can also query this model now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(model.layers)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 113%;text-align: left;">[<span style=" color: #C71F43;">&lt;</span><u> </u>main<u> </u>.Layer_Dense object at <span style=" color: #7358A5;">0x0000015E9D504BC8</span><span style=" color: #C71F43;">&gt;</span>, <span style=" color: #C71F43;">&lt;</span><u> </u>main<u> </u>. Activation_ReLU object at <span style=" color: #7358A5;">0x0000015E9D504C48</span><span style=" color: #C71F43;">&gt;</span>, <span style=" color: #C71F43;">&lt;</span><u> </u>main<u> </u>.Layer_Dense object at <span style=" color: #7358A5;">0x0000015E9D504C88</span><span style=" color: #C71F43;">&gt;</span>, <span style=" color: #C71F43;">&lt;</span><u> </u>main<u> </u>.Activation_ReLU object at <span style=" color: #7358A5;">0x0000015E9D504CC8</span><span style=" color: #C71F43;">&gt;</span>, <span style=" color: #C71F43;">&lt;</span><span class="s66"> </span>main<u> </u>.Layer_Dense object at <span style=" color: #7358A5;">0x0000015E9D504D08</span><span style=" color: #C71F43;">&gt;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">&lt;</span><u> </u>main<u> </u>.Activation_Linear object at <span style=" color: #7358A5;">0x0000015E9D504D88</span><span style=" color: #C71F43;">&gt;</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Besides adding layers, we also want to set a loss function and optimizer for the model. To do this, we’ll create a method called <span class="s35">set</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss and optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>): self.loss <span style=" color: #C71F43;">= </span>loss self.optimizer <span style=" color: #C71F43;">= </span>optimizer</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">The use of the asterisk in the parameter definitions notes that the subsequent parameters (<span class="s29">loss </span>and <span class="s29">optimizer </span>in this case) are keyword arguments. Since they have no default value assigned, they are required keyword arguments, which means that they have to be passed by names and values, making code more legible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we can add a call to this method into our newly-created model object, and pass the loss and optimizer objects:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)) model.add(Activation_Linear())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss and optimizer objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_MeanSquaredError(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>learning_rate<span class="s11">=</span><span class="s14">0.005</span><span class="s10">, </span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">),</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">After we’ve set our model’s layers, loss function, and optimizer, the next step is to train, so we’ll add a train method. For now, we’ll make it a placeholder and fill it in soon:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Temporary <span style=" color: #C71F43;">pass</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can then add a call to the train method in the model definition. We’ll pass the training data, the number of epochs (10000, as we’ve used so far), and an indicator of how often to print a training summary. We do not need or want to print it every step, so we’ll make it configurable:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)) model.add(Activation_Linear())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss and optimizer objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_MeanSquaredError(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>learning_rate<span class="s11">=</span><span class="s14">0.005</span><span class="s10">, </span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">),</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">10000</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">To actually train, we need to perform a forward pass. Performing this forward pass in the object is slightly more complicated because we want to do this in a loop over the layers, and we need to</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">know the previous layer’s output to pass data properly. One issue with querying the previous layer is that the first layer doesn’t have a “previous” layer. The first layer that we’re defining is the first <b>hidden </b>layer. One option we have then is to create an “input layer.” This is considered a layer</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">in a neural network but doesn’t have weights and biases associated with it. The input layer only contains the training data, and we’ll only use it as a “previous” layer to the first layer during the iteration of the layers in a loop. We’ll create a new class and call it in similarly as <span class="s35">Layer_Dense </span>class — <span class="s35">Layer_Input</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Input &quot;layer&quot;</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Input</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 118%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>): self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;"><a name="bookmark299">The </a><span class="s35">forward </span>method sets training samples as <span class="s22">self.output</span>. This property is common with other layers. There’s no need for a backward method here since we’ll never use it. It might seem silly right now to even have this class, but it should hopefully become clear how we’re going to use this shortly. The next thing we’re going to do is set the previous and next layer properties for each of the model’s layers. We’ll create a method called <span class="s35">finalize </span>in the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The last layer - the next object is the loss <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">This code creates an input layer and sets <span class="s22">next </span>and <span class="s22">prev </span>references for each layer contained within the <span class="s22">self.layers </span>list of a model object. We wanted to create the <span class="s35">Layer_Input </span>class to set the <span class="s22">prev </span>property of the first hidden layer in a loop since we are going to call all of the</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">layers in a uniform way. The <span class="s22">next </span>layer for the final layer will be the loss, which we already have created.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now that we have the necessary layer information for our model object to perform a forward pass, let’s add a forward method. We will use this forward method both for when we train and later when we just want to predict, which is also called <b>model inference</b>. Continuing the code within the <span class="s35">Model </span>class:</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">In this case, we take in <span class="s29">X </span>(input data), then simply pass this data through the <span class="s22">input_layer </span>in the <span class="s35">Model </span>object, which creates an <span class="s22">output </span>attribute in this object. From here, we begin iterating over the <span class="s22">self.layers</span>, the layers starting with the first hidden layer. We perform a forward pass on the <span class="s22">layer.prev.output</span>, the output data of the previous layer, for each layer. For the first hidden layer, the <span class="s22">layer.prev </span>is <span class="s22">self.input_layer</span>. The <span class="s22">output </span>attribute is created for each layer when we call the <span class="s35">forward </span>method, which is then used as input to a <span class="s35">forward </span>method call on the next layer. Once we’ve iterated over all of the layers, we return the final layer’s output.</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">That’s a forward pass, and now let’s go ahead and add this forward pass method call to the <span class="s35">train</span></p><p style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">method in the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Temporary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(output) exit()</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Full <span class="s35">Model </span>class up to this point:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss and optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>): self.loss <span style=" color: #C71F43;">= </span>loss self.optimizer <span style=" color: #C71F43;">= </span>optimizer</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># The last layer - the next object is the loss <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Temporary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(output) exit()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Finally, we can add in the <span class="s35">finalize </span>method call to the main code (recall this method makes, among other things, the model’s layers aware of their previous and next layers).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)) model.add(Activation_Linear())</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss and optimizer objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_MeanSquaredError(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>learning_rate<span class="s11">=</span><span class="s14">0.005</span><span class="s10">, </span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">),</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">10000</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Running this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: -6pt;line-height: 108%;text-align: justify;">[[ <span style=" color: #7358A5;">0.00000000e+00</span>] [<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.13209149e-08</span>] [<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.26418297e-08</span>]</p><p class="s14" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: 0pt;line-height: 108%;text-align: justify;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.12869511e-05</span>] [<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.12982725e-05</span>] [<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1.13095930e-05</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">At this point, we’ve covered the forward pass of our model in the <span class="s35">Model </span>class. We still need to calculate loss and accuracy along with doing backpropagation. Before doing this, we need to  know which layers are “trainable,” which means layers with weights and biases that we can tweak. To do this, we need to check if the layer has either a <span class="s22">weights </span>or <span class="s22">biases </span>attribute. We can check this with the following code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Where <span class="s22">i </span>is the index for the layer in the list of layers. We’ll put this code into the <span class="s35">finalize</span></p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">method. The full code for that method so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># Initialize a list containing trainable layers: </span>self.trainable_layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 113%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 113%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 13pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 113%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 113%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 13pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 113%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we’ll modify the common <span class="s35">Loss </span>class to contain the following:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set/remember trainable layers</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">remember_trainable_layers</span>(<span class="s23">self</span>, <span class="s23">trainable_layers</span>): self.trainable_layers <span style=" color: #C71F43;">= </span>trainable_layers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">The <span class="s35">remember_trainable_layers </span>method in the common <span class="s35">Loss </span>class “tells” the loss object which layers in the <span class="s35">Model </span>object are trainable. The <span class="s35">calculate </span>method was modified to also return the <span class="s22">self.regularization_loss() </span>during a single call. The <span class="s35">regularization_loss </span>method currently requires a layer object, but with the <span class="s22">self.trainable_layers </span>property set in <span class="s35">remember_trainable_layers</span>, method we can now iterate over the trainable layers to compute regularization loss for the entire model, rather than one layer at a time:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s31" style="padding-left: 7pt;text-indent: 0pt;text-align: center;">class <span class="s32">Loss</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: center;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate regularization loss # iterate all trainable layers</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">For calculating accuracy, we need predictions. So far, predicting has required different code depending on the type of model. For a softmax classifier, we do a <span class="s22">np.argmax()</span>, but for regression, the prediction is the direct output because of the linear activation function being used in an output layer. Ideally, we’d have a prediction method that would choose the appropriate method for our model. To do this, we’ll add a <span class="s35">predictions </span>method to each activation function class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>np.argmax(outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>(outputs <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Linear activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Linear</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">All the computations made inside the <span class="s35">predictions </span>functions are the same as those performed with appropriate models in previous chapters. While we have no plans for using the ReLU activation function for an output layer’s activation function, we’ll include it here for completeness:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 118%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">We still need to set a reference to the activation function for the final layer in the <span class="s35">Model </span>object. We can later call the <span class="s35">predictions </span>method, which will return predictions calculated from the outputs. We’ll set this in the <span class="s35">Model </span>class’ <span class="s35">finalize </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p class="s14" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss self.output_layer_activation <span style=" color: #C71F43;">= </span>self.layers[i]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Just like the different prediction methods, we also calculate accuracy in different ways. We’re going to implement this in a way similar to the specific loss class’ objects implementation — we’ll create specific accuracy classes and their objects, which we’ll associate with models.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">First, we’ll write a common <span class="s35">Accuracy </span>class containing (for now) just a single method, <span class="s35">calculate</span>, returning an accuracy calculated from comparison results. We’ve already added a call to the <span class="s22">self.compare </span>method that does not exist yet, but we’ll create it soon in other classes that will inherit from this <span class="s35">Accuracy </span>class. For now, it’s enough to know that it will return a list of <i>True </i>and <i>False </i>values, indicating if a prediction matches the ground-truth value. Next, we calculate the mean value (which treats <i>True </i>as <i>1 </i>and <i>False </i>as <i>0</i>) and return it as an accuracy. The</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common accuracy class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Accuracy</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates an accuracy</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># given predictions and ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get comparison results</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">comparisons <span style=" color: #C71F43;">= </span>self.compare(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate an accuracy </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return accuracy <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Next, we can work with this common <span class="s35">Accuracy </span>class, inheriting from it, then building further  for specific types of models. In general, each of these classes will contain two methods: <span class="s35">init </span>(not to be confused with a Python class’<u> </u><span class="s43">init </span>method) for initialization from inside the model object and <span class="s35">compare </span>for performing comparison calculations. For regression, the <span class="s35">init </span>method will calculate an accuracy precision, the same as we have written previously for the regression model, and have been running before the training loop. The <span class="s35">compare </span>method will contain the actual comparison code we have implemented in the training loop itself, which uses <span class="s22">self. precision</span>. Note that initialization won’t recalculate precision unless forced to do so by setting the <span class="s29">reinit </span>parameter to <span class="s21">True</span>. This allows for multiple use-cases, including setting <span class="s22">self.</span></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;"><span class="s22">precision </span>independently, calling <span class="s35">init </span>whenever needed (e.g., from outside of the model during its creation), and even calling it multiple times (which will become handy soon):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for regression model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Regression</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create precision property <span style=" color: #231F20;">self.precision </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates precision value</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># based on passed-in ground truth</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>, <span class="s23">reinit</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.precision </span>is <span style=" color: #7358A5;">None </span>or <span style=" color: #231F20;">reinit: self.precision </span>= <span style=" color: #231F20;">np.std(y) </span>/ <span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">np.absolute(predictions </span>- <span style=" color: #231F20;">y) </span>&lt; <span style=" color: #231F20;">self.precision</span></p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">We can then set the accuracy object from within the <span class="s35">set </span>method in our <span class="s35">Model </span>class the same way as the loss and optimizer currently:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>, <span class="s23">accuracy</span>): self.loss <span style=" color: #C71F43;">= </span>loss</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.optimizer <span style=" color: #C71F43;">= </span>optimizer self.accuracy <span style=" color: #C71F43;">= </span>accuracy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">Then we can finally add the loss and accuracy calculations to our model right after the completed forward pass’ code. Note that we also initialize the accuracy with <span class="s22">self.accuracy.init(y) </span>at the beginning of the <span class="s35">train </span>method, which can be called multiple times <span style=" color: #212121;">— </span>as noted earlier. In the case of regression accuracy, this will invoke a precision calculation a single time during the first call. The code of the <span class="s35">train </span>method with implemented loss and accuracy calculations:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y)</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">Finally, we’ll add a call to the previously created method <span class="s35">remember_trainable_layers </span>with the <span class="s35">Loss </span>class’ object, which we’ll do in the <span class="s35">finalize </span>method (<span class="s22">self.loss.remember_ trainable_layers(self.trainable_layers)</span>). The full model class code so far:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>, <span class="s23">accuracy</span>): self.loss <span style=" color: #C71F43;">= </span>loss</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.optimizer <span style=" color: #C71F43;">= </span>optimizer self.accuracy <span style=" color: #C71F43;">= </span>accuracy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Initialize a list containing trainable layers: </span>self.trainable_layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss self.output_layer_activation <span style=" color: #C71F43;">= </span>self.layers[i]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update loss object with trainable layers <span style=" color: #231F20;">self.loss.remember_trainable_layers(</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.trainable_layers</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y)</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Full code for the <span class="s35">Loss </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate regularization loss # iterate all trainable layers</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.weights)</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># only calculate when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p class="s11" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss </span><span style=" color: #A5A4A5;"># Set/remember trainable layers</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">remember_trainable_layers</span>(<span class="s23">self</span>, <span class="s23">trainable_layers</span>): self.trainable_layers <span style=" color: #C71F43;">= </span>trainable_layers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculates the data and regularization losses # given model output and ground truth values </span><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">Now that we’ve done a full forward pass and have calculated loss and accuracy, we can begin  the backward pass. The <span class="s35">backward </span>method in the <span class="s35">Model </span>class is structurally similar to the <span class="s35">forward </span>method, just in reverse and using different parameters. Following the backward pass in our previous training approach, we need to call the <span class="s35">backward </span>method of a loss object to create the <span class="s22">dinputs </span>property. Next, we’ll loop through all the layers in reverse order, calling their</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="s35">backward </span>methods with the <span class="s22">dinputs </span>property of the next layer (in normal order) as a parameter, effectively backpropagating the gradient returned by that next layer. Remember that we have set the loss object as a <span class="s22">next </span>layer in the last, output layer.</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># First call backward method on the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># this will set dinputs property that the last # layer will try to access shortly <span style=" color: #231F20;">self.loss.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Call backward method going through all the objects # in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.backward(layer.next.dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we’ll add a call of this <span class="s22">backward </span>method to the end of the <span class="s35">train </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 118%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">After this backward pass, the last action to perform is to optimize. We have previously been calling the optimizer object’s <span class="s22">update_params </span>method as many times as we had trainable layers. We have to make this code universal as well by looping through the list of trainable layers and calling <span class="s22">update_params() </span>in this loop:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 118%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Then we can output useful information — here’s where this last parameter to the <span class="s35">train </span>method becomes handy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span>print_every: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 2pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Making the full <span class="s35">train </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s67" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><h4 style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">def </span><span style=" color: #427F3C;">train</span>(<span style=" color: #CA6628;">self</span>, <span style=" color: #CA6628;">X</span>, <span style=" color: #CA6628;">y</span>, <span style=" color: #C71F43;">*</span>, <span style=" color: #CA6628;">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span style=" color: #CA6628;">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>):</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s67" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s67" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><h4 style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s67" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><h4 style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y)</h4><h4 style="padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>data_loss <span style=" color: #C71F43;">+ </span>regularization_loss</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</h4><h4 style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</h4><h4 style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s67" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h4 style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</h4><h4 style="padding-left: 92pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</h4><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s67" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Print a summary</p><h4 style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span>print_every: <span style=" color: #32A7BD;">print</span>(<span style=" color: #32A7BD;">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></h4><h4 style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></h4><h4 style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+ </span><span style=" color: #32A7BD;">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></h4><p class="s73" style="padding-left: 153pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">f</span>&#39;reg_loss: <span style=" color: #231F20;">{regularization_loss</span><span style=" color: #7358A5;">:.3f</span><span style=" color: #231F20;">}</span>), &#39; <span style=" color: #C71F43;">+ </span><span style=" color: #32A7BD;">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We can now pass the accuracy class’ object into the model and test our model’s performance:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create dataset </span>X, y <span style=" color: #C71F43;">= </span>sine_data()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)) model.add(Activation_Linear())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_MeanSquaredError(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>learning_rate<span class="s11">=</span><span class="s14">0.005</span><span class="s10">, </span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Regression()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">10000</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.006</span>, loss: <span style=" color: #7358A5;">0.085 </span>(data_loss: <span style=" color: #7358A5;">0.085</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.004549590536851684</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.032</span>, loss: <span style=" color: #7358A5;">0.035 </span>(data_loss: <span style=" color: #7358A5;">0.035</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.004170141784820684</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.934</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.00045875768419121016</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.970</span>, loss: <span style=" color: #7358A5;">0.000 </span>(data_loss: <span style=" color: #7358A5;">0.000</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.00045458678061641964</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Our new model is behaving well, and now we’re able to make new models more easily with our</p><p class="s35" style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">Model <span class="p">class. We have to continue to modify these classes to handle for entirely new models. For example, we haven’t yet handled for binary logistic regression. For this, we need to add two things. First, we need to calculate the categorical accuracy:</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for classification model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Categorical</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">binary</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>): <span style=" color: #A5A4A5;"># Binary mode?</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary <span style=" color: #C71F43;">= </span>binary</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># No initialization is needed</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>): <span style=" color: #C71F43;">pass</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if not <span style=" color: #231F20;">self.binary </span>and <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">: y </span>= <span style=" color: #231F20;">np.argmax(y, </span><span class="s23">axis</span>=<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">)</span></p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">predictions </span>== <span style=" color: #231F20;">y</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">This is the same as the accuracy calculation for classification, just wrapped into a class and with an additional switch parameter. This switch disables one-hot to sparse label conversion while this class is used with the binary cross-entropy model, since this model always require the ground- true values to be a 2D array and they&#39;re not one-hot encoded. Note that we do not perform any initialization here, but the method needs to exist since it’s going to be called from the <span class="s35">train </span>method of the <span class="s35">Model </span>class. The next thing that we need to add is the ability to validate the model using validation data. Validation requires only a forward pass and calculation of loss (just data loss). We’ll modify the <span class="s35">calculate </span>method of the <span class="s35">Loss </span>class to let it calculate the validation loss as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculates the data and regularization losses # given model output and ground truth values</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ve added a new parameter and condition to return just the data loss, as regularization loss is not being used in this case. To run it, we’ll pass predictions and targets the same way as with the training data. We will not return regularization loss by default, which means we need to update the call to this method in the <span class="s35">train </span>method to include regularization loss during training:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-top: 12pt;padding-left: 86pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 110pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y,</p><p class="s23" style="padding-left: 231pt;text-indent: 0pt;text-align: left;">include_regularization<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Then we can add the validation code to the <span class="s35">train </span>method in the <span class="s35">Model </span>class. We added the <span class="s29">validation_data </span>parameter to the function, which takes a tuple of validation data (samples and targets), the <span class="s15">if </span>statement to check if the validation data is present, and if it is — the code to perform a forward pass over these data, calculate loss and accuracy in the same way as during training and print the results:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p class="s14" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate the loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>self.loss.calculate(output, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">The full <span class="s35">train </span>method for the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y,</p><p class="s11" style="padding-left: 92pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span>print_every: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Perform the forward pass </span>output <span style=" color: #C71F43;">= </span>self.forward(X_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate the loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>self.loss.calculate(output, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now we can create the test data and test the binary logistic regression model with the following code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create train and test dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">2</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Reshape labels to be a list of lists</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Inner list contains one output (either 0 or 1) # per each output neuron, 1 in this case</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>y.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">y_test <span style=" color: #C71F43;">= </span>y_test.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Add layers</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.add(Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">64</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">))</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">1</span>)) model.add(Activation_Sigmoid())</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_BinaryCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">5e-7</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical(</span>binary<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10000</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.625</span>, loss: <span style=" color: #7358A5;">0.675 </span>(data_loss: <span style=" color: #7358A5;">0.674</span>, reg_loss: <span style=" color: #7358A5;">0.001</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0009999505024501287</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.630</span>, loss: <span style=" color: #7358A5;">0.669 </span>(data_loss: <span style=" color: #7358A5;">0.668</span>, reg_loss: <span style=" color: #7358A5;">0.001</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009999005098992651</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.905</span>, loss: <span style=" color: #7358A5;">0.312 </span>(data_loss: <span style=" color: #7358A5;">0.276</span>, reg_loss: <span style=" color: #7358A5;">0.037</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009950748768967994</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.905</span>, loss: <span style=" color: #7358A5;">0.312 </span>(data_loss: <span style=" color: #7358A5;">0.275</span>, reg_loss: <span style=" color: #7358A5;">0.036</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0009950253706593885</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.775</span>, loss: <span style=" color: #7358A5;">0.423</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">Now that we’re streamlining the forward and backward pass code, including validation, this is a good time to reintroduce dropout. Recall that dropout is a method to disable, or filter out, certain neurons in an attempt to regularize and improve our model’s ability to generalize. If dropout is employed in our model, we want to make sure to leave it out when performing validation and inference (predictions); in our previous code, we left it out by not calling its <span class="s35">forward </span>method during the forward pass during validation. Here we have a common method for performing a forward pass for both training and validation, so we need a different approach for turning off dropout — to inform the layers if we are during the training and let them “decide” on calculation to include. The first thing we’ll do is include a <span class="s29">training </span>boolean argument for the <span class="s35">forward </span>method in all the layer and activation classes, since we are calling them in a unified form:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>):</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">When we’re not training, we can set the output to the input directly in the <span class="s35">Layer_Dropout </span>class and return from the method without changing outputs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If not in the training mode - return values <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">training:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 118%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs.copy() <span style=" color: #C71F43;">return</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">When we are training, we will engage the dropout:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Save input values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.input <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If not in the training mode - return values <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">training:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs.copy() <span style=" color: #C71F43;">return</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Next, we modify the <span class="s35">forward </span>method of our <span class="s35">Model </span>class to add the <span class="s29">training </span>parameter and a call to the <span class="s35">forward </span>methods of the layers to take this parameter’s value:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">training</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X, training)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output, training)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">We also need to update the <span class="s35">train </span>method in the <span class="s35">Model </span>class since the <span class="s29">training </span>parameter in the forward method call will need to be set to <span class="s74">True</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then set to <span class="s21">False </span>during validation:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X_val, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Making the full <span class="s35">train </span>method in the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y,</p><p class="s11" style="padding-left: 92pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, y)</span></p><p class="s10" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span>print_every: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X_val, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate the loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>self.loss.calculate(output, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y_val) <span style=" color: #A5A4A5;"># Print a summary</span></p><p class="s12" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span><i>f</i><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">The last thing that we have to take care of, with the <span class="s35">Model </span>class, is the combined Softmax activation and CrossEntropy loss class that we created for faster gradient calculation. The challenge here is that previously we have been defining forward and backward passes by hand for every model separately. Now, however, we have loops over layers in both directions of calculations, a unified way of calculating outputs and gradients, and other improvements. We cannot just simply remove the Softmax activation and CrossEntropy loss and replace them with an object combining both. It won’t work with the code that we have so far, since we are handling the output activation function and loss in a specific way. Since the combined object contains just</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">a backward pass optimization, let’s leave the forward pass as is, using separate Softmax activation and Categorical Cross-Entropy loss objects, and handle just for the backward pass.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">To start, we want to automatically decide if the current model is a classifier and if it uses the</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Softmax activation and Categorical Cross-Entropy loss. This can be achieved by checking the class name of the last layer’s object, which is an activation function’s object, and by checking the class name of the loss function’s object. We’ll add this check to the end of the <span class="s35">finalize </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If output activation is Softmax and</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># loss function is Categorical Cross-Entropy # create an object of combined activation</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># and loss function containing # faster gradient calculation</p><p class="s10" style="padding-left: 86pt;text-indent: -18pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">isinstance</span>(self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>], Activation_Softmax) <span style=" color: #C71F43;">and </span>\ <span style=" color: #32A7BD;">isinstance</span>(self.loss, Loss_CategoricalCrossentropy): <span style=" color: #A5A4A5;"># Create an object of combined activation</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and loss functions </span>self.softmax_classifier_output <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">To make this check, we are using Python’s <span class="s43">isinstance </span>function, which returns <i>True </i>if a given object is an instance of a given class. If both of the tests return <i>True</i>, we are setting a new property containing an object of the <span class="s22">Activation_Softmax_Loss_CategoricalCrossentropy </span>class. We also want to initialize this property with a value of <span class="s21">None </span>in the <span class="s35">Model </span>class’ constructor:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"># Softmax classifier&#39;s output object <span style=" color: #231F20;">self.softmax_classifier_output </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">The last step is, during the backward pass, to check if this object is set and, if it is, to use it. To do so, we need to handle this case separately with a slightly modified version of the current code of the backward pass. First, we call the <span class="s22">backward </span>method of the combined object, then, since we won’t call the <span class="s22">backward </span>method of the activation function (the last object on a list of layers), set the <span class="s22">dinputs </span>of the object of this function with the gradient calculated within the activation/loss object. At the end, we can iterate all of the layers except for the last one and perform the backward pass on them:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If softmax classifier</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.softmax_classifier_output </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: </span><span style=" color: #A5A4A5;"># First call backward method</span></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># on the combined activation/loss # this will set dinputs property</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.backward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Since we&#39;ll not call backward method of the last layer # which is Softmax activation</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># as we used combined activation/loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># object, let&#39;s set dinputs in this object </span>self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>].dinputs <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.dinputs</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call backward method going through # all the objects but last</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers[:</span>-<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">]):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 216%;text-align: left;">layer.backward(layer.next.dinputs) <span style=" color: #C71F43;">return</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Full <span class="s35">Model </span>class up to this point:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier&#39;s output object <span style=" color: #231F20;">self.softmax_classifier_output </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>, <span class="s23">accuracy</span>): self.loss <span style=" color: #C71F43;">= </span>loss</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.optimizer <span style=" color: #C71F43;">= </span>optimizer self.accuracy <span style=" color: #C71F43;">= </span>accuracy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Initialize a list containing trainable layers: </span>self.trainable_layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer</p><p class="s10" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss self.output_layer_activation <span style=" color: #C71F43;">= </span>self.layers[i]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update loss object with trainable layers <span style=" color: #231F20;">self.loss.remember_trainable_layers(</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.trainable_layers</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If output activation is Softmax and</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># loss function is Categorical Cross-Entropy # create an object of combined activation</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># and loss function containing # faster gradient calculation</p><p class="s10" style="padding-left: 86pt;text-indent: -18pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">isinstance</span>(self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>], Activation_Softmax) <span style=" color: #C71F43;">and </span>\ <span style=" color: #32A7BD;">isinstance</span>(self.loss, Loss_CategoricalCrossentropy): <span style=" color: #A5A4A5;"># Create an object of combined activation</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and loss functions </span>self.softmax_classifier_output <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y,</p><p class="s11" style="padding-left: 92pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span>print_every: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X_val, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate the loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>self.loss.calculate(output, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y_val)</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">training</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X, training)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output, training)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If softmax classifier</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.softmax_classifier_output </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: </span><span style=" color: #A5A4A5;"># First call backward method</span></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># on the combined activation/loss # this will set dinputs property</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.backward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Since we&#39;ll not call backward method of the last layer # which is Softmax activation</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># as we used combined activation/loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># object, let&#39;s set dinputs in this object </span>self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>].dinputs <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.dinputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call backward method going through # all the objects but last</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers[:</span>-<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">]):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 216%;text-align: left;">layer.backward(layer.next.dinputs) <span style=" color: #C71F43;">return</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># First call backward method on the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># this will set dinputs property that the last # layer will try to access shortly <span style=" color: #231F20;">self.loss.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Call backward method going through all the objects # in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.backward(layer.next.dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Also we won’t need the initializer and the forward method of the <span class="s35">Activation_Softmax_ Loss_CategoricalCrossentropy </span>class anymore so we can remove them leaving in just the backward pass:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now we can test our updated <span class="s35">Model </span>object with dropout:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1000</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Add layers</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.add(Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">512</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 201pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">))</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">model.add(Activation_ReLU()) model.add(Layer_Dropout(<span style=" color: #7358A5;">0.1</span>)) model.add(Layer_Dense(<span style=" color: #7358A5;">512</span>, <span style=" color: #7358A5;">3</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>learning_rate<span class="s11">=</span><span class="s14">0.05</span><span class="s10">, </span>decay<span class="s11">=</span><span class="s14">5e-5</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10000</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.716</span>, loss: <span style=" color: #7358A5;">0.726 </span>(data_loss: <span style=" color: #7358A5;">0.666</span>, reg_loss: <span style=" color: #7358A5;">0.060</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.04975371909050202</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.787</span>, loss: <span style=" color: #7358A5;">0.615 </span>(data_loss: <span style=" color: #7358A5;">0.538</span>, reg_loss: <span style=" color: #7358A5;">0.077</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.049507401356502806</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">9900</span>, acc: <span style=" color: #7358A5;">0.861</span>, loss: <span style=" color: #7358A5;">0.436 </span>(data_loss: <span style=" color: #7358A5;">0.389</span>, reg_loss: <span style=" color: #7358A5;">0.046</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.0334459346466437</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10000</span>, acc: <span style=" color: #7358A5;">0.880</span>, loss: <span style=" color: #7358A5;">0.394 </span>(data_loss: <span style=" color: #7358A5;">0.347</span>, reg_loss: <span style=" color: #7358A5;">0.047</span>),</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">lr: <span style=" color: #7358A5;">0.03333444448148271</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.867</span>, loss: <span style=" color: #7358A5;">0.379</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">It seems like everything is working as intended. Now that we’ve got this <span class="s35">Model </span>class, we’re able to define new models without writing large amounts of code repeatedly. Rewriting code is annoying and leaves more room to make small, hard-to-notice mistakes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark297">Full code up to this point:</a><a name="bookmark300">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Save input values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If not in the training mode - return values <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">training:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs.copy() <span style=" color: #C71F43;">return</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Input &quot;layer&quot;</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Input</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 285pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>np.argmax(outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Save input and calculate/save output # of the sigmoid function</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.output </span>= <span style=" color: #7358A5;">1 </span>/ <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1 </span>+ <span style=" color: #231F20;">np.exp(</span>-<span style=" color: #231F20;">inputs))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative - calculates from output of the sigmoid function </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.output) <span style=" color: #C71F43;">* </span>self.output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>(outputs <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Linear activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Linear</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Just remember values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># derivative is 1, 1 * dvalues = dvalues - the chain rule </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span><span style=" color: #A5A4A5;"># Get corrected cache</span></p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate regularization loss # iterate all trainable layers</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">*</span></p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p class="s11" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss </span><span style=" color: #A5A4A5;"># Set/remember trainable layers</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">remember_trainable_layers</span>(<span class="s23">self</span>, <span class="s23">trainable_layers</span>): self.trainable_layers <span style=" color: #C71F43;">= </span>trainable_layers</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculates the data and regularization losses # given model output and ground truth values</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Binary cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_BinaryCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 108pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped)) sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 102pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Squared Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanSquaredError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L2 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean((y_true <span style=" color: #C71F43;">- </span>y_pred)<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">self.dinputs </span>= -<span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">(y_true </span>- <span style=" color: #231F20;">dvalues) </span>/ <span style=" color: #231F20;">outputs </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Absolute Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanAbsoluteError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L1 loss</span></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 28pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>): <span style=" color: #A5A4A5;"># Calculate loss</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean(np.abs(y_true <span style=" color: #C71F43;">- </span>y_pred), <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.sign(y_true <span style=" color: #C71F43;">- </span>dvalues) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common accuracy class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Accuracy</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates an accuracy</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># given predictions and ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get comparison results</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">comparisons <span style=" color: #C71F43;">= </span>self.compare(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate an accuracy </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return accuracy <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for classification model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Categorical</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">binary</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>): <span style=" color: #A5A4A5;"># Binary mode?</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary <span style=" color: #C71F43;">= </span>binary</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># No initialization is needed</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>): <span style=" color: #C71F43;">pass</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if not <span style=" color: #231F20;">self.binary </span>and <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">: y </span>= <span style=" color: #231F20;">np.argmax(y, </span><span class="s23">axis</span>=<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">)</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">predictions </span>== <span style=" color: #231F20;">y</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for regression model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Regression</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create precision property <span style=" color: #231F20;">self.precision </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates precision value</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># based on passed-in ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>, <span class="s23">reinit</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.precision </span>is <span style=" color: #7358A5;">None </span>or <span style=" color: #231F20;">reinit: self.precision </span>= <span style=" color: #231F20;">np.std(y) </span>/ <span style=" color: #7358A5;">250</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">np.absolute(predictions </span>- <span style=" color: #231F20;">y) </span>&lt; <span style=" color: #231F20;">self.precision</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier&#39;s output object <span style=" color: #231F20;">self.softmax_classifier_output </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>, <span class="s23">accuracy</span>): self.loss <span style=" color: #C71F43;">= </span>loss</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.optimizer <span style=" color: #C71F43;">= </span>optimizer self.accuracy <span style=" color: #C71F43;">= </span>accuracy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Initialize a list containing trainable layers: </span>self.trainable_layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss self.output_layer_activation <span style=" color: #C71F43;">= </span>self.layers[i]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update loss object with trainable layers <span style=" color: #231F20;">self.loss.remember_trainable_layers(</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.trainable_layers</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If output activation is Softmax and</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># loss function is Categorical Cross-Entropy # create an object of combined activation</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># and loss function containing # faster gradient calculation</p><p class="s10" style="padding-left: 86pt;text-indent: -18pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">isinstance</span>(self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>], Activation_Softmax) <span style=" color: #C71F43;">and </span>\ <span style=" color: #32A7BD;">isinstance</span>(self.loss, Loss_CategoricalCrossentropy): <span style=" color: #A5A4A5;"># Create an object of combined activation</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and loss functions </span>self.softmax_classifier_output <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, y,</p><p class="s11" style="padding-left: 92pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>epoch <span style=" color: #C71F43;">% </span>print_every: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(X_val, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Calculate the loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">loss <span style=" color: #C71F43;">= </span>self.loss.calculate(output, y_val)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions, y_val)</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">training</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X, training)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output, training)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If softmax classifier</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.softmax_classifier_output </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: </span><span style=" color: #A5A4A5;"># First call backward method</span></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># on the combined activation/loss # this will set dinputs property</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.backward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Since we&#39;ll not call backward method of the last layer # which is Softmax activation</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># as we used combined activation/loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># object, let&#39;s set dinputs in this object </span>self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>].dinputs <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.dinputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call backward method going through # all the objects but last</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers[:</span>-<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">]):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 216%;text-align: left;">layer.backward(layer.next.dinputs) <span style=" color: #C71F43;">return</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># First call backward method on the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># this will set dinputs property that the last # layer will try to access shortly <span style=" color: #231F20;">self.loss.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Call backward method going through all the objects # in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.backward(layer.next.dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1000</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Add layers</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.add(Layer_Dense(<span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">512</span>, <span class="s23">weight_regularizer_l2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">5e-4</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 201pt;text-indent: 0pt;text-align: left;">bias_regularizer_l2<span class="s11">=</span><span class="s14">5e-4</span><span class="s10">))</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">model.add(Activation_ReLU()) model.add(Layer_Dropout(<span style=" color: #7358A5;">0.1</span>)) model.add(Layer_Dense(<span style=" color: #7358A5;">512</span>, <span style=" color: #7358A5;">3</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>learning_rate<span class="s11">=</span><span class="s14">0.05</span><span class="s10">, </span>decay<span class="s11">=</span><span class="s14">5e-5</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-bottom: 4pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10000</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch18" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch18<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark301">Chapter 19</a><a name="bookmark309">&zwnj;</a><a name="bookmark310">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">A Real Dataset</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In practice, deep learning tends to involve massive datasets (often terabytes or more in size), and models can take days, weeks, or even months to train. This is why, so far, we’ve used programmatically-generated datasets to keep things manageable and fast, while we learn the math and other aspects of deep learning. The main objective of this book is to teach how neural networks work, rather than the application of deep learning to various problems. That said, we’ll explore a more realistic dataset now, since this will present new challenges to deep learning that we’ve not yet had to consider.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If you have explored deep learning before reading this book, you have likely become acquainted (and possibly annoyed) with the MNIST dataset, which is a dataset of images of handwritten digits (0 through 9) at a resolution of 28x28 pixels. It’s a relatively small dataset and is reasonably easy for models to learn. This dataset became the “hello world” of deep learning, and it was</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a href="https://arxiv.org/abs/1708.07747" class="s82" target="_blank">once a benchmark of machine learning algorithms. The problem with this dataset is that it’s comically easy to get 99%+ accuracy on the MNIST dataset, and doesn’t provide much room for learning how various parameters impact learning. In 2017, however, a company called Zalando released a dataset (</a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">https://arxiv.org/abs/1708.07747</span><a href="https://github.com/zalandoresearch/fashion-mnist" class="s82" target="_blank">) called Fashion MNIST (</a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">https://github.com/</span><a href="https://github.com/zalandoresearch/fashion-mnist" style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 12pt;" target="_blank"> </a><span style=" color: #355CAA; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 12pt;">zalandoresearch/fashion-mnist</span>), which is a drop-in replacement for the regular MNIST dataset.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The Fashion MNIST dataset is a collection of 60,000 training samples and 10,000 testing samples of 28x28 images of 10 various clothing items like shoes, boots, shirts, bags, and more. We’ll see some examples shortly, but first, we need the actual data. Since the original dataset consists of binary files containing encoded (in a specific format) image data, for this book, we have prepared and are hosting a preprocessed dataset consisting of .png images instead. It is usually wise to use lossless compression for images since lossy compression, like JPEG, affects images by changing their data). These images are also grouped by labels and separated into training and testing groups. The samples are the images of articles of clothing, and the labels are the classifications. Here are the numeric labels and their respective descriptions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:8.5pt" cellspacing="0"><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s75" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Label</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s75" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Description</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">0</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">T-shirt/top</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">1</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Trouser</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">2</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">Pullover</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">3</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Dress</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">4</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Coat</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">5</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Sandal</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">6</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Shirt</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">7</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Sneaker</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">8</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Bag</p></td></tr><tr style="height:18pt"><td style="width:55pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">9</p></td><td style="width:73pt;border-top-style:solid;border-top-width:1pt;border-top-color:#010202;border-left-style:solid;border-left-width:1pt;border-left-color:#010202;border-bottom-style:solid;border-bottom-width:1pt;border-bottom-color:#010202;border-right-style:solid;border-right-width:1pt;border-right-color:#010202"><p class="s76" style="padding-top: 1pt;padding-left: 4pt;text-indent: 0pt;text-align: left;">Ankle boot</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark302">Data preparation</a><a name="bookmark311">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">First, we will retrieve the data from the nnfs.io site. Let’s define the URL of the dataset, a filename to save it locally to and the folder for the extracted images:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">URL <span style=" color: #C71F43;">= </span><span style=" color: #8F8633;">&#39;https://nnfs.io/datasets/fashion_mnist_images.zip&#39; </span>FILE <span style=" color: #C71F43;">= </span><span style=" color: #8F8633;">&#39;fashion_mnist_images.zip&#39;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">FOLDER <span style=" color: #C71F43;">= </span><span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, download the compressed data (if the file is absent under the given path) using the <i>urllib</i>, a standard Python library:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">os </span>import <span style=" color: #231F20;">urllib</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">urllib.request</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">if not <span style=" color: #231F20;">os.path.isfile(FILE):</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 118%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;Downloading </span>{URL} <span style=" color: #8F8633;">and saving as </span>{FILE}<span style=" color: #8F8633;">...&#39;</span>) urllib.request.urlretrieve(URL, FILE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">From here, we’ll unzip the files using another standard Python library called <i>zipfile</i>. We’ll use the context wrapper (the <span class="s15">with </span>keyword, which will open and close file for us) to gather the zipped file handler and extract all of the included files using <span class="s22">.extractall </span>and the given <span class="s22">FOLDER</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">from <span style=" color: #231F20;">zipfile </span>import <span style=" color: #231F20;">ZipFile</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Unzipping images...&#39;</span>) <span style=" color: #C71F43;">with </span>ZipFile(FILE) <span style=" color: #C71F43;">as </span>zip_images:</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">zip_images.extractall(FOLDER)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 20pt;text-indent: -12pt;text-align: left;">The full code for retrieving the data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">from <span style=" color: #231F20;">zipfile </span>import <span style=" color: #231F20;">ZipFile </span>import <span style=" color: #231F20;">os</span></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">urllib</span></p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">urllib.request</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">URL <span style=" color: #C71F43;">= </span><span style=" color: #8F8633;">&#39;https://nnfs.io/datasets/fashion_mnist_images.zip&#39; </span>FILE <span style=" color: #C71F43;">= </span><span style=" color: #8F8633;">&#39;fashion_mnist_images.zip&#39;</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">FOLDER <span style=" color: #C71F43;">= </span><span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">if not <span style=" color: #231F20;">os.path.isfile(FILE):</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;Downloading </span>{URL} <span style=" color: #8F8633;">and saving as </span>{FILE}<span style=" color: #8F8633;">...&#39;</span>) urllib.request.urlretrieve(URL, FILE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Unzipping images...&#39;</span>) <span style=" color: #C71F43;">with </span>ZipFile(FILE) <span style=" color: #C71F43;">as </span>zip_images:</p><p class="s10" style="padding-left: 19pt;text-indent: 24pt;line-height: 216%;text-align: left;">zip_images.extractall(FOLDER) <span style=" color: #32A7BD;">print</span>(<span style=" color: #8F8633;">&#39;Done!&#39;</span>)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Running this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">Downloading https://nnfs.io/datasets/fashion_mnist_images.zip and saving <span style=" color: #F9F8F0;">as </span>fashion_mnist_images.zip<span style=" color: #7358A5;">...</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">Unzipping images<span style=" color: #7358A5;">... </span>Done!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">You should now have a directory called <i>fashion_mnist_images</i>, containing <i>test </i>and <i>train </i>directories and the data license. Inside of both the <i>test </i>and <i>train </i>directories, we have ten subdirectories, numbered 0 through 9. These numbers are classifications that correspond to the images within. For example, if we open directory <i>0</i>, we can see these are images of shirts with either short sleeves or no sleeves at all. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 192pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 19.01: <span class="p">Example t-shirt image from the Fasion MNIST dataset.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark303">Inside directory </a><i>7</i>, we have non-boot shoes, or <i>sneakers </i>as the creators of this dataset have classified them. For example:<a name="bookmark312">&zwnj;</a><a name="bookmark313">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 192pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 19.02: <span class="p">Example sneaker image from the Fasion MNIST dataset.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 130%;text-align: center;">It’s common practice to grayscale (go from 3-channel RGB values per pixel to a single black to white range of 0-255 per pixel) images, though these images are already grayscaled. It is also a common practice to resize images to <b>normalize </b>their dimensions, but once again, the Fashion MNIST dataset is prepared so that all the images are already all the same shape (28x28).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Data loading</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, we have to read these images into Python and associate the image (pixel) data with the respective labels. We can access the directories as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">os</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">labels <span style=" color: #C71F43;">= </span>os.listdir(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train&#39;</span>) <span style=" color: #32A7BD;">print</span>(labels)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #8F8633;">&#39;0&#39;</span>, <span style=" color: #8F8633;">&#39;1&#39;</span>, <span style=" color: #8F8633;">&#39;2&#39;</span>, <span style=" color: #8F8633;">&#39;3&#39;</span>, <span style=" color: #8F8633;">&#39;4&#39;</span>, <span style=" color: #8F8633;">&#39;5&#39;</span>, <span style=" color: #8F8633;">&#39;6&#39;</span>, <span style=" color: #8F8633;">&#39;7&#39;</span>, <span style=" color: #8F8633;">&#39;8&#39;</span>, <span style=" color: #8F8633;">&#39;9&#39;</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since the subdirectory names are the labels themselves, we can reference individual samples for each class by looking through the files in each numbered subdirectory:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">files <span style=" color: #C71F43;">= </span>os.listdir(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train/0&#39;</span>) <span style=" color: #32A7BD;">print</span>(files[:<span style=" color: #7358A5;">10</span>])</p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span>len<span style=" color: #231F20;">(files))</span></p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><a name="bookmark314">&gt;&gt;&gt;</a></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">[<span style=" color: #8F8633;">&#39;0000.png&#39;</span>, <span style=" color: #8F8633;">&#39;0001.png&#39;</span>, <span style=" color: #8F8633;">&#39;0002.png&#39;</span>, <span style=" color: #8F8633;">&#39;0003.png&#39;</span>, <span style=" color: #8F8633;">&#39;0004.png&#39;</span>, <span style=" color: #8F8633;">&#39;0005.png&#39;</span>, <span style=" color: #8F8633;">&#39;0006.png&#39;</span>, <span style=" color: #8F8633;">&#39;0007.png&#39;</span>, <span style=" color: #8F8633;">&#39;0008.png&#39;</span>, <span style=" color: #8F8633;">&#39;0009.png&#39;</span>]</p><p class="s14" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">6000</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As you can see, we have 6,000 samples of class 0. In total, we have 60,000 samples -- 6,000 per classification. Meaning our dataset is also already <b>balanced</b>; each class occurs with the same frequency. If a dataset is not already balanced, the neural network will likely become biased to predict the class containing the most images. This is because neural networks fundamentally seek out the steepest and quickest gradient descent to decrease loss, which might lead to a local</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">minimum making the model unable to find the global loss minimum. We have a total of 10 classes here, so a random prediction, with a balanced dataset, would have an accuracy of about 10%.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Imagine, however, if the balance of classes in the dataset was 64% for class 0 and 4% for 1 through 9. The neural network might very quickly learn to always predict class 0. Though the model would rapidly decrease loss initially, it would likely remain stuck predicting class 0 with an accuracy closer to 64%. In such a case, we’re better off trimming away samples from the high- frequency classes so that we have the same number of samples per class.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Another option is to use class weights, weighting classes that occur more often with a fraction of 1 when accounting for loss. Though we have never seen this work well in practice. With image data, another option would be to augment the samples through actions like cropping, rotation, and maybe flipping horizontally or vertically. Before applying such transformations, ensure they will generate valid samples that fit your objectives. Luckily for us, we don’t need to worry about that since the Fashion MNIST data are indeed perfectly balanced. We’ll now explore our data</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">by looking at individual samples. To handle image data, we’re going to make use of the Python package containing OpenCV, under the <b>cv2 </b>library, which you can install with pip:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">pip install opencv-python</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">And to load the image data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">cv2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train/7/0002.png&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">cv2.IMREAD_UNCHANGED)</p><p class="s12" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(image_data)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">We read in images with <span class="s22">cv2.imread()</span>, where the first parameter is the path to the image. The <span class="s22">cv2.IMREAD_UNCHANGED </span>argument notifies the <span class="s22">cv2 </span>package that we intend to read in these images in the same format as they were saved (grayscale in this case). By default, OpenCV will convert these images to use all 3 color channels, even though this is a grayscale image. As a result, we have a 2D array of numbers —  grayscale pixel values. If we formatted this otherwise messy</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">array before printing with the following line of the code, which will inform NumPy, since the loaded image is a numpy array object, to print more characters in a line:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np np.set_printoptions(</span><span class="s23">linewidth</span>=<span style=" color: #7358A5;">200</span><span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We’d still likely be able to identify the subject:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:5.5pt" cellspacing="0"><tr style="height:8pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:8pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:18pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 6pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:16pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:10pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: center;">1</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:139pt" colspan="9"><p class="s79" style="padding-left: 7pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0 49 135 182 150 59 0 0 0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:8pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:139pt" colspan="9"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: left;">78 255 220 212 219 255 246 191 155</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">87</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:10pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:171pt" colspan="11"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">57 206 215 203 191 203 212 216 217 220 211</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 8pt;text-align: right;">15</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:8pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:17pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">1</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:13pt"><p class="s79" style="padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: right;">58</p></td><td style="width:171pt" colspan="11"><p class="s79" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">231 220 210 199 209 218 218 217 208 200 215</p></td><td style="width:17pt"><p class="s79" style="padding-right: 4pt;text-indent: 0pt;line-height: 7pt;text-align: right;">56</p></td><td style="width:16pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:10pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 8pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 8pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">2</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 8pt;text-align: center;">0</p></td><td style="width:325pt" colspan="21"><p class="s79" style="padding-left: 7pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0 4 0 0 0 0 145 213 207 199 187 203 210 216 217 215 215 206 215 130 0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">1</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">2</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">4</p></td><td style="width:325pt" colspan="21"><p class="s79" style="padding-left: 7pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0 0 0 3 105 225 205 190 201 210 214 213 215 215 212 211 208 205 207 218 0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">1</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">5</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">7</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:325pt" colspan="21"><p class="s79" style="padding-left: 7pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0 52 162 217 189 174 157 187 198 202 217 220 223 224 222 217 211 217 201 247 65<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:8pt"><td style="width:14pt"><p class="s78" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">[</p></td><td style="width:14pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">21</p></td><td style="width:325pt" colspan="21"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">72 185 189 171 171 185 203 200 207 208 209 214 219 222 222 224 215 218 211 212 148<span style=" color: #231F20;">]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:9.35pt" cellspacing="0"><tr style="height:8pt"><td style="width:10pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:10pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:420pt" colspan="4"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">70 114 129 145 159 179 196 172 176 185 196 199 206 201 210 212 213 216 218 219 217 212 207 208 200 198 173<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:8pt"><td style="width:10pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:10pt"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:420pt" colspan="4"><p class="s79" style="padding-left: 1pt;text-indent: 0pt;line-height: 7pt;text-align: left;">122 158 184 194 192 193 196 203 209 211 211 215 218 221 222 226 227 227 226 226 223 222 216 211 208 216 185<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:20pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[ <span style=" color: #7358A5;">21</span></p></td><td style="width:19pt"><p class="s79" style="padding-left: 9pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:14pt"><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">12</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">48</p></td><td style="width:356pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;">82 123 152 170 184 195 211 225 232 233 237 242 242 240 240 238 236 222 209 200 193 185 106<span style=" color: #231F20;">]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:9.35pt" cellspacing="0"><tr style="height:8pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">26</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">47</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">54</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">18</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">5</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">2</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">4</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">6</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">9</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">9</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">8</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">9</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">6</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">6</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">4</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">2</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">10</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">27</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">45</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">55</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">59</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">57</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">50</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">44</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">51</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">58</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">62</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">65</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">56</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">54</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">57</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">59</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">61</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">60</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">63</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">68</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">67</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">66</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">73</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">77</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">74</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">65</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 5pt;text-indent: 0pt;line-height: 7pt;text-align: center;">39<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">4</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">9</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">18</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">23</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">26</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">25</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">23</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">25</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">29</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">37</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">38</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">37</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">39</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">36</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">29</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">31</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">33</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">34</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">28</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">24</p></td><td style="width:15pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">20</p></td><td style="width:16pt"><p class="s79" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 7pt;text-align: center;">14</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">7</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:9pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:8pt"><td style="width:8pt"><p class="s78" style="padding-left: 2pt;text-indent: 0pt;line-height: 7pt;text-align: left;">[</p></td><td style="width:14pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-right: 3pt;text-indent: 0pt;line-height: 7pt;text-align: right;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:16pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:15pt"><p class="s79" style="padding-left: 4pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0</p></td><td style="width:22pt"><p class="s79" style="padding-left: 6pt;padding-right: 1pt;text-indent: 0pt;line-height: 7pt;text-align: center;">0<span style=" color: #231F20;">]]</span></p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, it’s a sneaker. Rather than formatting the raw values to see what we’re looking at this way, we can use Matplotlib to visualize this. For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt plt.imshow(image_data) plt.show()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-bottom: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 19.03: <span class="p">Sneaker image shown with Matplotlib after loading with Python</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We can check another sample:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt, cv2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train/4/0011.png&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">cv2.IMREAD_UNCHANGED)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">plt.imshow(image_data) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-bottom: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 2pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 19.04: <span class="p">Jacket image shown with Matplotlib after loading with Python</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">It looks like a jacket. If we check our table from before, class 4 is “coat.” You might wonder about the strange coloring, but this is just the default from matplotlib not expecting grayscale. We can notify Matplotlib that this is grayscale by specifying a <span class="s29">cmap </span>(colormap) during the <span class="s22">plt.imshow() </span>call:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt, cv2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train/4/0011.png&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 40pt;text-indent: 0pt;text-align: center;">cv2.IMREAD_UNCHANGED)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 109%;text-align: left;">plt.imshow(image_data, <span class="s23">cmap</span><span style=" color: #C71F43;">=</span><span style=" color: #8F8633;">&#39;gray&#39;</span>) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-bottom: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 19.05: <span class="p">Grayscaled jacket image</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">Now we can iterate over all of the samples, load them, and put them into the input data (<b>X</b>) and targets (<b>y</b>) lists. First, we scan the train folder, which, as noted before, contains folders named from 0 to 9, which also act as sample labels. We iterate through these folders and the images inside them, appending them to a list variable (named <span class="s22">X</span><i>) </i>along with their respective labels to another list variable (named <span class="s22">y</span><i>)</i>, forming our samples and ground-truth, or target labels:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Scan all the directories and create a list of labels </span>labels <span style=" color: #C71F43;">= </span>os.listdir(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create lists for samples and labels </span>X <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each label folder </span>for <span style=" color: #231F20;">label </span>in <span style=" color: #231F20;">labels:</span></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># And for each image in given folder </span>for <span style=" color: #231F20;">file </span>in <span style=" color: #231F20;">os.listdir(os.path.join(</span></p><p class="s38" style="padding-left: 140pt;text-indent: 0pt;text-align: left;">&#39;fashion_mnist_images&#39;<span style=" color: #231F20;">, </span>&#39;train&#39;<span style=" color: #231F20;">, label</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">)):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Read the image</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">image <span style=" color: #C71F43;">= </span>cv2.imread(os.path.join(</p><p class="s38" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">&#39;fashion_mnist_images/train&#39;<span style=" color: #231F20;">, label, file</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">), cv2.IMREAD_UNCHANGED)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And append it and a label to the lists <span style=" color: #231F20;">X.append(image)</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">y.append(label)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We need to do this operation on both the testing and training data. Again, they are already nicely split up for us. Many times, you will need to separate your data into train and test groups on your own. We’ll convert the above code into a function to prevent duplicating the code for training and testing directories. This function will take a dataset type as a parameter: train or test, along with the path where those datasets are located:</p><p class="s11" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">cv2</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">os</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Loads a MNIST dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load_mnist_dataset</span>(<span class="s23">dataset</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Scan all the directories and create a list of labels </span>labels <span style=" color: #C71F43;">= </span>os.listdir(os.path.join(path, dataset))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create lists for samples and labels </span>X <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each label folder </span>for <span style=" color: #231F20;">label </span>in <span style=" color: #231F20;">labels:</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># And for each image in given folder</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">for <span style=" color: #231F20;">file </span>in <span style=" color: #231F20;">os.listdir(os.path.join(path, dataset, label)): </span><span style=" color: #A5A4A5;"># Read the image</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">image <span style=" color: #C71F43;">= </span>cv2.imread(os.path.join(</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;text-align: left;">path, dataset, label, file</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">), cv2.IMREAD_UNCHANGED)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And append it and a label to the lists <span style=" color: #231F20;">X.append(image)</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y.append(label)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Convert the data to proper numpy arrays and return </span><span style=" color: #C71F43;">return </span>np.array(X), np.array(y).astype(<span style=" color: #8F8633;">&#39;uint8&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Since <span class="s22">X </span>has been defined as a list, and we are adding images represented as NumPy arrays to this list, we’ll call <span class="s22">np.array() </span>on <span class="s22">X </span>at the end to transform it from a list into a proper NumPy array. We will do the same with the labels (<span class="s22">y</span>) since they are a list of numbers and additionally inform NumPy that our labels are integer (not float) values.</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we can write a function that will create and return our train and test data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># MNIST dataset (train + test)</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">create_data_mnist</span>(<span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Load both sets separately</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;train&#39;</span>, path)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;test&#39;</span>, path)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And return all the data <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">X, y, X_test, y_test</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Code up to this point for our new data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">cv2</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">os</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Loads a MNIST dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load_mnist_dataset</span>(<span class="s23">dataset</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Scan all the directories and create a list of labels </span>labels <span style=" color: #C71F43;">= </span>os.listdir(os.path.join(path, dataset))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create lists for samples and labels </span>X <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each label folder </span>for <span style=" color: #231F20;">label </span>in <span style=" color: #231F20;">labels:</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># And for each image in given folder</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">for <span style=" color: #231F20;">file </span>in <span style=" color: #231F20;">os.listdir(os.path.join(path, dataset, label)): </span><span style=" color: #A5A4A5;"># Read the image</span></p><p class="s10" style="padding-left: 19pt;text-indent: 72pt;line-height: 108%;text-align: left;">image <span style=" color: #C71F43;">= </span>cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And append it and a label to the lists <span style=" color: #231F20;">X.append(image)</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y.append(label)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Convert the data to proper numpy arrays and return </span><span style=" color: #C71F43;">return </span>np.array(X), np.array(y).astype(<span style=" color: #8F8633;">&#39;uint8&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># MNIST dataset (train + test)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">create_data_mnist</span>(<span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Load both sets separately</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;train&#39;</span>, path)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;test&#39;</span>, path)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And return all the data <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">X, y, X_test, y_test</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Thanks to this function, we can load in our data by doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark304">Data preprocessing</a><a name="bookmark315">&zwnj;</a><a name="bookmark316">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, we will <b>scale </b>the data (not the images, but the data representing them, the numbers). Neural networks tend to work best with data in the range of either 0 to 1 or -1 to 1. Here, the image data are within the range 0 to 255. We have a decision to make with <i>how </i>to scale these data. Usually, this process will be some experimentation and trial and error. For example, we could scale images to be between the range of -1 and 1 by taking each pixel value, subtracting half the maximum of all pixel values (i.e., 255/2 = 127.5), then dividing by this same half to produce a range bounded by -1 and 1. We could also scale our data between 0 and 1 by simply dividing it by 255 (the maximum value). To start, we opt to scale between -1 and 1. Before we do that, we have to change the datatype of the NumPy array, which is currently <span class="s22">uint8 </span>(unsigned integer, holds integer</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: left;">values in the range of 0 to 255). If we don’t do this, NumPy will convert it to a <span class="s22">float64 </span>data</p><p style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">type while our intention is to use <span class="s22">float32</span>, a 32-bit float value. This can be achieved by calling</p><p class="s22" style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">.astype(np.float32) <span class="p">on a NumPy array object. We will leave the labels untouched:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale features</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X_test <span style=" color: #C71F43;">= </span>(X_test.astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Ensure that you scale both training and testing data using identical methods. Later, when making predictions, you will also need to scale the input data for inference. It can be easy to forget to scale your data in these different places. You also want to be sure that any preprocessing, like scaling,</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">is informed only by your training dataset. In this example, we knew the minimum (min) and maximum (max) values would be 0 and 255, and performed linear scaling. You will often need to first query your dataset for min and max values for use in scaling. You may also use other methods to scale if your dataset has extreme outliers, as min/max may not work well. If this is the case, you might use some combination of the average value and standard deviation to create your scaling method.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A common mistake when scaling is to allow the testing dataset to inform transformations made to the training dataset. There is only one exception to this rule, which is when the data is being scaled linearly, for example, by the mentioned division by a constant number. Any non-linear scaling function could possibly leak information from testing or validation data into training data. Any preprocessing rules should be derived without knowledge of the testing dataset, but then</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;"><a name="bookmark317">applied to the testing set. For example, your entire dataset might have a min value of 0 and a max of 125, while the training dataset only has a min of 0 and a max of 100. You will still use the 100 value when scaling your testing dataset. This means that your testing dataset might not fit neatly between the bounds of -1 and 1 after scaling, but this usually should not be a problem. In the case of a bigger difference, you can additionally scale the data linearly by dividing them by some number.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Back to our data, let’s check that our data have been scaled:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(X.min(), X.max())</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">1.0 1.0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we check the shape of our input data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(X.shape)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">60000</span>, <span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">Our Dense layers work on batches of 1-dimensional vectors. They cannot operate on images shaped as a 28x28, 2-dimensional array. We need to take these 28x28 images and <b>flatten </b>them, which means to take every row of an image array and append it to the first row of that array, transforming the 2-dimensional (2D) array of an image into a 1-dimensional (1D) array (i.e., vector), or in other words, we could see this as unwrapping numbers in a 2D array to a list-like form. There are neural network models called convolutional neural networks that will allow you to pass 2D image data “as is,” but a dense neural network like we have here expects samples that are 1D. Even in convolutional neural networks, you will usually need to flatten data before feeding them to an output layer or a dense layer. To flatten an array with NumPy, we can reshape using <span class="s15">-</span><span class="s21">1 </span>as a first shape dimension, which means “however many elements are there” and effectively put them all in the first dimension making a flat 1D array. For an example of this concept:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">example <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], [<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]]) flattened <span style=" color: #C71F43;">= </span>example.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">print<span style=" color: #231F20;">(example) </span>print<span style=" color: #231F20;">(example.shape)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">print<span style=" color: #231F20;">(flattened) </span>print<span style=" color: #231F20;">(flattened.shape)</span></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:24pt"><p class="s25" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">&gt;&gt;&gt;</p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[[<span style=" color: #7358A5;">1</span></p></td><td style="width:23pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[<span style=" color: #7358A5;">3</span></p></td><td style="width:23pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4<span style=" color: #231F20;">]]</span></p></td></tr><tr style="height:13pt"><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">(<span style=" color: #7358A5;">2</span>,</p></td><td style="width:23pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2<span style=" color: #231F20;">)</span></p></td></tr></table><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><a name="bookmark318">[</a><span style=" color: #7358A5;">1 2 3 4</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">4</span>,)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">We could also use </span>np.flatten()<span class="p">, but our intention differs with a batch of samples. In the case of our samples, we still wish to retain all 60,000 of them, so we’d like to reshape our training data to be </span>(<span style=" color: #7358A5;">60000</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>)<span class="p">. This will notify NumPy that we wish to keep the 60,000 samples (first dimension), but flatten the rest (-1 as the second dimension means that we want to put all of the sample data in this single dimension into a 1D array). This will create 60,000 samples of 784</span></p><p class="s22" style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">features each. The 784 features are the result of 28·28. To do this, we’ll use the number of samples from the training (</span>X.shape[<span style=" color: #7358A5;">0</span>]<span class="p">) and testing (</span>X_test.shape[<span style=" color: #7358A5;">0</span>]<span class="p">) datasets respectively and reshape them:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Reshape to vectors</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X <span style=" color: #C71F43;">= </span>X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>)</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X_test <span style=" color: #C71F43;">= </span>X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">You can achieve the same result by explicitly defining the shape, instead of relying on NumPy inference:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">.reshape(X.shape[<span style=" color: #7358A5;">0</span>], X.shape[<span style=" color: #7358A5;">1</span>]<span style=" color: #C71F43;">*</span>X.shape[<span style=" color: #7358A5;">2</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">which would be more explicit, but we find this to be less legible.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark305">Data Shuffling</a><a name="bookmark319">&zwnj;</a><a name="bookmark320">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Our dataset currently consists of samples and their target classifications, in order, from 0 to 9. To illustrate, we can query our <i>y </i>data at various points. The first 6000 will all be 0. For instance:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(y[<span style=" color: #7358A5;">0</span>:<span style=" color: #7358A5;">10</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 0 0 0 0 0 0 0 0 0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">If we then query a bit later:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(y[<span style=" color: #7358A5;">6000</span>:<span style=" color: #7358A5;">6010</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">1 1 1 1 1 1 1 1 1 1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is a problem if we train our network with data in this order; for the same reason, an imbalanced dataset is problematic.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While we train on the first 6,000 samples, the model will learn that the quickest way to reduce loss is to always predict a 0, since it’ll see several batches of the data with class 0 only. Then, between 6,000 and 12,000, the loss will initially spike as the label will change and the model will be predicting, incorrectly, still label 0, and will likely learn that now it needs to always predict class 1 (as that’s all it sees in batches of labels which we optimize for). The model will cycle local minimums following whichever label is currently being repeated over batches and will most likely never find a global minimum. This process will continue until we get through all samples, repeating however many epochs we selected.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Preferably, there would be many classifications (ideally some from each class) of samples per fitment to keep the model from becoming biased towards any single class, simply because that’s the class it’s been seeing lately. Thus, we often will randomly shuffle the data. We didn’t need to shuffle our previous training data, like the spiral data, since we were training on the entire</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">dataset at once anyway, rather than individual batches. With this larger dataset that we’re training on in batches, we want to shuffle the data, as it’s currently organized in order of chunks of 6,000 samples per label. When shuffling, we want to ensure that we shuffle the sample and target</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">arrays the same; otherwise, we’ll have a very confused (and very wrong, in most cases) model as labels will no longer match samples. Hence, we cannot simply call <span class="s20">shuffle() </span>on both of them separately. There are many ways to achieve this, but what we’ll do is gather all of the “keys,” which are the same for samples and targets, and then shuffle them.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">These keys will be values from 0 to 59999 in this case.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) <span style=" color: #32A7BD;">print</span>(keys[:<span style=" color: #7358A5;">10</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">array([<span style=" color: #7358A5;">0 1 2 3 4 5 6 7 8 9</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then, we can shuffle these keys:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">nnfs nnfs.init()</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">np.random.shuffle(keys) <span style=" color: #32A7BD;">print</span>(keys[:<span style=" color: #7358A5;">10</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[ <span style=" color: #7358A5;">3048 19563 58303 8870 40228 31488 21860 56864 845 25770</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now, this is essentially the new order of indexes, which we can then apply by doing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This tells NumPy to return values of given indices as we would normally index NumPy arrays, but we’re using a variable that keeps a list of randomly ordered indices. Then we can check a slice of targets:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(y[:<span style=" color: #7358A5;">15</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 3 9 1 6 5 3 9 0 4 8 9 0 6 6</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">They seem to be shuffled. We can check individual samples as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">plt.imshow((X[<span style=" color: #7358A5;">8</span>].reshape(<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))) <span style=" color: #A5A4A5;"># Reshape as image is a vector already </span>plt.show()</p><p class="s11" style="padding-top: 2pt;padding-bottom: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 19.06: <span class="p">Random (shirt) image after shuffling</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We can then check the class at the same index:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(y[<span style=" color: #7358A5;">8</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Class 0 is indeed “shirt,” and so these data do look properly shuffled. You may check a few more manually to ensure your data are as expected. If the model does not train or appears to be misbehaving, you will want to double-check how you preprocessed the data.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark306">Batches</a><a name="bookmark321">&zwnj;</a><a name="bookmark322">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far, we’ve trained our models by feeding the entire dataset as a single “batch” through the model. We discussed earlier in Chapter 2 why it is preferred to do more than 1 sample at a time, but is there a batch size that would be too big? Our dataset has been small enough for us to get away with the behavior of feeding the entire dataset at once, but real-world datasets can often be terabytes or more in size, which is a nonstarter for the majority of computers to process as</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">a single batch. A batch is a slice of a fixed size from the data. When we train with batches, we iterate through the dataset in a chunk, or “batch, ” of data at a time, performing a forward pass, loss calculation, backward pass, and optimization. If the data have been shuffled, and each batch is large enough and somewhat representative of the dataset, it is a fair assumption that each gradient of each batch should be a good approximation of the direction towards a global minimum. If</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the batch is too small, the direction of the gradient descent can fluctuate too much from batch to batch, causing the model to take a long time to train.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Common batch sizes range between 32 and 128 samples. You can go smaller if you’re having trouble fitting everything into memory, or larger if you want training to go faster, but this range is the typical range of batch sizes. You will usually see accuracy and loss improvements by increasing the batch size from say 2 to 8, or 8 to 32. At some point, however, you will</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">see diminishing returns regarding accuracy and loss if you keep increasing the batch size. Additionally, training with large batch sizes will become slow compared to the speed achievable with smaller batch sizes — like our examples earlier with the spiral data, requiring 10 thousand epochs to train! As is often the case with neural networks, it’s a lot of trial and error with your specific data and model. For example, imagine we select a batch size of 128, and we opt to do 10 epochs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This means, for each epoch, we will iterate over our data, fitting 128 samples at a time to train our model. Each batch of samples being trained is referred to as a <b>step</b>. We can calculate the number of <b>steps </b>by dividing the number of samples by the batch size:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">steps <span style=" color: #C71F43;">= </span>X.shape[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">// </span>BATCH_SIZE</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">We use the integer division operator, <span class="s15">//</span>, (instead of the floating-point division operator, <span class="s15">/</span>) to return an integer, as the number of steps cannot contain a fraction. This is the number of iterations that we’ll be making per epoch in a loop. If there are some straggler samples left over, we can add</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">them in by simply adding one more step:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 34pt;text-indent: -26pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span>steps <span style=" color: #C71F43;">* </span>BATCH_SIZE <span style=" color: #C71F43;">&lt; </span>X.shape[<span style=" color: #7358A5;">0</span>]: steps <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Why we add this 1 can be presented in a simple example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">batch_size <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">X <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">X <span style=" color: #C71F43;">= </span>[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>, <span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>, <span style=" color: #7358A5;">5</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #32A7BD;">print</span>(<span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 118%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Integer division rounds down; thus, if there are any samples left, we’ll add 1 to form the last batch with the remainder.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">An example of code leading up to training a model using batches:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">nnfs</span></p><p class="s11" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">from <span style=" color: #231F20;">nnfs.datasets </span>import <span style=" color: #231F20;">spiral_data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">nnfs.init()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>spiral_data(<span class="s23">samples</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">100</span>, <span class="s23">classes</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">3</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">EPOCHS <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">10</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">BATCH_SIZE <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">128 </span><span style=" color: #A5A4A5;"># We take 128 samples at once</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>steps <span style=" color: #C71F43;">= </span>X.shape[<span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">// </span>BATCH_SIZE</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining data, # but not a full batch, this won&#39;t include it.</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add 1 to include the remaining samples in 1 more step. </span><span style=" color: #C71F43;">if </span>steps <span style=" color: #C71F43;">* </span>BATCH_SIZE <span style=" color: #C71F43;">&lt; </span>X.shape[<span style=" color: #7358A5;">0</span>]:</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">steps <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s11" style="padding-top: 2pt;padding-left: 44pt;text-indent: -24pt;line-height: 108%;text-align: left;">for <span style=" color: #231F20;">epoch </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(EPOCHS): </span>for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(steps):</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>BATCH_SIZE:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>BATCH_SIZE] batch_y <span style=" color: #C71F43;">= </span>y[step<span style=" color: #C71F43;">*</span>BATCH_SIZE:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>BATCH_SIZE]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Now we perform forward pass, loss calculation, # backward pass and update parameters</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We loaded the dataset, defined the number of epochs and a batch size, then calculated the number of steps. Next, we have two loops — an outer one over the epochs and an inner one over the steps. During each step in each epoch, we’re selecting a slice of the training data. Now that we know how to train the model in batches, we’re interested in the training loss and accuracy for each step and epoch. So far, we’ve only been calculating loss per fit, but recall that we fitted against the entire dataset at once. Now, we’ll be interested in both batch-wise statistics and epoch-wise. For the overall loss and accuracy, we want to calculate a sample-wise average. To do this, we will accumulate the sum of losses from all epoch batches and counts to calculate the mean value at the end of each epoch. We’ll start in the common <span class="s35">Loss </span>class’ <span class="s35">calculate </span>method by adding:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of losses and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(sample_losses) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Making the full <span class="s35">calculate </span>method in the <span class="s35">Loss </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculates the data and regularization losses # given model output and ground truth values</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of losses and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(sample_losses) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’re saving the sum and the count so we can calculate the mean at any point. To do that, we’ll add a new method called <span class="s35">calculate_accumulated </span>inside the <span class="s35">Loss </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">This method can also return the regularization loss if <span class="s29">include_regularization </span>is set to <span class="s21">True</span>. The regularization loss does not need to be accumulated as it’s calculated from the current state of layer parameters, at the time it’s called. We’ll be using this ability during training, but not while evaluating and predicting; we’ll discuss this in more detail shortly.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Finally, in order to reset the sum and count values for a new epoch, we’ll add one last method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Making our full common <span class="s35">Loss </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate regularization loss # iterate all trainable layers</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: -42pt;line-height: 108%;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">* </span>\ layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p class="s11" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss </span><span style=" color: #A5A4A5;"># Set/remember trainable layers</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">remember_trainable_layers</span>(<span class="s23">self</span>, <span class="s23">trainable_layers</span>): self.trainable_layers <span style=" color: #C71F43;">= </span>trainable_layers</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculates the data and regularization losses # given model output and ground truth values</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of losses and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(sample_losses) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">We’ll want to implement the same things for the <span class="s35">Accuracy </span>class now:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common accuracy class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Accuracy</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates an accuracy</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># given predictions and ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get comparison results</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">comparisons <span style=" color: #C71F43;">= </span>self.compare(predictions, y) <span style=" color: #A5A4A5;"># Calculate an accuracy</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>np.mean(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of matching values and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(comparisons) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return accuracy <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate an accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Here, we’ve added setting the <span class="s22">accumulated_sum </span>and <span class="s22">accumulated_count </span>properties in the <span class="s35">calculate </span>method for the epoch accuracy calculation, added a new <span class="s35">calculate_</span></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;"><span class="s35">accumulated </span>method that returns this accuracy, and finally added a <span class="s35">new_pass </span>method to reset the <span class="s22">accumulated_sum </span>and <span class="s22">accumulated_count </span>values that we’ll use at the beginning of each epoch.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Now, we’ll modify the <span class="s35">train </span>method for our <span class="s35">Model </span>class. First, we’ll add a new parameter called <span class="s29">batch_size</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 104pt;text-indent: -60pt;line-height: 118%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: justify;">We’ll default this parameter to <span class="s21">None</span>, which means to use the entire dataset as the batch. In this case, training will take 1 step per epoch, where that step consists of feeding all the data through the network at once.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not set <span style=" color: #231F20;">train_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If there is validation data passed,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># set default number of steps for validation as well </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">validation_steps <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As discussed, most “real life” datasets will require a batch size smaller than that of all samples. We’ll handle that using the method that we described earlier: performing integer division of the number of all samples by the batch size and eventually adding 1 to include any remaining samples that did not form a full batch (we’ll do that for both training and validation data):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">train_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Dividing rounds down. If there are some remaining # data, but not a full batch, this won&#39;t include it # Add 1 to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: justify;">if <span style=" color: #231F20;">train_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): train_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: validation_steps </span>= <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val) </span>// <span style=" color: #231F20;">batch_size</span></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data, but nor full batch, this won&#39;t include it # Add 1 to include this not full batch</p><p class="s11" style="padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val): validation_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Next, starting at the top, we’ll modify the loop over epochs to print an epoch number and then reset the accumulated epoch loss and accuracy values. Then, inside of here, we’ll add a new loop that will iterate over steps in the epoch.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print epoch number </span><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">self.loss.new_pass()</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.accuracy.new_pass()</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(train_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Inside of each step, we’ll need to grab the <b>batch </b>of data that we’ll use to train — either the full dataset if the <span class="s22">batch_size </span>parameter is still the default <span class="s21">None </span>or a slice of size <span class="s22">batch_size</span>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 110pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: -6pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X batch_y <span style=" color: #C71F43;">= </span>y</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 118%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size] batch_y <span style=" color: #C71F43;">= </span>y[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">With each of these batches, we fit and print information, similar to how we were fitting per epoch. The difference now is we use <span class="s22">batch_X </span>instead of <span class="s22">X </span>and <span class="s22">batch_y </span>instead of <span class="s22">y</span>. The other change is the <span class="s15">if </span>statement for the summary printing that will account for steps instead of epochs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>) <span style=" color: #A5A4A5;"># Calculate loss</span></p><p class="s10" style="padding-left: 141pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, batch_y,</p><p class="s11" style="padding-left: 116pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 225pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions,</p><p class="s10" style="padding-top: 1pt;padding-left: 328pt;text-indent: 0pt;text-align: left;">batch_y)</p><p class="s17" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 116pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>step <span style=" color: #C71F43;">% </span>print_every <span style=" color: #C71F43;">or </span>step <span style=" color: #C71F43;">== </span>train_steps <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;step: </span>{step}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 2pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we’d like to print information like accuracy and loss per epoch:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print epoch loss and accuracy </span>epoch_data_loss, epoch_regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="text-indent: 0pt;text-align: right;">self.loss.calculate_accumulated(</p><p class="s23" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">include_regularization<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">epoch_loss <span style=" color: #C71F43;">= </span>epoch_data_loss <span style=" color: #C71F43;">+ </span>epoch_regularization_loss epoch_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated() <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;training, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{epoch_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{epoch_loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{epoch_data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{epoch_regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 2pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">If the batch size is set, the chances are that our validation data will be larger than this batch size, so we need to add batching for the validation data as well:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss # and accuracy objects <span style=" color: #231F20;">self.loss.new_pass() self.accuracy.new_pass()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(validation_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val batch_y <span style=" color: #C71F43;">= </span>y_val</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;text-align: left;">]</p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_y <span style=" color: #C71F43;">= </span>y_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="text-indent: 0pt;text-align: center;">]</p><p class="s17" style="padding-top: 2pt;padding-left: 140pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate the loss <span style=" color: #231F20;">self.loss.calculate(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 141pt;text-indent: 108pt;line-height: 108%;text-align: left;">output) self.accuracy.calculate(predictions, batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print validation loss and accuracy </span>validation_loss <span style=" color: #C71F43;">= </span>self.loss.calculate_accumulated() validation_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span><i>f</i><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{validation_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{validation_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">Compared to our current codebase, we’ve added calls to the <span class="s22">new_pass </span>method, of both loss and accuracy objects, which reset values accumulated during the training step. Next, we introduced batches (a loop iterating over steps), and removed catching a return from the loss calculation  (we don’t care about batch loss during validation, just the final, overall loss). The last steps were to add handling for the overall validation loss and replace <span class="s22">X_val </span>with <span class="s22">batch_X </span>and <span class="s22">y_val </span>to <span class="s22">batch_y </span>to match the changes made to the training code.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">This makes our full <span class="s35">train </span>method for the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">train_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If there is validation data passed,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># set default number of steps for validation as well </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">validation_steps <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p class="s11" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">train_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Dividing rounds down. If there are some remaining # data, but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: justify;">if <span style=" color: #231F20;">train_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): train_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: validation_steps </span>= <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val) </span>// <span style=" color: #231F20;">batch_size</span></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data, but nor full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val): validation_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: justify;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print epoch number </span><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">self.loss.new_pass()</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.accuracy.new_pass()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(train_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X batch_y <span style=" color: #C71F43;">= </span>y</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size] batch_y <span style=" color: #C71F43;">= </span>y[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, batch_y,</p><p class="s11" style="padding-left: 116pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p class="s10" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 225pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions,</p><p class="s10" style="padding-top: 1pt;padding-left: 328pt;text-indent: 0pt;text-align: left;">batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 116pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>step <span style=" color: #C71F43;">% </span>print_every <span style=" color: #C71F43;">or </span>step <span style=" color: #C71F43;">== </span>train_steps <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;step: </span>{step}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print epoch loss and accuracy </span>epoch_data_loss, epoch_regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="text-indent: 0pt;text-align: right;">self.loss.calculate_accumulated(</p><p class="s23" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">include_regularization<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">epoch_loss <span style=" color: #C71F43;">= </span>epoch_data_loss <span style=" color: #C71F43;">+ </span>epoch_regularization_loss epoch_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span><i>f</i><span style=" color: #8F8633;">&#39;training, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{epoch_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{epoch_loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{epoch_data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{epoch_regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss # and accuracy objects <span style=" color: #231F20;">self.loss.new_pass() self.accuracy.new_pass()</span></p><p class="s17" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(validation_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val batch_y <span style=" color: #C71F43;">= </span>y_val</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;text-align: left;">]</p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_y <span style=" color: #C71F43;">= </span>y_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="text-indent: 0pt;text-align: center;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate the loss <span style=" color: #231F20;">self.loss.calculate(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 141pt;text-indent: 108pt;line-height: 108%;text-align: left;">output) self.accuracy.calculate(predictions, batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print validation loss and accuracy </span>validation_loss <span style=" color: #C71F43;">= </span>self.loss.calculate_accumulated() validation_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{validation_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{validation_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark307">Training</a><a name="bookmark323">&zwnj;</a><a name="bookmark324">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At this point, we’re ready to train using batches and our new dataset. As a reminder, we create the data with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then shuffle with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then flatten sample-wise and scale to the range of -1 to 1:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 104pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Then construct our model consisting of 2 hidden layers using ReLU activation, an output layer with softmax activation since we’re building a classification model, cross-entropy loss, Adam optimizer, and categorical accuracy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">64</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">64</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Set loss, optimizer and accuracy objects:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">5e-5</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Finally, we finalize and train!</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">5</span><span class="s10">, </span>batch_size<span class="s11">=</span><span class="s14">128</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.078</span>, loss: <span style=" color: #7358A5;">2.303 </span>(data_loss: <span style=" color: #7358A5;">2.303</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.001</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.719</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.660</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.660<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009950248756218907</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.789</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.560</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.560<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009900990099009901</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.781</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.612</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.612<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009852216748768474</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.781</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.518</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.518<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.000980392156862745</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.833</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.400</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.400<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.0009771350400625367</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.720</span>, loss: <span style=" color: #7358A5;">0.746 </span>(data_loss: <span style=" color: #7358A5;">0.746</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0009771350400625367</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.805</span>, loss: <span style=" color: #7358A5;">0.537</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.859</span>, loss: <span style=" color: #7358A5;">0.444 </span>(data_loss: <span style=" color: #7358A5;">0.444</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0009770873027505008</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.789</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.475</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.475<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.000972337012008362</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.859</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.357</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.357<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009676326866321544</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.836</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.461</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.461<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009629736626703259</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.789</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.437</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.437<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr></table><p class="s14" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009583592888974076</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.885</span>, loss: <span style=" color: #7358A5;">0.324 </span>(data_loss: <span style=" color: #7358A5;">0.324</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009552466924583273</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.832</span>, loss: <span style=" color: #7358A5;">0.461 </span>(data_loss: <span style=" color: #7358A5;">0.461</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009552466924583273</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.836</span>, loss: <span style=" color: #7358A5;">0.458</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">3</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.859</span>, loss: <span style=" color: #7358A5;">0.387 </span>(data_loss: <span style=" color: #7358A5;">0.387</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009552010698251983</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.820</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.433</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.433<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009506607091928891</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.859</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.320</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.320<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009461633077869241</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.859</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.424</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.424<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009417082587814295</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.812</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.394</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.394<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009372949667260287</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.875</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.286</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.286<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.000934317481080071</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.851</span>, loss: <span style=" color: #7358A5;">0.407 </span>(data_loss: <span style=" color: #7358A5;">0.407</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.000934317481080071</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.847</span>, loss: <span style=" color: #7358A5;">0.422</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">4</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.859</span>, loss: <span style=" color: #7358A5;">0.350 </span>(data_loss: <span style=" color: #7358A5;">0.350</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009342738356612324</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.828</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.398</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.398<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009299297903008323</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.867</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.310</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.310<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009256259545517657</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.891</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.393</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.393<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009213617727000506</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.836</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.363</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.363<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009171366992250195</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.885</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.264</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.264<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.0009142857142857143</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.862</span>, loss: <span style=" color: #7358A5;">0.378 </span>(data_loss: <span style=" color: #7358A5;">0.378</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009142857142857143</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.855</span>, loss: <span style=" color: #7358A5;">0.404</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">5</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.836</span>, loss: <span style=" color: #7358A5;">0.333 </span>(data_loss: <span style=" color: #7358A5;">0.333</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009142439202779302</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.828</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.368</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.368<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009100837277029487</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.867</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.307</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.307<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009059612248595759</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.891</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.380</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.380<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009018759018759019</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.859</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.342</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.342<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.0008978272580355541</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.885</span>, loss: <span style=" color: #7358A5;">0.241 </span>(data_loss: <span style=" color: #7358A5;">0.241</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0008950948800572861</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.869</span>, loss: <span style=" color: #7358A5;">0.357 </span>(data_loss: <span style=" color: #7358A5;">0.357</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0008950948800572861</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.860</span>, loss: <span style=" color: #7358A5;">0.389</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The model trained successfully and achieved pretty good accuracy. This was done with a new, real, much more challenging dataset and in just 5 epochs instead of 10000. Training also went faster than with our previous attempts at spiral data, where we trained by fitting the whole dataset at once.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">So far, we’ve only mentioned how important it is to shuffle the training data and what might happen if we attempt to train on non-shuffled data. Now would be a good time to exemplify what happens when we don’t shuffle it. We can comment out the shuffling code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Shuffle the training dataset</p><p class="s17" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># keys = np.array(range(X.shape[0])) # np.random.shuffle(keys)</p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;"># X = X[keys] # y = y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Running again, we can see that we end on:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.000</span>, loss: <span style=" color: #7358A5;">2.302 </span>(data_loss: <span style=" color: #7358A5;">2.302</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.001</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">2.338</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2.338<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009950248756218907</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.401</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.401<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009900990099009901</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.214</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.214<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009852216748768474</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">2.278</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">2.278<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.000980392156862745</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.018<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.0009771350400625367</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.381</span>, loss: <span style=" color: #7358A5;">2.246 </span>(data_loss: <span style=" color: #7358A5;">2.246</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0009771350400625367</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.100</span>, loss: <span style=" color: #7358A5;">6.982</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.000</span>, loss: <span style=" color: #7358A5;">8.201 </span>(data_loss: <span style=" color: #7358A5;">8.201</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.0009770873027505008</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">4.577</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">4.577<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.000972337012008362</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.383</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.821</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.821<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009676326866321544</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.964</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.964<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009629736626703259</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.545</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.545<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009583592888974076</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.013</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.013<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.0009552466924583273</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.597</span>, loss: <span style=" color: #7358A5;">1.573 </span>(data_loss: <span style=" color: #7358A5;">1.573</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009552466924583273</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.109</span>, loss: <span style=" color: #7358A5;">3.917</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">3</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.000</span>, loss: <span style=" color: #7358A5;">3.431 </span>(data_loss: <span style=" color: #7358A5;">3.431</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009552010698251983</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">3.519</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">3.519<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009506607091928891</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.859</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.559</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.559<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009461633077869241</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.225</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.225<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009417082587814295</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.151</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.151<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.0009372949667260287</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.012</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.012<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.000934317481080071</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.638</span>, loss: <span style=" color: #7358A5;">1.478 </span>(data_loss: <span style=" color: #7358A5;">1.478</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.000934317481080071</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.134</span>, loss: <span style=" color: #7358A5;">3.108</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">4</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.031</span>, loss: <span style=" color: #7358A5;">2.620 </span>(data_loss: <span style=" color: #7358A5;">2.620</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009342738356612324</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">4.128</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">4.128<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009299297903008323</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.891</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.891<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009256259545517657</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.118</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.118<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009213617727000506</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.065</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.065<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009171366992250195</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.011</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.011<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.0009142857142857143</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.644</span>, loss: <span style=" color: #7358A5;">1.335 </span>(data_loss: <span style=" color: #7358A5;">1.335</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009142857142857143</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.189</span>, loss: <span style=" color: #7358A5;">3.050</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">5</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.016</span>, loss: <span style=" color: #7358A5;">2.734 </span>(data_loss: <span style=" color: #7358A5;">2.734</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0009142439202779302</p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">2.848</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">2.848<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009100837277029487</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.547</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">1.108</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">1.108<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009059612248595759</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.992</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.018</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.018<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0009018759018759019</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.065</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.065<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.0008978272580355541</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">1.000</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.010</p></td><td style="width:73pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.010<span style=" color: #231F20;">,</span></p></td><td style="width:61pt"><p class="s24" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:48pt"><p class="s26" style="padding-right: 3pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:24pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 8pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.0008950948800572861</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:61pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:48pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:24pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.744</span>, loss: <span style=" color: #7358A5;">0.961 </span>(data_loss: <span style=" color: #7358A5;">0.961</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.0008950948800572861</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.200</span>, loss: <span style=" color: #7358A5;">3.311</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">As we can see, this doesn’t work well at all. We can observe how the model approached a perfect accuracy of 1 during training, but epoch accuracy remained poor, and the validation accuracy proved that the model did not learn. Training accuracy quickly became high, since the model learned to predict just one label (as it repeatedly saw only that label). Once the label changed</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">in the training data, the model quickly learned to predict only that new label, as that’s all it saw in every batch. This process repeated to the end of an epoch and then over all epochs. Epoch accuracy is lower because it took a while for the model to learn the new label after a switch, and it showed a low accuracy during this period. Validation accuracy was calculated after training for a given epoch ended, and as we remember, the model learned to predict just one label. In the case of validation, the label that the model predicted was the last label it had seen — its accuracy was close to 1/10 as our training dataset consists of 10 classes.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Re-enable shuffling, and then you can tinker around with your model to see if you can further improve results. Here is an example with a larger model, a higher learning rate decay, and twice as many epochs:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10</span><span class="s10">, </span>batch_size<span class="s11">=</span><span class="s14">128</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.891</span>, loss: <span style=" color: #7358A5;">0.263 </span>(data_loss: <span style=" color: #7358A5;">0.263</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-bottom: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0001915341888527102</p><table style="border-collapse:collapse;margin-left:17.489pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.883</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.257</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.257<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00018793459875963167</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.922</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.227</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.227<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00018446781036709093</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.898</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.282</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.282<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00018112660749864155</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.914</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.299</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.299<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00017790428749332856</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.917</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.192</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.192<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.00017577781683951485</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.894</span>, loss: <span style=" color: #7358A5;">0.291 </span>(data_loss: <span style=" color: #7358A5;">0.291</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.00017577781683951485</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We improved accuracy and decreased loss a bit by simply increasing the model size, decay, and number of epochs.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark308">Full code up to now:</a><a name="bookmark325">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs </span>import <span style=" color: #231F20;">os</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">cv2 nnfs.init()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Save input values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If not in the training mode - return values <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">training:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs.copy() <span style=" color: #C71F43;">return</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Input &quot;layer&quot;</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Input</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 285pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>np.argmax(outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Save input and calculate/save output # of the sigmoid function</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.output </span>= <span style=" color: #7358A5;">1 </span>/ <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1 </span>+ <span style=" color: #231F20;">np.exp(</span>-<span style=" color: #231F20;">inputs))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative - calculates from output of the sigmoid function </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.output) <span style=" color: #C71F43;">* </span>self.output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>(outputs <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Linear activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Linear</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Just remember values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># derivative is 1, 1 * dvalues = dvalues - the chain rule </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights)</p><p class="s17" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;text-align: left;"># If there is no momentum array for weights</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span><span style=" color: #A5A4A5;"># Get corrected cache</span></p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate regularization loss # iterate all trainable layers</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: -42pt;line-height: 108%;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">* </span>\ layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set/remember trainable layers</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">remember_trainable_layers</span>(<span class="s23">self</span>, <span class="s23">trainable_layers</span>): self.trainable_layers <span style=" color: #C71F43;">= </span>trainable_layers</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculates the data and regularization losses # given model output and ground truth values</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of losses and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(sample_losses) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Binary cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_BinaryCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 108pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped)) sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 102pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Squared Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanSquaredError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L2 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean((y_true <span style=" color: #C71F43;">- </span>y_pred)<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">self.dinputs </span>= -<span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">(y_true </span>- <span style=" color: #231F20;">dvalues) </span>/ <span style=" color: #231F20;">outputs </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Absolute Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanAbsoluteError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L1 loss</span></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 28pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>): <span style=" color: #A5A4A5;"># Calculate loss</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean(np.abs(y_true <span style=" color: #C71F43;">- </span>y_pred), <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.sign(y_true <span style=" color: #C71F43;">- </span>dvalues) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common accuracy class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Accuracy</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates an accuracy</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># given predictions and ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get comparison results</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">comparisons <span style=" color: #C71F43;">= </span>self.compare(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate an accuracy </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of matching values and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(comparisons) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return accuracy <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate an accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for classification model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Categorical</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">binary</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>): <span style=" color: #A5A4A5;"># Binary mode?</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary <span style=" color: #C71F43;">= </span>binary</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># No initialization is needed</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>): <span style=" color: #C71F43;">pass</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if not <span style=" color: #231F20;">self.binary </span>and <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">: y </span>= <span style=" color: #231F20;">np.argmax(y, </span><span class="s23">axis</span>=<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">)</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">predictions </span>== <span style=" color: #231F20;">y</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for regression model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Regression</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create precision property <span style=" color: #231F20;">self.precision </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates precision value</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># based on passed-in ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>, <span class="s23">reinit</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.precision </span>is <span style=" color: #7358A5;">None </span>or <span style=" color: #231F20;">reinit: self.precision </span>= <span style=" color: #231F20;">np.std(y) </span>/ <span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">np.absolute(predictions </span>- <span style=" color: #231F20;">y) </span>&lt; <span style=" color: #231F20;">self.precision</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier&#39;s output object <span style=" color: #231F20;">self.softmax_classifier_output </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>, <span class="s23">accuracy</span>): self.loss <span style=" color: #C71F43;">= </span>loss</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.optimizer <span style=" color: #C71F43;">= </span>optimizer self.accuracy <span style=" color: #C71F43;">= </span>accuracy</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Initialize a list containing trainable layers: </span>self.trainable_layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss self.output_layer_activation <span style=" color: #C71F43;">= </span>self.layers[i]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update loss object with trainable layers <span style=" color: #231F20;">self.loss.remember_trainable_layers(</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.trainable_layers</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If output activation is Softmax and</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># loss function is Categorical Cross-Entropy # create an object of combined activation</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># and loss function containing # faster gradient calculation</p><p class="s10" style="padding-left: 86pt;text-indent: -18pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">isinstance</span>(self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>], Activation_Softmax) <span style=" color: #C71F43;">and </span>\ <span style=" color: #32A7BD;">isinstance</span>(self.loss, Loss_CategoricalCrossentropy): <span style=" color: #A5A4A5;"># Create an object of combined activation</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and loss functions </span>self.softmax_classifier_output <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">train_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If there is validation data passed,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># set default number of steps for validation as well </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">validation_steps <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For better readability </span>X_val, y_val <span style=" color: #C71F43;">= </span>validation_data</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">train_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">train_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): train_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: validation_steps </span>= <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val) </span>// <span style=" color: #231F20;">batch_size</span></p><p class="s17" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data but nor full batch, this won&#39;t include it</p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Add `1` to include this not full batch</p><p class="s11" style="padding-top: 1pt;padding-left: 141pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val): validation_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print epoch number </span><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">self.loss.new_pass()</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.accuracy.new_pass()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(train_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X batch_y <span style=" color: #C71F43;">= </span>y</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size] batch_y <span style=" color: #C71F43;">= </span>y[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, batch_y,</p><p class="s11" style="padding-left: 116pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 225pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions,</p><p class="s10" style="padding-top: 1pt;padding-left: 328pt;text-indent: 0pt;text-align: left;">batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, batch_y)</span></p><p class="s10" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 116pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>step <span style=" color: #C71F43;">% </span>print_every <span style=" color: #C71F43;">or </span>step <span style=" color: #C71F43;">== </span>train_steps <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;step: </span>{step}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print epoch loss and accuracy </span>epoch_data_loss, epoch_regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="text-indent: 0pt;text-align: right;">self.loss.calculate_accumulated(</p><p class="s23" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">include_regularization<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">epoch_loss <span style=" color: #C71F43;">= </span>epoch_data_loss <span style=" color: #C71F43;">+ </span>epoch_regularization_loss epoch_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span><i>f</i><span style=" color: #8F8633;">&#39;training, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{epoch_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{epoch_loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{epoch_data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{epoch_regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss # and accuracy objects <span style=" color: #231F20;">self.loss.new_pass() self.accuracy.new_pass()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(validation_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val batch_y <span style=" color: #C71F43;">= </span>y_val</p><p class="s17" style="padding-top: 2pt;padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;text-align: left;">]</p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_y <span style=" color: #C71F43;">= </span>y_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="text-indent: 0pt;text-align: center;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate the loss <span style=" color: #231F20;">self.loss.calculate(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 141pt;text-indent: 108pt;line-height: 108%;text-align: left;">output) self.accuracy.calculate(predictions, batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print validation loss and accuracy </span>validation_loss <span style=" color: #C71F43;">= </span>self.loss.calculate_accumulated() validation_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{validation_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{validation_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">training</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X, training)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output, training)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If softmax classifier</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.softmax_classifier_output </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: </span><span style=" color: #A5A4A5;"># First call backward method</span></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># on the combined activation/loss # this will set dinputs property</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.backward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Since we&#39;ll not call backward method of the last layer # which is Softmax activation</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># as we used combined activation/loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># object, let&#39;s set dinputs in this object </span>self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>].dinputs <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.dinputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call backward method going through # all the objects but last</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers[:</span>-<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">]):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 216%;text-align: left;">layer.backward(layer.next.dinputs) <span style=" color: #C71F43;">return</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># First call backward method on the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># this will set dinputs property that the last # layer will try to access shortly <span style=" color: #231F20;">self.loss.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Call backward method going through all the objects # in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.backward(layer.next.dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Loads a MNIST dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load_mnist_dataset</span>(<span class="s23">dataset</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Scan all the directories and create a list of labels </span>labels <span style=" color: #C71F43;">= </span>os.listdir(os.path.join(path, dataset))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create lists for samples and labels </span>X <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>[]</p><p class="s11" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each label folder </span>for <span style=" color: #231F20;">label </span>in <span style=" color: #231F20;">labels:</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># And for each image in given folder</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">for <span style=" color: #231F20;">file </span>in <span style=" color: #231F20;">os.listdir(os.path.join(path, dataset, label)): </span><span style=" color: #A5A4A5;"># Read the image</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">image <span style=" color: #C71F43;">= </span>cv2.imread(</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And append it and a label to the lists <span style=" color: #231F20;">X.append(image)</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y.append(label)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Convert the data to proper numpy arrays and return </span><span style=" color: #C71F43;">return </span>np.array(X), np.array(y).astype(<span style=" color: #8F8633;">&#39;uint8&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># MNIST dataset (train + test)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">create_data_mnist</span>(<span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Load both sets separately</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;train&#39;</span>, path)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;test&#39;</span>, path)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And return all the data <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">X, y, X_test, y_test</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10</span><span class="s10">, </span>batch_size<span class="s11">=</span><span class="s14">128</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch19" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch19<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark326">Chapter 20</a><a name="bookmark327">&zwnj;</a><a name="bookmark328">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Model Evaluation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In Chapter 11, Testing or Out-of-Sample Data, we covered the differences between validation and testing data. With our model up to this point, we’ve validated during training, but currently have no great way to run a test on data or perform a prediction. To begin, we’re going to add a new <span class="s35">evaluate </span>method to the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Evaluates the model using passed-in dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">evaluate</span>(<span class="s23">self</span>, <span class="s23">X_val</span>, <span class="s23">y_val</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">This method takes in samples (<span class="s29">X_val</span>), target outputs (<span class="s29">y_val</span>), and an optional batch size. First, we calculate the number of steps given the length of the data and the <span class="s29">batch_size </span>argument. This is the same as in the <span class="s35">train </span>method:</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">validation_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">validation_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X_val) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Dividing rounds down. If there are some remaining # data, but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: justify;">if <span style=" color: #231F20;">validation_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val): validation_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then, we’re going to move a chunk of code from the <span class="s35">Model </span>class’ <span class="s35">train </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 110pt;text-indent: -66pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p class="s14" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s14" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">...</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss # and accuracy objects <span style=" color: #231F20;">self.loss.new_pass() self.accuracy.new_pass()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(validation_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val batch_y <span style=" color: #C71F43;">= </span>y_val</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 165pt;text-indent: 0pt;text-align: left;">]</p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_y <span style=" color: #C71F43;">= </span>y_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="text-indent: 0pt;text-align: center;">]</p><p class="s17" style="padding-top: 2pt;padding-left: 140pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate the loss <span style=" color: #231F20;">self.loss.calculate(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 141pt;text-indent: 108pt;line-height: 108%;text-align: left;">output) self.accuracy.calculate(predictions, batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print validation loss and accuracy </span>validation_loss <span style=" color: #C71F43;">= </span>self.loss.calculate_accumulated() validation_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{validation_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 153pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{validation_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll move that code, along with the code parts for the number of steps calculation and resetting accumulated loss and accuracy, to the <span class="s35">evaluate </span>method, making it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Evaluates the model using passed-in dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">evaluate</span>(<span class="s23">self</span>, <span class="s23">X_val</span>, <span class="s23">y_val</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">validation_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">validation_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X_val) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Dividing rounds down. If there are some remaining # data, but not a full batch, this won&#39;t include it # Add `1` to include this not full minibatch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: justify;">if <span style=" color: #231F20;">validation_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val): validation_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss # and accuracy objects <span style=" color: #231F20;">self.loss.new_pass() self.accuracy.new_pass()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: justify;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(validation_steps):</span></p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val batch_y <span style=" color: #C71F43;">= </span>y_val</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">]</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_y <span style=" color: #C71F43;">= </span>y_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate the loss <span style=" color: #231F20;">self.loss.calculate(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">output) self.accuracy.calculate(predictions, batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print validation loss and accuracy </span>validation_loss <span style=" color: #C71F43;">= </span>self.loss.calculate_accumulated() validation_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{validation_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{validation_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now, where that block of code once was in the <span class="s35">Model </span>class’ <span class="s35">train </span>method, we can call the new</p><p class="s35" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">evaluate <span class="p">method:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 110pt;text-indent: -66pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p class="s14" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s14" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">...</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Evaluate the model: </span>self.evaluate(<span style=" color: #C71F43;">*</span>validation_data,</p><p class="s23" style="padding-top: 1pt;padding-left: 201pt;text-indent: 0pt;text-align: left;">batch_size<span class="s11">=</span><span class="s10">batch_size)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">If you’re confused about the <span class="s15">*</span><span class="s22">validation_data </span>part, the asterisk, called the <b>starred expression</b>, unpacks the <span class="s22">validation_data </span>list into singular values. For a simple example of how this works:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">a <span style=" color: #C71F43;">= </span>(<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">test</span>(<span class="s23">n1</span>, <span class="s23">n2</span>): <span style=" color: #32A7BD;">print</span>(n1, n2)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">test(<span style=" color: #C71F43;">*</span>a)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">&gt;&gt;&gt; <span style=" color: #7358A5;">1 2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: justify;">Now that we have this separate <span class="s35">evaluate </span>method, we can evaluate the model whenever we please — either during training or on-demand, by passing the validation or testing data. First, we’ll create and train a model as usual:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 80pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10</span><span class="s10">, </span>batch_size<span class="s11">=</span><span class="s14">128</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can then add code to evaluate. Right now, we don’t have any specific testing data besides what we’ve used for validation data, but we can use this, for now, to test this method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">model.evaluate(X_test, y_test)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Running this, we get:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s14" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">...</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">epoch: <span style=" color: #7358A5;">10</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">step: <span style=" color: #7358A5;">0</span>, acc: <span style=" color: #7358A5;">0.891</span>, loss: <span style=" color: #7358A5;">0.263 </span>(data_loss: <span style=" color: #7358A5;">0.263</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.0001915341888527102</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.5pt" cellspacing="0"><tr style="height:13pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">step: <span style=" color: #7358A5;">100</span>, acc: <span style=" color: #7358A5;">0.883</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.257</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.257<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00018793459875963167</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">200</span>, acc: <span style=" color: #7358A5;">0.922</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.227</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.227<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00018446781036709093</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">300</span>, acc: <span style=" color: #7358A5;">0.898</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.282</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.282<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00018112660749864155</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">400</span>, acc: <span style=" color: #7358A5;">0.914</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.299</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.299<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00017790428749332856</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:139pt"><p class="s24" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">step: <span style=" color: #7358A5;">468</span>, acc: <span style=" color: #7358A5;">0.917</span>,</p></td><td style="width:36pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">loss:</p></td><td style="width:36pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.192</p></td><td style="width:73pt"><p class="s24" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">(data_loss:</p></td><td style="width:42pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.192<span style=" color: #231F20;">,</span></p></td><td style="width:60pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">reg_loss:</p></td><td style="width:49pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.000<span style=" color: #231F20;">),</span></p></td><td style="width:23pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">lr:</p></td></tr><tr style="height:13pt"><td style="width:139pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.00017577781683951485</p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:36pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:73pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:42pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:60pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:49pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:23pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.894</span>, loss: <span style=" color: #7358A5;">0.291 </span>(data_loss: <span style=" color: #7358A5;">0.291</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">0.00017577781683951485</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The validation accuracy and loss are repeated twice and show the same values at the end since we’re validating during the training and evaluating right after on the same data. You’ll often train a model, tweak its hyperparameters, train it all over again, and so on, using training and validation data passed into the training method. Then, whenever you find the model and hyperparameters that appear to perform the best, you’ll use that model on testing data and, in the future, to make predictions in production.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we can also run evaluation on the training data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">model.evaluate(X, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Running this prints:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.895</span>, loss: <span style=" color: #7358A5;">0.285</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">“Validation” here means that we evaluated the model, but we have done this using the training data. We compare that to the result of training on this data which we have just performed:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">training, acc: <span style=" color: #7358A5;">0.894</span>, loss: <span style=" color: #7358A5;">0.291 </span>(data_loss: <span style=" color: #7358A5;">0.291</span>, reg_loss: <span style=" color: #7358A5;">0.000</span>), lr:</p><p class="s14" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">0.00017577781683951485</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">You may notice that, despite using the same dataset, there is some difference between accuracy and loss values. This difference comes from the fact that the model prints accuracy and loss accumulated during the epoch, while the model was still learning; thus, mean accuracy and loss differ from the evaluation on the training data that has been run after the last epoch of training. <b>Running evaluation on the training data at the end of the training process will return the final accuracy and loss.</b></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the next chapter, we will add the ability to save and load our models; we’ll also construct a way to retrieve and set a model’s parameters.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch20" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch20<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark329">Chapter 21</a><a name="bookmark330">&zwnj;</a><a name="bookmark336">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;line-height: 130%;text-align: left;">Saving and Loading Models and Their Parameters</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Retrieving Parameters</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">There are situations where we’d like to take a closer look into model parameters to see if we have dead or exploding neurons. To retrieve these parameters, we will iterate over the trainable layers, take their parameters, and put them into a list. The only trainable layer type that we have here is the <i>Dense </i>layer. Let’s add a method to the <span class="s35">Layer_Dense </span>class to retrieve parameters:</p><p class="s80" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dense layer <span style=" color: #32A7BD;">class </span><span class="s32">Layer_Dense</span><span class="s10">:</span></p><p class="s14" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Retrieve layer parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">get_parameters</span>(<span class="s23">self</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.weights, self.biases</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">Within the <span class="s35">Model </span>class, we’ll add <span class="s35">get_parameters </span>method, which will iterate over the trainable layers of the model, run their <span class="s35">get_parameters </span>method, and append returned weights and biases to a list:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s80" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Model class <span style=" color: #32A7BD;">class </span><span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Retrieves and returns parameters of trainable layers</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">get_parameters</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list for parameters </span>parameters <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Iterable trainable layers and get their parameters </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">parameters.append(layer.get_parameters())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return a list <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">parameters</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 7pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Now, after training a model, we can grab the parameters by running:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">parameters <span style=" color: #C71F43;">= </span>model.get_parameters()</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">For example:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10</span><span class="s10">, </span>batch_size<span class="s11">=</span><span class="s14">128</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Retrieve and print parameters </span>parameters <span style=" color: #C71F43;">= </span>model.get_parameters() <span style=" color: #32A7BD;">print</span>(parameters)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">This will look <i>something </i>like (we trim the output to save space):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">[(array([[ <span style=" color: #7358A5;">0.03538642</span>, <span style=" color: #7358A5;">0.00794717</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.04143231</span>, <span style=" color: #7358A5;">...</span>, <span style=" color: #7358A5;">0.04267325</span>,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:59.696pt" cellspacing="0"><tr style="height:13pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">-<span style=" color: #7358A5;">0.00935107</span><span style=" color: #231F20;">,</span></p></td><td style="width:157pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 11pt;text-align: left;">0.01872394<span style=" color: #231F20;">],</span></p></td><td style="width:109pt" colspan="2"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[ <span style=" color: #7358A5;">0.03289384</span>,</p></td><td style="width:157pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.00691249<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.03424096<span style=" color: #231F20;">,</span></p></td><td style="width:34pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:75pt"><p class="s26" style="padding-left: 4pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.02362755<span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.00903602</span><span style=" color: #231F20;">,</span></p></td><td style="width:157pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00977725<span style=" color: #231F20;">],</span></p></td><td style="width:34pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:75pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[ <span style=" color: #7358A5;">0.02189022</span>,</p></td><td style="width:157pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.01362374</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">0.01442819</span><span style=" color: #231F20;">,</span></p></td><td style="width:34pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:75pt"><p class="s26" style="padding-left: 4pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.01320345<span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.02083327</span><span style=" color: #231F20;">,</span></p></td><td style="width:157pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.02499157<span style=" color: #231F20;">],</span></p></td><td style="width:34pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:75pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:157pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:34pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:75pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[ <span style=" color: #7358A5;">0.0146937 </span>,</p></td><td style="width:157pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.02869027</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">0.02198809</span><span style=" color: #231F20;">,</span></p></td><td style="width:34pt"><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:75pt"><p class="s26" style="padding-left: 4pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.01459295<span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.02335824</span><span style=" color: #231F20;">,</span></p></td><td style="width:157pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00935643<span style=" color: #231F20;">],</span></p></td><td style="width:34pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:75pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.00090149</span>,</p></td><td style="width:157pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.01082182<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.06013806<span style=" color: #231F20;">,</span></p></td><td style="width:34pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:75pt"><p class="s26" style="padding-left: 4pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">0.00704454<span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.0039093 </span><span style=" color: #231F20;">,</span></p></td><td style="width:157pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.00311571<span style=" color: #231F20;">],</span></p></td><td style="width:34pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:75pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">[ <span style=" color: #7358A5;">0.03660082</span>,</p></td><td style="width:157pt"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">0.00809607</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">0.02737131</span><span style=" color: #231F20;">,</span></p></td><td style="width:34pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:75pt"><p class="s26" style="padding-left: 4pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">0.02216582<span style=" color: #231F20;">,</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.01710589</span>, <span style=" color: #7358A5;">0.01578414</span>]], <span class="s23">dtype</span><span style=" color: #C71F43;">=</span>float32), array([[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">2.24505737e-02</span>, <span style=" color: #7358A5;">5.40090213e-03</span>, <span style=" color: #7358A5;">2.91307438e-02</span>,</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">1.04323691e-02</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">9.52822249e-03</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">1.48109728e-02</span><span style=" color: #231F20;">,</span></p><p class="s14" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">...<span style=" color: #231F20;">,</span></p><p class="s14" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;">0.04158591<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.01614098<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.0134403 <span style=" color: #231F20;">, </span>0.00708392<span style=" color: #231F20;">, </span>0.0284729 <span style=" color: #231F20;">,</span></p><p class="s10" style="padding-top: 1pt;padding-left: 74pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">0.00336277</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.00085383</span>, <span style=" color: #7358A5;">0.00163819</span>]], <span class="s23">dtype</span><span style=" color: #C71F43;">=</span>float32)),</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">(array([[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.00196577</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.00335329</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.01362851</span>, <span style=" color: #7358A5;">...</span>, <span style=" color: #7358A5;">0.00397028</span>,</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:59.454pt" cellspacing="0"><tr style="height:13pt"><td style="width:84pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.00027816<span style=" color: #231F20;">,</span></p></td><td style="width:85pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.00427755<span style=" color: #231F20;">],</span></p></td><td style="width:181pt" colspan="3"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[ <span style=" color: #7358A5;">0.04438829</span>,</p></td><td style="width:85pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.09197803</span><span style=" color: #231F20;">,</span></p></td><td style="width:72pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.02897452<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-left: 2pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">...<span style=" color: #231F20;">,</span></p></td><td style="width:78pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.11920264</span><span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.03808296<span style=" color: #231F20;">,</span></p></td><td style="width:85pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.00536136</span><span style=" color: #231F20;">],</span></p></td><td style="width:72pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[ <span style=" color: #7358A5;">0.04146343</span>,</p></td><td style="width:85pt"><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.03637529</span><span style=" color: #231F20;">,</span></p></td><td style="width:72pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.04973305<span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">...<span style=" color: #231F20;">,</span></p></td><td style="width:78pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.13564698</span><span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.08259197</span><span style=" color: #231F20;">,</span></p></td><td style="width:85pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.02467288</span><span style=" color: #231F20;">],</span></p></td><td style="width:72pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s26" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">...<span style=" color: #231F20;">,</span></p></td><td style="width:85pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:72pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">[ <span style=" color: #7358A5;">0.03495856</span>,</p></td><td style="width:85pt"><p class="s26" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;">0.03902597<span style=" color: #231F20;">,</span></p></td><td style="width:72pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">0.0028984 <span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: center;">...<span style=" color: #231F20;">,</span></p></td><td style="width:78pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.10016892</span><span style=" color: #231F20;">,</span></p></td></tr><tr style="height:13pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">-<span style=" color: #7358A5;">0.11356542</span><span style=" color: #231F20;">,</span></p></td><td style="width:85pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.05866433<span style=" color: #231F20;">],</span></p></td><td style="width:72pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:16pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.00857899</span>,</p></td><td style="width:157pt" colspan="2"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;text-align: left;">-<span style=" color: #7358A5;">0.02612676</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">0.01050871</span><span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-left: 2pt;padding-right: 2pt;text-indent: 0pt;text-align: center;">...<span style=" color: #231F20;">,</span></p></td><td style="width:78pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;text-align: right;">-<span style=" color: #7358A5;">0.00551328</span><span style=" color: #231F20;">,</span></p></td></tr><tr style="height:14pt"><td style="width:84pt"><p class="s25" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">-<span style=" color: #7358A5;">0.01432311</span><span style=" color: #231F20;">,</span></p></td><td style="width:157pt" colspan="2"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 12pt;text-align: left;">-<span style=" color: #7358A5;">0.00916382</span><span style=" color: #231F20;">],</span></p></td><td style="width:31pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td><td style="width:78pt"><p style="text-indent: 0pt;text-align: left;"><br/></p></td></tr><tr style="height:13pt"><td style="width:84pt"><p class="s24" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.20444085</span>,</p></td><td style="width:157pt" colspan="2"><p class="s25" style="padding-left: 2pt;text-indent: 0pt;line-height: 11pt;text-align: left;">-<span style=" color: #7358A5;">0.01483698</span><span style=" color: #231F20;">, </span>-<span style=" color: #7358A5;">0.09321352</span><span style=" color: #231F20;">,</span></p></td><td style="width:31pt"><p class="s26" style="padding-left: 1pt;padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: center;">...<span style=" color: #231F20;">,</span></p></td><td style="width:78pt"><p class="s26" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">0.02114356<span style=" color: #231F20;">,</span></p></td></tr></table><p class="s10" style="padding-left: 67pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.0762504 </span>, <span style=" color: #7358A5;">0.03600615</span>]], <span class="s23">dtype</span><span style=" color: #C71F43;">=</span>float32), array([[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">0.0103433 </span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.00158314<span style=" color: #231F20;">, </span>0.02268587<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.02352985<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.02144126<span style=" color: #231F20;">,</span></p><p class="s14" style="padding-top: 1pt;padding-left: 67pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">-</span>0.00777614<span style=" color: #231F20;">, </span>0.00795028<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.00622872<span style=" color: #231F20;">, </span>0.06918745<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>0.00743477<span style=" color: #231F20;">]],</span></p><p class="s23" style="padding-top: 1pt;padding-left: 55pt;text-indent: 0pt;text-align: left;">dtype<span class="s11">=</span><span class="s10">float32))]</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark331">Setting Parameters</a><a name="bookmark337">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">If we have a method to get parameters, we will likely also want to have a method that will set parameters. We’ll do this similar to how we setup the <span class="s35">get_parameters </span>method, starting with the <span class="s35">Layer_Dense </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set weights and biases in a layer instance</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set_parameters</span>(<span class="s23">self</span>, <span class="s23">weights</span>, <span class="s23">biases</span>): self.weights <span style=" color: #C71F43;">= </span>weights</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.biases <span style=" color: #C71F43;">= </span>biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we can update the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Updates the model with new parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set_parameters</span>(<span class="s23">self</span>, <span class="s23">parameters</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate over the parameters and layers</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and update each layers with each set of the parameters </span>for <span style=" color: #231F20;">parameter_set, layer </span>in <span style=" color: #32A7BD;">zip</span><span style=" color: #231F20;">(parameters,</span></p><p class="s10" style="padding-left: 92pt;text-indent: 169pt;line-height: 108%;text-align: left;">self.trainable_layers): layer.set_parameters(<span style=" color: #C71F43;">*</span>parameter_set)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">We are also iterating over the trainable layers here, but what we are doing next needs a bit more explanation. First, the <span class="s43">zip</span><span class="s22">() </span>function takes in iterables, like lists, and returns a new iterable with pairwise combinations of all the iterables passed in as parameters. In other words (and using our example), <span class="s43">zip</span><span class="s22">() </span>takes a list of parameters and a list of layers and returns an iterator containing tuples of 0th elements of both lists, then the 1st elements of both lists, the 2nd elements from both lists, and so on. This way, we can iterate over parameters and the layer they belong to at the same time. As our parameters are a tuple of weights and biases, we will unpack them with a starred expression so that our <span class="s35">Layer_Dense </span>method can take them as separate parameters. This approach gives us flexibility if we’d like to use layers with different numbers of parameter groups.</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">One difference that presents itself now is that this allows us to have a model that never needed an optimizer. If we don’t train a model but, instead, load already trained parameters into it, we won’t optimize anything. To account for this, we’ll visit the <span class="s35">finalize </span>method of the <span class="s35">Model </span>class, changing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p class="s14" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Update loss object with trainable layers <span style=" color: #231F20;">self.loss.remember_trainable_layers(</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.trainable_layers</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">To (we added an <span class="s15">if </span>statement to set a list of trainable layers to the loss function, only if this loss object exists):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p class="s14" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">...</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update loss object with trainable layers </span>if <span style=" color: #231F20;">self.loss </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.loss.remember_trainable_layers( self.trainable_layers</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">Next, we’ll change the <span class="s35">Model </span>class’ <span class="s35">set </span>method to allow us to pass in only given parameters. We’ll assign default values and add <span class="s15">if </span>statements to use parameters only when they’re present. To do that, we’ll change:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span>, <span class="s23">optimizer</span>, <span class="s23">accuracy</span>): self.loss <span style=" color: #C71F43;">= </span>loss</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.optimizer <span style=" color: #C71F43;">= </span>optimizer self.accuracy <span style=" color: #C71F43;">= </span>accuracy</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">To:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-top: 3pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">optimizer</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">accuracy</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">loss </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: self.loss </span>= <span style=" color: #231F20;">loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">optimizer </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: self.optimizer </span>= <span style=" color: #231F20;">optimizer</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 118%;text-align: left;">if <span style=" color: #231F20;">accuracy </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: self.accuracy </span>= <span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We can now train a model, retrieve its parameters, create a new model, and set its parameters with those retrieved from the previously-trained model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set loss, optimizer and accuracy objects <span style=" color: #231F20;">model.set(</span></p><p class="s23" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>optimizer<span class="s11">=</span><span class="s10">Optimizer_Adam(</span>decay<span class="s11">=</span><span class="s14">1e-3</span><span class="s10">), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model.train(X, y, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span>(X_test, y_test),</p><p class="s23" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">epochs<span class="s11">=</span><span class="s14">10</span><span class="s10">, </span>batch_size<span class="s11">=</span><span class="s14">128</span><span class="s10">, </span>print_every<span class="s11">=</span><span class="s14">100</span><span class="s10">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Retrieve model parameters </span>parameters <span style=" color: #C71F43;">= </span>model.get_parameters()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># New model</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Set loss and accuracy objects</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We do not set optimizer object this time - there&#39;s no need to do it # as we won&#39;t train the model</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">model.set(</p><p class="s23" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Set model with parameters instead of training it <span style=" color: #231F20;">model.set_parameters(parameters)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Evaluate the model <span style=" color: #231F20;">model.evaluate(X_test, y_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">(model training output removed) validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark332">Saving Parameters</a><a name="bookmark338">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">We’ll extend this further now by actually saving the parameters into a file. To do this, we’ll add a <span class="s35">save_parameters </span>method in the <span class="s35">Model </span>class. We’ll use Python’s built-in <i>pickle </i>module to serialize any Python object. Serialization is a process of turning an object, which can be of any abstract form, into a binary representation <span style=" color: #212121;">— </span>a set of bytes that can be, for example, saved into a</p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: justify;">file. This serialized form contains all the information needed to recreate the object later. <i>Pickle </i>can either return the bytes of the serialized object or save them directly to a file. We’ll make use of the latter ability, so let’s import <i>pickle</i>:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">pickle</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">Then we’ll add a new method to the <span class="s35">Model </span>class. Before having <i>pickle </i>save our parameters into a file, we’ll need to create a file-handler by opening a file in binary-write mode. We will then pass this handler along to the data into <span class="s22">pickle.dump()</span>. To create the file, we need a filename that we’ll save the data into; we’ll pass it in as a parameter:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Saves the parameters to a file</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">save_parameters</span>(<span class="s23">self</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Open a file in the binary-write mode # and save parameters to it</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 118%;text-align: left;"><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;wb&#39;</span>) <span style=" color: #C71F43;">as </span>f: pickle.dump(self.get_parameters(), f)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">With this method, you can save the parameters of a trained model by running:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">model.save_parameters(<span style=" color: #8F8633;">&#39;fashion_mnist.parms&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a name="bookmark333">Loading Parameters</a><a name="bookmark339">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">Presumably, if we are saving model parameters into a file, we would also like to have a way to load them from this file. Loading parameters is very similar to saving the parameters, just reversed. We’ll open the file in a binary-read mode and have <i>pickle </i>read from it, deserializing</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: justify;">parameters back into a list. Then we call the <span class="s22">set_parameters </span>method that we created earlier and pass in the loaded parameters:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Loads the weights and updates a model instance with them</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load_parameters</span>(<span class="s23">self</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Open file in the binary-read mode,</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># load weights and update trainable layers </span><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;rb&#39;</span>) <span style=" color: #C71F43;">as </span>f:</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">self.set_parameters(pickle.load(f))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We set up a model, load in the parameters file (we did not train this model), and test the model to check if it works:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 80pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Instantiate the model </span>model <span style=" color: #C71F43;">= </span>Model()</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add layers </span>model.add(Layer_Dense(X.shape[<span style=" color: #7358A5;">1</span>], <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">128</span>)) model.add(Activation_ReLU()) model.add(Layer_Dense(<span style=" color: #7358A5;">128</span>, <span style=" color: #7358A5;">10</span>)) model.add(Activation_Softmax())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Set loss and accuracy objects</p><p class="s17" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># We do not set optimizer object this time - there&#39;s no need to do it # as we won&#39;t train the model</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">model.set(</p><p class="s23" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;">loss<span class="s11">=</span><span class="s10">Loss_CategoricalCrossentropy(), </span>accuracy<span class="s11">=</span><span class="s10">Accuracy_Categorical()</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Finalize the model <span style=" color: #231F20;">model.finalize()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set model with parameters instead of training it </span>model.load_parameters(<span style=" color: #8F8633;">&#39;fashion_mnist.parms&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Evaluate the model <span style=" color: #231F20;">model.evaluate(X_test, y_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While we can save and load model parameter values, we still need to define the model. It must be the exact configuration as the model that we’re importing parameters from. It would be easier if we could save the model itself.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark334">Saving the Model</a><a name="bookmark340">&zwnj;</a><a name="bookmark341">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 115%;text-align: left;">Why didn’t we save the whole model in the first place? Saving just weights versus saving the whole model has different use cases along with pros and cons. With saved weights, you can, for example, initialize a model with those weights, trained from similar data, and then train that model to work with your specific data. This is called <b>transfer learning </b>and is outside of the scope of this book. Weights can be used to visualize the model (like in some animations that we have created for the purpose of this book, starting from chapter 6), identify dead neurons, implement more complicated models (like <b>reinforcement learning</b>, where weights collected from multiple models are committed to a single network), and so on. A file containing just weights is also much smaller than an entire model. A model initialized from weights loads faster and uses less memory, as the optimizer and related parts are not created. One downside of loading just weights and biases is that the initialized model does not contain the optimizer’s state. It is possible to train the model further, but it’s more optimal to load a full model if we intend to train it. When saving the full model, everything related to it is saved as well; this includes the optimizer’s state (that allows us to easily continue the training) and model’s structure.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">We’ll create another method in the <span class="s35">Model </span>class that we’ll use to save the entire model. The first thing we’ll do is make a copy of the model since we’re going to edit it before saving, and we may also want to save a model during the training process as a <b>checkpoint</b>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s81" style="padding-left: 34pt;text-indent: 0pt;text-align: left;"># Saves the model</p><p class="s22" style="padding-top: 1pt;padding-left: 34pt;text-indent: 0pt;text-align: left;"><span class="s64">def </span><span style=" color: #427F3C;">save</span>(<span class="s29">self</span>, <span class="s29">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 60pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Make a deep copy of current model instance </span>model <span style=" color: #C71F43;">= </span>copy.deepcopy(self)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">We import the <i>copy </i>module to support this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">copy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The <i>copy </i>module offers two methods that allow us to copy the model — <i>copy </i>and <i>deepcopy</i>. While <i>copy </i>is faster, it only copies the first level of the object’s properties, causing copies of our model objects to have some references common to the original model. For example, our model object has a list of layers — the list is the top-level property, and the layers themselves</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">are secondary — therefore, references to the layer objects will be shared by both the original and copied model objects. Due to these challenges with <i>copy</i>, we’ll use the <i>deepcopy </i>method to recursively traverse all objects and create a full copy.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we’ll remove the accumulated loss and accuracy:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">model.loss.new_pass()</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">model.accuracy.new_pass()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then remove any data in the input layer, and reset the gradients, if any exist:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Remove data from input layer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 112%;text-align: left;"><span style=" color: #A5A4A5;"># and gradients from the loss object </span>model.input_layer.<u> </u>dict<u> </u>.pop(<span style=" color: #8F8633;">&#39;output&#39;</span>, <span style=" color: #7358A5;">None</span>) model.loss.<u> </u>dict<u> </u>.pop(<span style=" color: #8F8633;">&#39;dinputs&#39;</span>, <span style=" color: #7358A5;">None</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">Both <span class="s22">model.input_layer </span>and <span class="s22">model.loss </span>are class instances. They’re attributes of the <span class="s35">Model </span>object but also objects themselves. One of the dunder properties (called “dunder” because of the double underscores) that exists for all classes is the<u> </u><span class="s22">dict </span>property. It contains names and values for the class object’s properties. We can then use the built-in <span class="s22">pop </span>method on these values, which means we remove them from that instance of the class’ object. The <span class="s22">pop </span>method will wind up throwing an error if the key we pass as the first parameter doesn’t exist, as the <span class="s22">pop </span>method wants to return the value of the key that it removes. We use the second parameter of the <span class="s22">pop </span>method <span style=" color: #212121;">— </span>which is the default value that we want to return if the key doesn’t exist <span style=" color: #212121;">— </span>to prevent these errors. We will set this parameter to <i>None </i>— we do not intend to catch the removed values, and it doesn’t really matter what the default value is. This way, we do not have to check if a given property exists, in times like when we’d like to delete it using the <i>del </i>statement, and some of them might not exist.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we’ll iterate over all the layers to remove their properties:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each layer remove inputs, output and dinputs properties </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">model.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span><span style=" color: #32A7BD;">property </span><span style=" color: #C71F43;">in </span>[<span style=" color: #8F8633;">&#39;inputs&#39;</span>, <span style=" color: #8F8633;">&#39;output&#39;</span>, <span style=" color: #8F8633;">&#39;dinputs&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 78pt;line-height: 108%;text-align: left;"><span style=" color: #8F8633;">&#39;dweights&#39;</span>, <span style=" color: #8F8633;">&#39;dbiases&#39;</span>]: layer.<u> </u>dict<u> </u>.pop(<span style=" color: #32A7BD;">property</span>, <span style=" color: #7358A5;">None</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">With these things cleaned up, we can save the model object. To do that, we have to open a file in a binary-write mode, and call <span class="s22">pickle.dump() </span>with the model object and the file handler as parameters:</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 12pt;text-align: left;"># Open a file in the binary-write mode and save the model</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;wb&#39;</span>) <span style=" color: #C71F43;">as </span>f: pickle.dump(model, f)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">This makes the full <span class="s35">save </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Saves the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">save</span>(<span class="s23">self</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Make a deep copy of current model instance </span>model <span style=" color: #C71F43;">= </span>copy.deepcopy(self)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">model.loss.new_pass()</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">model.accuracy.new_pass()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Remove data from the input layer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and gradients from the loss object </span>model.input_layer.<u> </u>dict<u> </u>.pop(<span style=" color: #8F8633;">&#39;output&#39;</span>, <span style=" color: #7358A5;">None</span>) model.loss.<u> </u>dict<u> </u>.pop(<span style=" color: #8F8633;">&#39;dinputs&#39;</span>, <span style=" color: #7358A5;">None</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each layer remove inputs, output and dinputs properties </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">model.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span><span style=" color: #32A7BD;">property </span><span style=" color: #C71F43;">in </span>[<span style=" color: #8F8633;">&#39;inputs&#39;</span>, <span style=" color: #8F8633;">&#39;output&#39;</span>, <span style=" color: #8F8633;">&#39;dinputs&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 78pt;line-height: 108%;text-align: left;"><span style=" color: #8F8633;">&#39;dweights&#39;</span>, <span style=" color: #8F8633;">&#39;dbiases&#39;</span>]: layer.<u> </u>dict<u> </u>.pop(<span style=" color: #32A7BD;">property</span>, <span style=" color: #7358A5;">None</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Open a file in the binary-write mode and save the model </span><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;wb&#39;</span>) <span style=" color: #C71F43;">as </span>f:</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">pickle.dump(model, f)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">This means we can train a model, then save it whenever we wish with:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s22" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">model.save(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark335">Loading the Model</a><a name="bookmark342">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Loading a model will ideally take place before a model object even exists. What we mean by this is we could load a model by calling a method of the <span class="s35">Model </span>class instead of the object:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;line-height: 113%;text-align: left;">To achieve this, we’re going to use the <span class="s22">@</span><span class="s43">staticmethod </span>decorator. This decorator can be used with class methods to run them on uninitialized objects, where the <span class="s29">self </span>does not exist (notice that it is missing the function definition). In our case, we’re going to use it to immediately create a model object without first needing to instantiate a model object. Within this method, we’ll open a file using the passed-in path, in binary-read mode, and use pickle to deserialize the saved model:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Loads and returns a model <span style=" color: #231F20;">@</span><span style=" color: #32A7BD;">staticmethod</span></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load</span>(<span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Open file in the binary-read mode, load a model </span><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;rb&#39;</span>) <span style=" color: #C71F43;">as </span>f:</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>pickle.load(f)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return a model <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Since we already have a saved model, let’s create the data, and then load a model to see if it works:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Shuffle the training dataset </span>keys <span style=" color: #C71F43;">= </span>np.array(<span style=" color: #32A7BD;">range</span>(X.shape[<span style=" color: #7358A5;">0</span>])) np.random.shuffle(keys)</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>X[keys] y <span style=" color: #C71F43;">= </span>y[keys]</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">X <span style=" color: #C71F43;">= </span>(X.reshape(X.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5 </span>X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">-</span></p><p class="s14" style="padding-left: 98pt;text-indent: 0pt;text-align: left;">127.5<span style=" color: #231F20;">) </span><span style=" color: #C71F43;">/ </span>127.5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Evaluate the model <span style=" color: #231F20;">model.evaluate(X_test, y_test)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">validation, acc: <span style=" color: #7358A5;">0.874</span>, loss: <span style=" color: #7358A5;">0.354</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Saving the full trained model is a common way of saving a model. It saves parameters (weights and biases) and instances of all the model’s objects and the data they generated. That is going to be, for example, the optimizer state like cache, learning rate decay, full model structure, etc. Loading the model, in this case, is as easy as calling one method and the model is ready to use, whether we want to continue training it or use it for a prediction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch21" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch21 Chapter code, further resources, and errata for this chapter.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark343">Chapter 22</a><a name="bookmark345">&zwnj;</a><a name="bookmark346">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Prediction / Inference</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">While we often spend most of our time focusing on training and testing models, the whole reason we’re doing any of this is to have a model that takes new inputs and produces desired outputs.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This will typically involve many attempts to train the best model possible, save that model, and load that saved model to do inference, or prediction.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In the case of Fashion MNIST classification, we’d like to load a trained model, show it never- before-seen images, and have it predict the correct classification. To do this, we’ll add a new <span class="s35">predict </span>method to the <span class="s35">Model </span>class:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Predicts on the samples</p><p class="s10" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predict</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 114%;text-align: left;">Note that we predict <span class="s29">X </span>with a possible <span class="s29">batch_size</span>. This means all predictions, including predictions on just one sample, will still be fed in as a list of samples in the form of a NumPy array, whose first dimension is the list samples, and second is sample data. For example, if we would like to predict on a single image, we still need to create a NumPy array mimicking a list containing a single sample — with a shape of <i>(1, 784) </i>where <i>1 </i>is this single sample, and <i>784 </i>is</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">the number of features in a sample (pixels per image). Similar to the <span class="s35">evaluate </span>method, we’ll calculate the number of steps we plan to take:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">prediction_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">prediction_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Dividing rounds down. If there are some remaining # data, but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 118%;text-align: justify;">if <span style=" color: #231F20;">prediction_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): prediction_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then create a list that we’ll populate with the predictions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Model outputs </span>output <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ll iterate over the batches, passing the samples for predictions forward through the network, and populating the <span class="s22">output </span>with the predictions:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: justify;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(prediction_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">batch_output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 118%;text-align: left;"># Append batch prediction to the list of predictions <span style=" color: #231F20;">output.append(batch_output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">After running this, the <span class="s22">output </span>is a list of batch predictions. Each of them is a NumPy array, a partial result made by predicting on a batch of samples from the input data array. Any applications, or programs, that will make use of the inference output of our models, we expect to simply</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">pass in a list of samples and get back a list of predictions (both in the form of a NumPy array as mentioned before). Since we’re not focused on training, we’re only using batches in prediction</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">to ensure our model can fit into memory, but we’re going to get a return that’s also in batches of predictions. We can see a simple example of this:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np output </span>= <span style=" color: #231F20;">[]</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], [<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]]) output.append(b)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">6</span>], [<span style=" color: #7358A5;">7</span>, <span style=" color: #7358A5;">8</span>]]) output.append(b)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">9</span>, <span style=" color: #7358A5;">10</span>], [<span style=" color: #7358A5;">11</span>, <span style=" color: #7358A5;">12</span>]])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">output.append(b) <span style=" color: #32A7BD;">print</span>(output)</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]]), array([[<span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">6</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">7</span>, <span style=" color: #7358A5;">8</span>]]), array([[ <span style=" color: #7358A5;">9</span>, <span style=" color: #7358A5;">10</span>],</p><p class="s10" style="padding-top: 1pt;padding-left: 62pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">11</span>, <span style=" color: #7358A5;">12</span>]])]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this example, we see an output with a batch size of 2 and 6 total samples. The output is a list of arrays, with each array housing a batch of predictions. Instead, we want just 1 list of predictions, no more batches. To achieve this, we’re going to use NumPy’s <span class="s22">vstack </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np output </span>= <span style=" color: #231F20;">[]</span></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], [<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]]) output.append(b)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">6</span>], [<span style=" color: #7358A5;">7</span>, <span style=" color: #7358A5;">8</span>]]) output.append(b)</p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>np.array([[<span style=" color: #7358A5;">9</span>, <span style=" color: #7358A5;">10</span>], [<span style=" color: #7358A5;">11</span>, <span style=" color: #7358A5;">12</span>]])</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">output.append(b)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">output <span style=" color: #C71F43;">= </span>np.vstack(output) <span style=" color: #32A7BD;">print</span>(output)</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:17.478pt" cellspacing="0"><tr style="height:13pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: center;">1</p></td><td style="width:23pt"><p class="s26" style="padding-left: 8pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">2<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: center;">3</p></td><td style="width:23pt"><p class="s26" style="padding-left: 8pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">4<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: center;">5</p></td><td style="width:23pt"><p class="s26" style="padding-left: 8pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">6<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:14pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 12pt;text-align: right;">[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 12pt;text-align: center;">7</p></td><td style="width:23pt"><p class="s26" style="padding-left: 8pt;padding-right: 1pt;text-indent: 0pt;line-height: 12pt;text-align: center;">8<span style=" color: #231F20;">]</span></p></td></tr><tr style="height:13pt"><td style="width:18pt"><p class="s24" style="padding-right: 2pt;text-indent: 0pt;line-height: 11pt;text-align: right;">[</p></td><td style="width:12pt"><p class="s26" style="text-indent: 0pt;line-height: 11pt;text-align: center;">9</p></td><td style="width:23pt"><p class="s26" style="padding-left: 1pt;padding-right: 1pt;text-indent: 0pt;line-height: 11pt;text-align: center;">10<span style=" color: #231F20;">]</span></p></td></tr></table><p class="s10" style="padding-top: 1pt;padding-left: 25pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">11 12</span>]]</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;">It takes a list of objects and stacks them, if possible, creating a homologous array. This is a preferable form of the return from the <span class="s35">predict </span>method when we pass a list of samples. With plain Python, we might just add to the list each step:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], [<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">+= </span>b</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">6</span>], [<span style=" color: #7358A5;">7</span>, <span style=" color: #7358A5;">8</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">+= </span>b</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">b <span style=" color: #C71F43;">= </span>[[<span style=" color: #7358A5;">9</span>, <span style=" color: #7358A5;">10</span>], [<span style=" color: #7358A5;">11</span>, <span style=" color: #7358A5;">12</span>]]</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">output <span style=" color: #C71F43;">+= </span>b <span style=" color: #32A7BD;">print</span>(output)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 2pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">[[<span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">2</span>], [<span style=" color: #7358A5;">3</span>, <span style=" color: #7358A5;">4</span>], [<span style=" color: #7358A5;">5</span>, <span style=" color: #7358A5;">6</span>], [<span style=" color: #7358A5;">7</span>, <span style=" color: #7358A5;">8</span>], [<span style=" color: #7358A5;">9</span>, <span style=" color: #7358A5;">10</span>], [<span style=" color: #7358A5;">11</span>, <span style=" color: #7358A5;">12</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We add results to a list and stack them at the end, instead of appending to the NumPy array each batch to avoid a performance penalty. Unlike plain Python, NumPy is written in <i>C </i>language and creates data objects in memory differently. That means that there is no easy way of adding data to the existing NumPy array, other than merging two arrays and saving the result as a new array. But this will lead to a performance penalty, since the further in predictions we are, the bigger the resulting array is. The fastest and most optimal way is to append NumPy arrays to a list and stack them vertically at once when we have collected all of the partial results.</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 14pt;text-align: left;">We’ll add the <span class="s22">np.vstack </span>to the end of the outputs that we return:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 118%;text-align: left;"># Stack and return results <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">np.vstack(output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Making our full <span class="s35">predict </span>method:</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Predicts on the samples</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predict</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">prediction_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">prediction_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: justify;"># Dividing rounds down. If there are some remaining # data, but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: justify;">if <span style=" color: #231F20;">prediction_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): prediction_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Model outputs </span>output <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: justify;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(prediction_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">batch_output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Append batch prediction to the list of predictions <span style=" color: #231F20;">output.append(batch_output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Stack and return results <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">np.vstack(output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now we can load the model and test the prediction functionality:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Create dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">X, y, X_test, y_test <span style=" color: #C71F43;">= </span>create_data_mnist(<span style=" color: #8F8633;">&#39;fashion_mnist_images&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Scale and reshape samples</p><p class="s10" style="padding-top: 1pt;padding-left: 80pt;text-indent: -60pt;line-height: 108%;text-align: left;">X_test <span style=" color: #C71F43;">= </span>(X_test.reshape(X_test.shape[<span style=" color: #7358A5;">0</span>], <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Predict on the first 5 samples from validation dataset # and print the result</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">confidences <span style=" color: #C71F43;">= </span>model.predict(X_test[:<span style=" color: #7358A5;">5</span>]) <span style=" color: #32A7BD;">print</span>(confidences)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 26pt;text-indent: -6pt;line-height: 108%;text-align: left;">[[<span style=" color: #7358A5;">9.6826810e-01 8.3330568e-05 1.0794386e-03 1.3643305e-03 7.6704117e-07 5.5963554e-08 2.9197156e-02 8.6661328e-16 6.8134182e-06 1.8056496e-12</span>] [<span style=" color: #7358A5;">7.7293724e-01 2.0613789e-03 9.3451981e-04 9.0647154e-02 3.4899445e-04 2.0565639e-07 1.3301854e-01 6.3095896e-12 5.2045987e-05 7.7830048e-11</span>] [<span style=" color: #7358A5;">9.4310820e-01 5.1831361e-05 1.4724518e-03 8.1068231e-04 7.9751426e-06 9.9619001e-07 5.4532889e-02 2.9622423e-13 1.4997837e-05 2.2963499e-10</span>] [<span style=" color: #7358A5;">9.8930722e-01 1.2575739e-04 2.5738587e-04 1.4423713e-04 2.5113836e-06 5.6183376e-07 1.0156924e-02 2.8593078e-13 5.5162018e-06 1.4746830e-10</span>] [<span style=" color: #7358A5;">9.2869467e-01 7.3713978e-04 1.7579789e-03 2.1864739e-03 1.7945129e-05 1.9282908e-05 6.6521421e-02 5.1533548e-11 6.5157568e-05 7.2020221e-09</span>]]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">It looks like it’s working! After spending so much time training and finding the best hyperparameters, a common issue people have is actually <i>using </i>the model. As a reminder, each  of the subarrays in the output is a vector of confidences containing a confidence metric per class. The first thing that we need to do in this case is to gather the argmax values of these confidence vectors. Recall that we’re using a softmax classifier, so this neural network is attempting to fit to one-hot vectors, where the correct class is represented by a 1, and the others by 0s. When doing inference, it is unlikely to achieve such a perfect result, but the index associated with the highest value in the output is what we determine the model is predicting; we’re just using the argmax. We could write code to do this, but we’ve already done that in all of the activation function classes, where we added a <span class="s35">predictions </span>method:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">...</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 118%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>np.argmax(outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’ve also set an attribute in our model with the output layer’s activation function, which means we can generically acquire predictions by performing:</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Predict on the first 5 samples from validation dataset, print the result </span>confidences <span style=" color: #C71F43;">= </span>model.predict(X_test[:<span style=" color: #7358A5;">5</span>])</p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.output_layer_activation.predictions(confidences) <span style=" color: #32A7BD;">print</span>(predictions)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print first 5 labels </span><span style=" color: #32A7BD;">print</span>(y_test[:<span style=" color: #7358A5;">5</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 0 0 0 0</span>]</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">[<span style=" color: #7358A5;">0 0 0 0 0</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In this case, our model predicted all “class 0,” and our test labels were all class 0 as well. Since shuffling our testing data isn’t essential, we never shuffled them, so they’re going in the original order like our training data was. This explains why all these predictions are 0s.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">In practice, we don’t care what class number something is, we want to know <i>what </i>it is. In this case, class numbers map directly to names, so we add the following dictionary to our code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: -24pt;line-height: 108%;text-align: left;">fashion_mnist_labels <span style=" color: #C71F43;">= </span>{ <span style=" color: #7358A5;">0</span>: <span style=" color: #8F8633;">&#39;T-shirt/top&#39;</span>,</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">1</span>: <span style=" color: #8F8633;">&#39;Trouser&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">2</span>: <span style=" color: #8F8633;">&#39;Pullover&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">3</span>: <span style=" color: #8F8633;">&#39;Dress&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">4</span>: <span style=" color: #8F8633;">&#39;Coat&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">5</span>: <span style=" color: #8F8633;">&#39;Sandal&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">6</span>: <span style=" color: #8F8633;">&#39;Shirt&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">7</span>: <span style=" color: #8F8633;">&#39;Sneaker&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">8</span>: <span style=" color: #8F8633;">&#39;Bag&#39;</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">9<span style=" color: #231F20;">: </span><span style=" color: #8F8633;">&#39;Ankle boot&#39;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">}</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Then we could get the string classification by performing:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>prediction <span style=" color: #C71F43;">in </span>predictions: <span style=" color: #32A7BD;">print</span>(fashion_mnist_labels[prediction])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 111%;text-align: justify;">T-shirt/top T-shirt/top T-shirt/top T-shirt/top T-shirt/top</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">This is great, but we still have to <i>actually </i>predict something instead of the training data. When covering deep learning, the training steps often get all the focus; we want to see those accuracy and loss metrics look good! It works well to focus on training for tutorials that aim to show people how to <i>use </i>a framework, but one of the larger pain points we see is <i>applying </i>the models in production, or just running predictions on new data that was sourced from the wild (especially since outside data is rarely formatted to match your training data).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">At the moment, we have a model trained on items of clothing, so we need some truly new samples. Luckily, you’re probably a person who owns some clothes; if so, you can take photos of those to start with. If not, use the following sample photos:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/datasets/tshirt.png" class="s27">https://nnfs.io/datasets/tshirt.png</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 158pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-left: 72pt;text-indent: 0pt;line-height: 246%;text-align: center;">Fig 20.01: <a href="https://nnfs.io/datasets/pants.png" class="s82" target="_blank">Hand-made t-shirt image for the purpose of inference. </a><a href="https://nnfs.io/datasets/pants.png" class="s27" target="_blank">https://nnfs.io/datasets/pants.png</a></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 72pt;text-indent: 0pt;text-align: center;">Fig 20.02: <span class="p">Hand-made pants image for the purpose of inference.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">You can also try your hand at hand-drawing samples like these. Once you have new images/ samples that you wish to use in production, you’ll need to preprocess them in the same way the training samples were. Some of these changes are fairly difficult to forget, like the image</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: justify;">resolution or number of color channels; we’d get an error if we didn’t do those things. Let’s start preprocessing our image by loading it in. We’ll use the <i>cv2 </i>package to read in the image:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">cv2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;tshirt.png&#39;</span>, cv2.IMREAD_UNCHANGED)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;">We can view the image:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt plt.imshow(cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)) plt.show()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 20.03: <span class="p">Hand-made t-shirt image loaded with Python.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Note that we’re doing <i>cv2.cvtColor </i>because OpenCV uses BGR (blue, green, red pixel values) color format by default, but matplotlib uses RGB (red, green, blue), so we’re converting the colormap to display the image.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">The first thing we’ll do is read this image as grayscale instead of RGB. This is in contrast to the Fashion MNIST images, which are grayscaled, and we have used <span class="s22">cv2.IMREAD_UNCHANGED </span>as a parameter to the <span class="s22">cv2.imread() </span>to inform OpenCV that our intention is to read images grayscaled and unchanged. Here, we have a color image, and this parameter won’t work as</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 112%;text-align: left;">“unchanged” means containing all the colors; thus, we’ll use <span class="s22">cv2.IMREAD_GRAYSCALE </span>to force grayscaling when we read in our image:</p><p class="s11" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">cv2</span></p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;tshirt.png&#39;</span>, cv2.IMREAD_GRAYSCALE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s83" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">Then we can display it:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 112%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt plt.imshow(image_data, </span><span class="s23">cmap</span>=<span style=" color: #8F8633;">&#39;gray&#39;</span><span style=" color: #231F20;">) plt.show()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Note that we use a gray colormap with <span class="s22">plt.imshow() </span>by passing the <span class="s30">&#39;gray&#39; </span>argument into the</p><p class="s29" style="padding-top: 1pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">cmap <span class="p">parameter. The result is a grayscale image:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 14pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 20.04: <span class="p">Grayscaled hand-made t-shirt image loaded with Python.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Next, we’ll resize the image to be the same 28x28 resolution as our training data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.resize(image_data, (<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">We then display this resized image:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-bottom: 4pt;padding-left: 20pt;text-indent: 0pt;line-height: 118%;text-align: left;">plt.imshow(image_data, <span class="s23">cmap</span><span style=" color: #C71F43;">=</span><span style=" color: #8F8633;">&#39;gray&#39;</span>) plt.show()</p><p style="padding-left: 13pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 20.05: <span class="p">Grayscaled and scaled down hand-made t-shirt image.</span></h3><p class="s21" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 113%;text-align: left;"><span class="p">Next, we’ll flatten and scale the image. While the scale operation is the same as for the training data, the flattening is a bit different; we don’t have a list of images but a single image, and, as previously explained, a single image must be passed in as a list containing this single image. We flatten by applying </span><span style=" color: #231F20;">.reshape(</span>1<span style=" color: #231F20;">, </span><span style=" color: #C71F43;">-</span>1<span style=" color: #231F20;">) </span><span class="p">to the image. The </span>1 <span class="p">argument represents the number of samples, and the </span><span style=" color: #C71F43;">-</span>1 <span class="p">flattens the image to a vector of length 784. This produces a 1x784 array with our one sample and 784 features (i.e., 28x28 pixels):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>(image_data.reshape(<span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Now we can load in our model and predict on this image data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Predict on the image</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">confidences <span style=" color: #C71F43;">= </span>model.predict(image_data)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Get prediction instead of confidence levels</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.output_layer_activation.predictions(confidences)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Get label name from label index</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">prediction <span style=" color: #C71F43;">= </span>fashion_mnist_labels[predictions[<span style=" color: #7358A5;">0</span>]] <span style=" color: #32A7BD;">print</span>(prediction)</p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Making our code up to this point that loads, preprocesses, and predicts:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Label index to label name relation </span>fashion_mnist_labels <span style=" color: #C71F43;">= </span>{</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">0</span>: <span style=" color: #8F8633;">&#39;T-shirt/top&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">1</span>: <span style=" color: #8F8633;">&#39;Trouser&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">2</span>: <span style=" color: #8F8633;">&#39;Pullover&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">3</span>: <span style=" color: #8F8633;">&#39;Dress&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">4</span>: <span style=" color: #8F8633;">&#39;Coat&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">5</span>: <span style=" color: #8F8633;">&#39;Sandal&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">6</span>: <span style=" color: #8F8633;">&#39;Shirt&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">7</span>: <span style=" color: #8F8633;">&#39;Sneaker&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">8</span>: <span style=" color: #8F8633;">&#39;Bag&#39;</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">9<span style=" color: #231F20;">: </span><span style=" color: #8F8633;">&#39;Ankle boot&#39;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">}</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Read an image</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;tshirt.png&#39;</span>, cv2.IMREAD_GRAYSCALE)</p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Resize to the same size as Fashion MNIST images </span>image_data <span style=" color: #C71F43;">= </span>cv2.resize(image_data, (<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Reshape and scale pixel data</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>(image_data.reshape(<span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Predict on the image</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.predict(image_data)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Get prediction instead of confidence levels</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.output_layer_activation.predictions(predictions)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Get label name from label index</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">prediction <span style=" color: #C71F43;">= </span>fashion_mnist_labels[predictions[<span style=" color: #7358A5;">0</span>]] <span style=" color: #32A7BD;">print</span>(prediction)</p><p class="s22" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;"><span class="p">Note that we are using </span>predictions[<span style=" color: #7358A5;">0</span>] <span class="p">as we passed in a single image in the form of a list, and the model returns a list containing a single prediction.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 7pt;text-indent: 0pt;text-align: left;">Only one problem…</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">Ankle boot</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">What’s wrong? Let’s compare our currently-preprocessed image to the training data:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">cv2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">mnist_image <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;fashion_mnist_images/train/0/0000.png&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 52pt;text-indent: 0pt;text-align: center;">cv2.IMREAD_UNCHANGED)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 118%;text-align: left;">plt.imshow(mnist_image, <span class="s23">cmap</span><span style=" color: #C71F43;">=</span><span style=" color: #8F8633;">&#39;gray&#39;</span>) plt.show()</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;line-height: 231%;text-align: left;">Fig 20.06: <span class="p">Example t-shirt image from the Fashion MNIST dataset Now we compare this original and example training image to our’s:</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 20.07: <span class="p">Grayscaled and scaled down hand-made t-shirt image.</span></h3><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">The training data that we’ve used is color-inverted (i.e., the background is black instead of white, and so on). To invert our image before scaling, we can use pixel math directly instead of using OpenCV. We’ll subtract all the pixel values from the maximum pixel value: 255. For example, a value of <i>0 </i>will become <i>255 - 0 = 255</i>, and the value of <i>255 </i>will become <i>255 - 255 = 0</i>.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">image_data </span>= <span style=" color: #7358A5;">255 </span>- <span style=" color: #231F20;">image_data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">With this small change, our prediction code becomes:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Read an image</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;tshirt.png&#39;</span>, cv2.IMREAD_GRAYSCALE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Resize to the same size as Fashion MNIST images </span>image_data <span style=" color: #C71F43;">= </span>cv2.resize(image_data, (<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Invert image colors </span><span style=" color: #231F20;">image_data </span>= <span style=" color: #7358A5;">255 </span>- <span style=" color: #231F20;">image_data</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Reshape and scale pixel data</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>(image_data.reshape(<span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Predict on the image</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">confidences <span style=" color: #C71F43;">= </span>model.predict(image_data)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Get prediction instead of confidence levels</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.output_layer_activation.predictions(confidences)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 20pt;text-indent: 0pt;text-align: left;"># Get label name from label index</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;line-height: 216%;text-align: left;">prediction <span style=" color: #C71F43;">= </span>fashion_mnist_labels[predictions[<span style=" color: #7358A5;">0</span>]] <span style=" color: #32A7BD;">print</span>(prediction)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">T-shirt/top</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Now it works! The reason it works now, and not work previously, is from how the <i>Dense </i>layers work — they learn feature (pixel in this case) values and the correlation between them. Contrast this with convolutional layers, which are being trained to find and understand features on images (not features as data input nodes, but actual characteristics/traits, such as lines and curves).</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Because pixel values were very different, the model incorrectly put its “guess” in this case. Convolutional layers may properly predict in this case, as-is.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s try the pants:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">matplotlib.pyplot </span>as <span style=" color: #231F20;">plt </span>import <span style=" color: #231F20;">cv2</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 112%;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;pants.png&#39;</span>, cv2.IMREAD_UNCHANGED) plt.imshow(cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)) plt.show()</p><p style="padding-left: 57pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h3 style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;line-height: 246%;text-align: left;">Fig 20.08: <span class="p">Hand-made pants image loaded with Python. Now we’ll preprocess:</span></h3><p class="s17" style="padding-left: 20pt;text-indent: 0pt;line-height: 10pt;text-align: left;"># Read an image</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;pants.png&#39;</span>, cv2.IMREAD_GRAYSCALE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Resize to the same size as Fashion MNIST images </span>image_data <span style=" color: #C71F43;">= </span>cv2.resize(image_data, (<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Invert image colors </span><span style=" color: #231F20;">image_data </span>= <span style=" color: #7358A5;">255 </span>- <span style=" color: #231F20;">image_data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Let’s see what we have:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;">plt.imshow(image_data, <span class="s23">cmap</span><span style=" color: #C71F43;">=</span><span style=" color: #8F8633;">&#39;gray&#39;</span>) plt.show()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><span/></p><h3 style="padding-top: 6pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Fig 20.09: <span class="p">Grayscaled and scaled down hand-made t-shirt image.</span></h3><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;">Making our code:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Label index to label name relation </span>fashion_mnist_labels <span style=" color: #C71F43;">= </span>{</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">0</span>: <span style=" color: #8F8633;">&#39;T-shirt/top&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">1</span>: <span style=" color: #8F8633;">&#39;Trouser&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">2</span>: <span style=" color: #8F8633;">&#39;Pullover&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">3</span>: <span style=" color: #8F8633;">&#39;Dress&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">4</span>: <span style=" color: #8F8633;">&#39;Coat&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">5</span>: <span style=" color: #8F8633;">&#39;Sandal&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">6</span>: <span style=" color: #8F8633;">&#39;Shirt&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">7</span>: <span style=" color: #8F8633;">&#39;Sneaker&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">8</span>: <span style=" color: #8F8633;">&#39;Bag&#39;</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">9<span style=" color: #231F20;">: </span><span style=" color: #8F8633;">&#39;Ankle boot&#39;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">}</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Read an image</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;pants.png&#39;</span>, cv2.IMREAD_GRAYSCALE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Resize to the same size as Fashion MNIST images </span>image_data <span style=" color: #C71F43;">= </span>cv2.resize(image_data, (<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Invert image colors </span><span style=" color: #231F20;">image_data </span>= <span style=" color: #7358A5;">255 </span>- <span style=" color: #231F20;">image_data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Reshape and scale pixel data</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>(image_data.reshape(<span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Predict on the image</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">confidences <span style=" color: #C71F43;">= </span>model.predict(image_data)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Get prediction instead of confidence levels</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.output_layer_activation.predictions(confidences)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Get label name from label index</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 216%;text-align: left;">prediction <span style=" color: #C71F43;">= </span>fashion_mnist_labels[predictions[<span style=" color: #7358A5;">0</span>]] <span style=" color: #32A7BD;">print</span>(prediction)</p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">&gt;&gt;&gt;</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">Trouser</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">A success again! We have now coded in the last feature of our model, which closes the list of the topics that we covered in this book.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s8" style="padding-top: 10pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark344">Full code:</a><a name="bookmark347">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-top: 22pt;padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">numpy </span>as <span style=" color: #231F20;">np </span>import <span style=" color: #231F20;">nnfs </span>import <span style=" color: #231F20;">os</span></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;">import <span style=" color: #231F20;">cv2 </span>import <span style=" color: #231F20;">pickle </span>import <span style=" color: #231F20;">copy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;text-align: left;">nnfs.init()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dense layer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dense</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Layer initialization</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">n_inputs</span>, <span class="s23">n_neurons</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 122pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>weight_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l1<span class="s11">=</span><span class="s14">0</span><span class="s10">, </span>bias_regularizer_l2<span class="s11">=</span><span class="s14">0</span><span class="s10">):</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Initialize weights and biases</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.weights <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0.01 </span><span style=" color: #C71F43;">* </span>np.random.randn(n_inputs, n_neurons) self.biases <span style=" color: #C71F43;">= </span>np.zeros((<span style=" color: #7358A5;">1</span>, n_neurons))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Set regularization strength </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">= </span>weight_regularizer_l1 self.weight_regularizer_l2 <span style=" color: #C71F43;">= </span>weight_regularizer_l2 self.bias_regularizer_l1 <span style=" color: #C71F43;">= </span>bias_regularizer_l1 self.bias_regularizer_l2 <span style=" color: #C71F43;">= </span>bias_regularizer_l2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs, weights and biases </span>self.output <span style=" color: #C71F43;">= </span>np.dot(inputs, self.weights) <span style=" color: #C71F43;">+ </span>self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradients on parameters</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">= </span>np.dot(self.inputs.T, dvalues) self.dbiases <span style=" color: #C71F43;">= </span>np.sum(dvalues, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0</span>, <span class="s23">keepdims</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Gradients on regularization # L1 on weights</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.weights) dL1[self.weights </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dweights <span style=" color: #C71F43;">+= </span>self.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on weights</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dweights </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.weight_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 195pt;text-indent: 0pt;text-align: left;">self.weights</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># L1 on biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">: dL1 </span>= <span style=" color: #231F20;">np.ones_like(self.biases) dL1[self.biases </span>&lt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">] </span>= -<span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.dbiases <span style=" color: #C71F43;">+= </span>self.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>dL1 <span style=" color: #A5A4A5;"># L2 on biases</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">self.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.dbiases </span>+= <span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">self.bias_regularizer_l2 </span>* <span style=" color: #231F20;">\</span></p><p class="s10" style="padding-top: 1pt;padding-left: 189pt;text-indent: 0pt;text-align: left;">self.biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.dot(dvalues, self.weights.T)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Retrieve layer parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">get_parameters</span>(<span class="s23">self</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">self.weights, self.biases</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set weights and biases in a layer instance</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set_parameters</span>(<span class="s23">self</span>, <span class="s23">weights</span>, <span class="s23">biases</span>): self.weights <span style=" color: #C71F43;">= </span>weights</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.biases <span style=" color: #C71F43;">= </span>biases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Dropout</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Dropout</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Init</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">rate</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.rate </span>= <span style=" color: #7358A5;">1 </span>- <span style=" color: #231F20;">rate</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Save input values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If not in the training mode - return values <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">training:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs.copy() <span style=" color: #C71F43;">return</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Generate and save scaled mask</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary_mask <span style=" color: #C71F43;">= </span>np.random.binomial(<span style=" color: #7358A5;">1</span>, self.rate,</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 114pt;line-height: 108%;text-align: left;"><span class="s23">size</span>=<span style=" color: #231F20;">inputs.shape) </span>/ <span style=" color: #231F20;">self.rate </span><span style=" color: #A5A4A5;"># Apply mask to output values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>inputs <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>): <span style=" color: #A5A4A5;"># Gradient on values</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>self.binary_mask</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Input &quot;layer&quot;</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Layer_Input</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># ReLU activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_ReLU</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate output values from inputs </span>self.output <span style=" color: #C71F43;">= </span>np.maximum(<span style=" color: #7358A5;">0</span>, inputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Since we need to modify original variable, # let&#39;s make a copy of values first </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Zero gradient where input values were negative </span>self.dinputs[self.inputs <span style=" color: #C71F43;">&lt;= </span><span style=" color: #7358A5;">0</span>] <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Softmax activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Remember input values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get unnormalized probabilities</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">exp_values <span style=" color: #C71F43;">= </span>np.exp(inputs <span style=" color: #C71F43;">- </span>np.max(inputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">))</span></p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Normalize them for each sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">probabilities <span style=" color: #C71F43;">= </span>exp_values <span style=" color: #C71F43;">/ </span>np.sum(exp_values, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>,</p><p class="s23" style="padding-top: 1pt;padding-left: 286pt;text-indent: 0pt;text-align: left;">keepdims<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">self.output <span style=" color: #C71F43;">= </span>probabilities <span style=" color: #A5A4A5;"># Backward pass</span></p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create uninitialized array </span>self.dinputs <span style=" color: #C71F43;">= </span>np.empty_like(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Enumerate outputs and gradients</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -48pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">for </span>index, (single_output, single_dvalues) <span style=" color: #C71F43;">in </span>\ <span style=" color: #32A7BD;">enumerate</span>(<span style=" color: #32A7BD;">zip</span>(self.output, dvalues)):</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Flatten output array</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">single_output <span style=" color: #C71F43;">= </span>single_output.reshape(<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>, <span style=" color: #7358A5;">1</span>) <span style=" color: #A5A4A5;"># Calculate Jacobian matrix of the output</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">jacobian_matrix <span style=" color: #C71F43;">= </span>np.diagflat(single_output) <span style=" color: #C71F43;">- </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">np.dot(single_output, single_output.T) <span style=" color: #A5A4A5;"># Calculate sample-wise gradient</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and add it to the array of sample gradients </span>self.dinputs[index] <span style=" color: #C71F43;">= </span>np.dot(jacobian_matrix,</p><p class="s10" style="padding-left: 267pt;text-indent: 0pt;text-align: left;">single_dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>np.argmax(outputs, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Sigmoid activation</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Sigmoid</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Save input and calculate/save output # of the sigmoid function</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.inputs <span style=" color: #C71F43;">= </span>inputs</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #231F20;">self.output </span>= <span style=" color: #7358A5;">1 </span>/ <span style=" color: #231F20;">(</span><span style=" color: #7358A5;">1 </span>+ <span style=" color: #231F20;">np.exp(</span>-<span style=" color: #231F20;">inputs))</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Derivative - calculates from output of the sigmoid function </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues <span style=" color: #C71F43;">* </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.output) <span style=" color: #C71F43;">* </span>self.output</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>(outputs <span style=" color: #C71F43;">&gt; </span><span style=" color: #7358A5;">0.5</span>) <span style=" color: #C71F43;">* </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Linear activation</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Linear</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">inputs</span>, <span class="s23">training</span>): <span style=" color: #A5A4A5;"># Just remember values </span>self.inputs <span style=" color: #C71F43;">= </span>inputs self.output <span style=" color: #C71F43;">= </span>inputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># derivative is 1, 1 * dvalues = dvalues - the chain rule </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculate predictions for outputs</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predictions</span>(<span class="s23">self</span>, <span class="s23">outputs</span>): <span style=" color: #C71F43;">return </span>outputs</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># SGD optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_SGD</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings,</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># learning rate of 1. is default for this optimizer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">momentum</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.momentum <span style=" color: #C71F43;">= </span>momentum</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If we use momentum <span style=" color: #C71F43;">if </span><span style=" color: #231F20;">self.momentum:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain momentum arrays, create them # filled with zeros</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_momentums&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) <span style=" color: #A5A4A5;"># If there is no momentum array for weights</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># The array doesn&#39;t exist for biases yet either. </span>layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Build weight updates with momentum - take previous # updates multiplied by retain factor and update with # current gradients</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">weight_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.weight_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">= </span>weight_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Build bias updates </span>bias_updates <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.momentum <span style=" color: #C71F43;">* </span>layer.bias_momentums <span style=" color: #C71F43;">- </span>\ self.current_learning_rate <span style=" color: #C71F43;">* </span>layer.dbiases</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">= </span>bias_updates</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD updates (as before momentum update) <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 195pt;text-indent: -102pt;line-height: 108%;text-align: left;">weight_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\ layer.dweights</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">bias_updates <span style=" color: #C71F43;">= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 183pt;text-indent: 0pt;text-align: left;">layer.dbiases</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update weights and biases using either # vanilla or momentum updates </span>layer.weights <span style=" color: #C71F43;">+= </span>weight_updates layer.biases <span style=" color: #C71F43;">+= </span>bias_updates</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adagrad optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adagrad</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1.</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>): self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.decay <span style=" color: #C71F43;">= </span>decay self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">+= </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">+= </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># RMSprop optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_RMSprop</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">rho</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.rho <span style=" color: #C71F43;">= </span>rho</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.rho <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.rho) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dweights <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.weight_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">layer.dbiases <span style=" color: #C71F43;">/ </span>\ (np.sqrt(layer.bias_cache) <span style=" color: #C71F43;">+ </span>self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Adam optimizer</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Optimizer_Adam</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Initialize optimizer - set settings</p><p class="s10" style="padding-top: 1pt;padding-left: 122pt;text-indent: -78pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span class="s23">learning_rate</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.001</span>, <span class="s23">decay</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.</span>, <span class="s23">epsilon</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1e-7</span>, <span class="s23">beta_1</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.9</span>, <span class="s23">beta_2</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">0.999</span>):</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.current_learning_rate <span style=" color: #C71F43;">= </span>learning_rate self.decay <span style=" color: #C71F43;">= </span>decay</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.iterations <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0 </span>self.epsilon <span style=" color: #C71F43;">= </span>epsilon self.beta_1 <span style=" color: #C71F43;">= </span>beta_1 self.beta_2 <span style=" color: #C71F43;">= </span>beta_2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once before any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">pre_update_params</span>(<span class="s23">self</span>): <span style=" color: #C71F43;">if </span>self.decay:</p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.current_learning_rate <span style=" color: #C71F43;">= </span>self.learning_rate <span style=" color: #C71F43;">* </span>\ (<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1. </span><span style=" color: #C71F43;">+ </span>self.decay <span style=" color: #C71F43;">* </span>self.iterations))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Update parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">update_params</span>(<span class="s23">self</span>, <span class="s23">layer</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer does not contain cache arrays, # create them filled with zeros</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span><span style=" color: #32A7BD;">hasattr</span>(layer, <span style=" color: #8F8633;">&#39;weight_cache&#39;</span>): layer.weight_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.weight_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.weights) layer.bias_momentums <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases) layer.bias_cache <span style=" color: #C71F43;">= </span>np.zeros_like(layer.biases)</p><p class="s10" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update momentum with current gradients </span>layer.weight_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 219pt;text-indent: 0pt;text-align: left;">layer.weight_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 151pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dweights layer.bias_momentums <span style=" color: #C71F43;">= </span>self.beta_1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 207pt;text-indent: 0pt;text-align: left;">layer.bias_momentums <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 139pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1) <span style=" color: #C71F43;">* </span>layer.dbiases <span style=" color: #A5A4A5;"># Get corrected momentum</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># self.iteration is 0 at first pass # and we need to start with 1 here</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">= </span>layer.weight_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">= </span>layer.bias_momentums <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_1 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update cache with squared current gradients </span>layer.weight_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.weight_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dweights<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span>layer.bias_cache <span style=" color: #C71F43;">= </span>self.beta_2 <span style=" color: #C71F43;">* </span>layer.bias_cache <span style=" color: #C71F43;">+ </span>\</p><p class="s10" style="padding-left: 68pt;text-indent: 24pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2) <span style=" color: #C71F43;">* </span>layer.dbiases<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2 </span><span style=" color: #A5A4A5;"># Get corrected cache</span></p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">weight_cache_corrected <span style=" color: #C71F43;">= </span>layer.weight_cache <span style=" color: #C71F43;">/ </span>\ (<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">bias_cache_corrected <span style=" color: #C71F43;">= </span>layer.bias_cache <span style=" color: #C71F43;">/ </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>self.beta_2 <span style=" color: #C71F43;">** </span>(self.iterations <span style=" color: #C71F43;">+ </span><span style=" color: #7358A5;">1</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Vanilla SGD parameter update + normalization # with square rooted cache</p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">layer.weights <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">weight_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(weight_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 68pt;text-indent: 126pt;line-height: 108%;text-align: left;">self.epsilon) layer.biases <span style=" color: #C71F43;">+= -</span>self.current_learning_rate <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-left: 171pt;text-indent: 0pt;line-height: 108%;text-align: left;">bias_momentums_corrected <span style=" color: #C71F43;">/ </span>\ (np.sqrt(bias_cache_corrected) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 195pt;text-indent: 0pt;text-align: left;">self.epsilon)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Call once after any parameter updates</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">post_update_params</span>(<span class="s23">self</span>): self.iterations <span style=" color: #C71F43;">+= </span><span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Common loss class</p><p class="s31" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Loss</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Regularization loss calculation</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">regularization_loss</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># 0 by default <span style=" color: #231F20;">regularization_loss </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate regularization loss # iterate all trainable layers</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.weight_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.weights))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - weights</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.weight_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.weight_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: -42pt;line-height: 108%;text-align: left;">np.sum(layer.weights <span style=" color: #C71F43;">* </span>\ layer.weights)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L1 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># calculate only when factor greater than 0 </span>if <span style=" color: #231F20;">layer.bias_regularizer_l1 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l1 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(np.abs(layer.biases))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># L2 regularization - biases</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">if <span style=" color: #231F20;">layer.bias_regularizer_l2 </span>&gt; <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">regularization_loss <span style=" color: #C71F43;">+= </span>layer.bias_regularizer_l2 <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 255pt;text-indent: 0pt;text-align: left;">np.sum(layer.biases <span style=" color: #C71F43;">* </span>\</p><p class="s10" style="padding-top: 1pt;padding-left: 298pt;text-indent: 0pt;text-align: left;">layer.biases)</p><p class="s11" style="padding-left: 44pt;text-indent: 24pt;line-height: 28pt;text-align: left;">return <span style=" color: #231F20;">regularization_loss </span><span style=" color: #A5A4A5;"># Set/remember trainable layers</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">remember_trainable_layers</span>(<span class="s23">self</span>, <span class="s23">trainable_layers</span>): self.trainable_layers <span style=" color: #C71F43;">= </span>trainable_layers</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculates the data and regularization losses # given model output and ground truth values</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate sample losses </span>sample_losses <span style=" color: #C71F43;">= </span>self.forward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>np.mean(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of losses and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(sample_losses) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(sample_losses)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">include_regularization</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate mean loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">data_loss <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If just data loss - return it <span style=" color: #C71F43;">if not </span><span style=" color: #231F20;">include_regularization:</span></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">data_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">data_loss, self.regularization_loss()</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_CategoricalCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples in a batch </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(y_pred)</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Probabilities for target values - # only if categorical labels</p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">: correct_confidences </span>= <span style=" color: #231F20;">y_pred_clipped[</span></p><p class="s12" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">range<span style=" color: #231F20;">(samples), y_true</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Mask values - only for one-hot encoded labels </span>elif <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">correct_confidences <span style=" color: #C71F43;">= </span>np.sum( y_pred_clipped <span style=" color: #C71F43;">* </span>y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Losses</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">negative_log_likelihoods <span style=" color: #C71F43;">= -</span>np.log(correct_confidences) <span style=" color: #C71F43;">return </span>negative_log_likelihoods</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of labels in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>labels <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If labels are sparse, turn them into one-hot vector </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y_true <span style=" color: #C71F43;">= </span>np.eye(labels)[y_true]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate gradient </span>self.dinputs <span style=" color: #C71F43;">= -</span>y_true <span style=" color: #C71F43;">/ </span>dvalues <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier - combined Softmax activation # and cross-entropy loss for faster backward step</p><p class="s31" style="padding-left: 20pt;text-indent: 0pt;text-align: left;">class <span class="s32">Activation_Softmax_Loss_CategoricalCrossentropy</span><span class="s10">():</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: justify;"><span style=" color: #A5A4A5;"># If labels are one-hot encoded, # turn them into discrete values </span>if <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y_true.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: justify;">y_true <span style=" color: #C71F43;">= </span>np.argmax(y_true, <span class="s23">axis</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Copy so we can safely modify </span>self.dinputs <span style=" color: #C71F43;">= </span>dvalues.copy() <span style=" color: #A5A4A5;"># Calculate gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs[<span style=" color: #32A7BD;">range</span>(samples), y_true] <span style=" color: #C71F43;">-= </span><span style=" color: #7358A5;">1 </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Binary cross-entropy loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_BinaryCrossentropy</span>(<span class="s44">Loss</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>y_pred_clipped <span style=" color: #C71F43;">= </span>np.clip(y_pred, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate sample-wise loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">* </span>np.log(y_pred_clipped) <span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 108pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">* </span>np.log(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_pred_clipped)) sample_losses <span style=" color: #C71F43;">= </span>np.mean(sample_losses, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Clip data to prevent division by 0</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Clip both sides to not drag mean towards any value </span>clipped_dvalues <span style=" color: #C71F43;">= </span>np.clip(dvalues, <span style=" color: #7358A5;">1e-7</span>, <span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1e-7</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= -</span>(y_true <span style=" color: #C71F43;">/ </span>clipped_dvalues <span style=" color: #C71F43;">-</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 102pt;line-height: 108%;text-align: left;">(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>y_true) <span style=" color: #C71F43;">/ </span>(<span style=" color: #7358A5;">1 </span><span style=" color: #C71F43;">- </span>clipped_dvalues)) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Squared Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanSquaredError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L2 loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean((y_true <span style=" color: #C71F43;">- </span>y_pred)<span style=" color: #C71F43;">**</span><span style=" color: #7358A5;">2</span>, <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Gradient on values</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #231F20;">self.dinputs </span>= -<span style=" color: #7358A5;">2 </span>* <span style=" color: #231F20;">(y_true </span>- <span style=" color: #231F20;">dvalues) </span>/ <span style=" color: #231F20;">outputs </span><span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Mean Absolute Error loss</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Loss_MeanAbsoluteError</span>(<span class="s44">Loss</span>): <span style=" color: #A5A4A5;"># L1 loss</span></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 28pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">y_pred</span>, <span class="s23">y_true</span>): <span style=" color: #A5A4A5;"># Calculate loss</span></p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">sample_losses <span style=" color: #C71F43;">= </span>np.mean(np.abs(y_true <span style=" color: #C71F43;">- </span>y_pred), <span class="s23">axis</span><span style=" color: #C71F43;">=-</span><span style=" color: #7358A5;">1</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">sample_losses</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">dvalues</span>, <span class="s23">y_true</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Number of samples </span>samples <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues)</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Number of outputs in every sample</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># We&#39;ll use the first sample to count them </span>outputs <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(dvalues[<span style=" color: #7358A5;">0</span>])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate gradient</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>np.sign(y_true <span style=" color: #C71F43;">- </span>dvalues) <span style=" color: #C71F43;">/ </span>outputs <span style=" color: #A5A4A5;"># Normalize gradient</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.dinputs <span style=" color: #C71F43;">= </span>self.dinputs <span style=" color: #C71F43;">/ </span>samples</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Common accuracy class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Accuracy</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates an accuracy</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># given predictions and ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Get comparison results</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">comparisons <span style=" color: #C71F43;">= </span>self.compare(predictions, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate an accuracy </span>accuracy <span style=" color: #C71F43;">= </span>np.mean(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Add accumulated sum of matching values and sample count </span>self.accumulated_sum <span style=" color: #C71F43;">+= </span>np.sum(comparisons) self.accumulated_count <span style=" color: #C71F43;">+= </span><span style=" color: #32A7BD;">len</span>(comparisons)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return accuracy <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates accumulated accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">calculate_accumulated</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Calculate an accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accumulated_sum <span style=" color: #C71F43;">/ </span>self.accumulated_count</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return the data and regularization losses <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Reset variables for accumulated accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">new_pass</span>(<span class="s23">self</span>): self.accumulated_sum <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.accumulated_count <span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">0</span></p><p class="s17" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for classification model</p><p class="s10" style="padding-top: 1pt;padding-left: 20pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Categorical</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">binary</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>): <span style=" color: #A5A4A5;"># Binary mode?</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">self.binary <span style=" color: #C71F43;">= </span>binary</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># No initialization is needed</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>): <span style=" color: #C71F43;">pass</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if not <span style=" color: #231F20;">self.binary </span>and <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(y.shape) </span>== <span style=" color: #7358A5;">2</span><span style=" color: #231F20;">: y </span>= <span style=" color: #231F20;">np.argmax(y, </span><span class="s23">axis</span>=<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">)</span></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">predictions </span>== <span style=" color: #231F20;">y</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Accuracy calculation for regression model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">class </span><span style=" color: #427F3C;">Accuracy_Regression</span>(<span class="s44">Accuracy</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Create precision property <span style=" color: #231F20;">self.precision </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Calculates precision value</p><p class="s17" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># based on passed-in ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">init</span>(<span class="s23">self</span>, <span class="s23">y</span>, <span class="s23">reinit</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.precision </span>is <span style=" color: #7358A5;">None </span>or <span style=" color: #231F20;">reinit: self.precision </span>= <span style=" color: #231F20;">np.std(y) </span>/ <span style=" color: #7358A5;">250</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Compares predictions to the ground truth values</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">compare</span>(<span class="s23">self</span>, <span class="s23">predictions</span>, <span class="s23">y</span>):</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">np.absolute(predictions </span>- <span style=" color: #231F20;">y) </span>&lt; <span style=" color: #231F20;">self.precision</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Model class</p><p class="s31" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">class <span class="s32">Model</span><span class="s10">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def</span><span class="s33"> </span><span style=" color: #32A7BD;">init</span><u> </u>(<span class="s23">self</span>):</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list of network objects </span>self.layers <span style=" color: #C71F43;">= </span>[]</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Softmax classifier&#39;s output object <span style=" color: #231F20;">self.softmax_classifier_output </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">None</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Add objects to the model</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">add</span>(<span class="s23">self</span>, <span class="s23">layer</span>): self.layers.append(layer)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Set loss, optimizer and accuracy</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set</span>(<span class="s23">self</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">loss</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">optimizer</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">accuracy</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">loss </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: self.loss </span>= <span style=" color: #231F20;">loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">optimizer </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: self.optimizer </span>= <span style=" color: #231F20;">optimizer</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">accuracy </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: self.accuracy </span>= <span style=" color: #231F20;">accuracy</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Finalize the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">finalize</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create and set the input layer </span>self.input_layer <span style=" color: #C71F43;">= </span>Layer_Input()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Count all the objects </span>layer_count <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(self.layers)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Initialize a list containing trainable layers: </span>self.trainable_layers <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate the objects</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">i </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(layer_count):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If it&#39;s the first layer,</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># the previous layer object is the input layer </span>if <span style=" color: #231F20;">i </span>== <span style=" color: #7358A5;">0</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.input_layer self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># All layers except for the first and the last </span>elif <span style=" color: #231F20;">i </span>&lt; <span style=" color: #231F20;">layer_count </span>- <span style=" color: #7358A5;">1</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># The last layer - the next object is the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Also let&#39;s save aside the reference to the last object # whose output is the model&#39;s output</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">else<span style=" color: #231F20;">:</span></p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">self.layers[i].prev <span style=" color: #C71F43;">= </span>self.layers[i<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>] self.layers[i].next <span style=" color: #C71F43;">= </span>self.loss self.output_layer_activation <span style=" color: #C71F43;">= </span>self.layers[i]</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># If layer contains an attribute called &quot;weights&quot;, # it&#39;s a trainable layer -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># add it to the list of trainable layers # We don&#39;t need to check for biases -</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># checking for weights is enough</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">hasattr</span>(self.layers[i], <span style=" color: #8F8633;">&#39;weights&#39;</span>): self.trainable_layers.append(self.layers[i])</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Update loss object with trainable layers </span>if <span style=" color: #231F20;">self.loss </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">self.loss.remember_trainable_layers( self.trainable_layers</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If output activation is Softmax and</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># loss function is Categorical Cross-Entropy # create an object of combined activation</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># and loss function containing # faster gradient calculation</p><p class="s10" style="padding-left: 86pt;text-indent: -18pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if </span><span style=" color: #32A7BD;">isinstance</span>(self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>], Activation_Softmax) <span style=" color: #C71F43;">and </span>\ <span style=" color: #32A7BD;">isinstance</span>(self.loss, Loss_CategoricalCrossentropy): <span style=" color: #A5A4A5;"># Create an object of combined activation</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and loss functions </span>self.softmax_classifier_output <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">Activation_Softmax_Loss_CategoricalCrossentropy()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Train the model</p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: -60pt;line-height: 108%;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">train</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">y</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">epochs</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>, <span class="s23">print_every</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">1</span>, <span class="s23">validation_data</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Initialize accuracy object <span style=" color: #231F20;">self.accuracy.init(y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">train_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">train_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">train_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): train_steps </span>+= <span style=" color: #7358A5;">1</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Main training loop</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span>epoch <span style=" color: #C71F43;">in </span><span style=" color: #32A7BD;">range</span>(<span style=" color: #7358A5;">1</span>, epochs<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Print epoch number </span><span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;epoch: </span>{epoch}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">self.loss.new_pass()</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.accuracy.new_pass()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(train_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X batch_y <span style=" color: #C71F43;">= </span>y</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size] batch_y <span style=" color: #C71F43;">= </span>y[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">True</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;text-align: left;"># Calculate loss</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">data_loss, regularization_loss <span style=" color: #C71F43;">= </span>\ self.loss.calculate(output, batch_y,</p><p class="s11" style="padding-left: 116pt;text-indent: 145pt;line-height: 108%;text-align: left;"><span class="s23">include_regularization</span>=<span style=" color: #7358A5;">True</span><span style=" color: #231F20;">) loss </span>= <span style=" color: #231F20;">data_loss </span>+ <span style=" color: #231F20;">regularization_loss</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 225pt;text-indent: 0pt;text-align: left;">output)</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 0pt;text-align: left;">accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate(predictions,</p><p class="s10" style="padding-top: 1pt;padding-left: 328pt;text-indent: 0pt;text-align: left;">batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Perform backward pass <span style=" color: #231F20;">self.backward(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Optimize (update parameters) </span>self.optimizer.pre_update_params() <span style=" color: #C71F43;">for </span>layer <span style=" color: #C71F43;">in </span>self.trainable_layers:</p><p class="s10" style="padding-left: 116pt;text-indent: 24pt;line-height: 108%;text-align: left;">self.optimizer.update_params(layer) self.optimizer.post_update_params()</p><p class="s17" style="padding-top: 2pt;padding-left: 116pt;text-indent: 0pt;text-align: left;"># Print a summary</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">if not </span>step <span style=" color: #C71F43;">% </span>print_every <span style=" color: #C71F43;">or </span>step <span style=" color: #C71F43;">== </span>train_steps <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">1</span>: <span style=" color: #32A7BD;">print</span>(<span class="s31">f</span><span style=" color: #8F8633;">&#39;step: </span>{step}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 177pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print epoch loss and accuracy </span>epoch_data_loss, epoch_regularization_loss <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="text-indent: 0pt;text-align: right;">self.loss.calculate_accumulated(</p><p class="s23" style="padding-top: 1pt;text-indent: 0pt;text-align: right;">include_regularization<span class="s11">=</span><span class="s14">True</span><span class="s10">)</span></p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;">epoch_loss <span style=" color: #C71F43;">= </span>epoch_data_loss <span style=" color: #C71F43;">+ </span>epoch_regularization_loss epoch_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s12" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">print<span style=" color: #231F20;">(</span><i>f</i><span style=" color: #8F8633;">&#39;training, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{epoch_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{epoch_loss<span style=" color: #7358A5;">:.3f</span>} <span style=" color: #8F8633;">(&#39; </span><span style=" color: #C71F43;">+ </span><span class="s31">f</span><span style=" color: #8F8633;">&#39;data_loss: </span>{epoch_data_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;reg_loss: </span>{epoch_regularization_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">), &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s38" style="padding-top: 1pt;padding-left: 128pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span>&#39;lr: <span style=" color: #231F20;">{self.optimizer.current_learning_rate}</span>&#39;<span style=" color: #231F20;">)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># If there is the validation data </span>if <span style=" color: #231F20;">validation_data </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Evaluate the model: </span>self.evaluate(<span style=" color: #C71F43;">*</span>validation_data,</p><p class="s23" style="padding-left: 201pt;text-indent: 0pt;text-align: left;">batch_size<span class="s11">=</span><span class="s10">batch_size)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Evaluates the model using passed-in dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">evaluate</span>(<span class="s23">self</span>, <span class="s23">X_val</span>, <span class="s23">y_val</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">validation_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">validation_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X_val) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">validation_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X_val): validation_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss # and accuracy objects <span style=" color: #231F20;">self.loss.new_pass() self.accuracy.new_pass()</span></p><p class="s17" style="padding-top: 2pt;padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(validation_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val batch_y <span style=" color: #C71F43;">= </span>y_val</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">]</p><p class="s10" style="padding-top: 1pt;padding-left: 140pt;text-indent: -24pt;line-height: 108%;text-align: left;">batch_y <span style=" color: #C71F43;">= </span>y_val[ step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Calculate the loss <span style=" color: #231F20;">self.loss.calculate(output, batch_y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get predictions and calculate an accuracy </span>predictions <span style=" color: #C71F43;">= </span>self.output_layer_activation.predictions(</p><p class="s10" style="padding-left: 92pt;text-indent: 108pt;line-height: 108%;text-align: left;">output) self.accuracy.calculate(predictions, batch_y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Get and print validation loss and accuracy </span>validation_loss <span style=" color: #C71F43;">= </span>self.loss.calculate_accumulated() validation_accuracy <span style=" color: #C71F43;">= </span>self.accuracy.calculate_accumulated()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Print a summary <span style=" color: #32A7BD;">print</span><span style=" color: #231F20;">(</span><span class="s31">f</span><span style=" color: #8F8633;">&#39;validation, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;acc: </span>{validation_accuracy<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">, &#39; </span><span style=" color: #C71F43;">+</span></p><p class="s10" style="padding-top: 1pt;padding-left: 104pt;text-indent: 0pt;text-align: left;"><span class="s31">f</span><span style=" color: #8F8633;">&#39;loss: </span>{validation_loss<span style=" color: #7358A5;">:.3f</span>}<span style=" color: #8F8633;">&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Predicts on the samples</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">predict</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span style=" color: #C71F43;">*</span>, <span class="s23">batch_size</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">None</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Default value if batch size is not being set <span style=" color: #231F20;">prediction_steps </span><span style=" color: #C71F43;">= </span><span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Calculate number of steps </span>if <span style=" color: #231F20;">batch_size </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">prediction_steps <span style=" color: #C71F43;">= </span><span style=" color: #32A7BD;">len</span>(X) <span style=" color: #C71F43;">// </span>batch_size</p><p class="s17" style="padding-top: 2pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Dividing rounds down. If there are some remaining # data but not a full batch, this won&#39;t include it # Add `1` to include this not full batch</p><p class="s11" style="padding-left: 116pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">prediction_steps </span>* <span style=" color: #231F20;">batch_size </span>&lt; <span style=" color: #32A7BD;">len</span><span style=" color: #231F20;">(X): prediction_steps </span>+= <span style=" color: #7358A5;">1</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Model outputs </span>output <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate over steps</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;text-align: left;">for <span style=" color: #231F20;">step </span>in <span style=" color: #32A7BD;">range</span><span style=" color: #231F20;">(prediction_steps):</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># If batch size is not set -</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># train using one step and full dataset </span>if <span style=" color: #231F20;">batch_size </span>is <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Otherwise slice a batch <span style=" color: #C71F43;">else</span><span style=" color: #231F20;">:</span></p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">batch_X <span style=" color: #C71F43;">= </span>X[step<span style=" color: #C71F43;">*</span>batch_size:(step<span style=" color: #C71F43;">+</span><span style=" color: #7358A5;">1</span>)<span style=" color: #C71F43;">*</span>batch_size]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># Perform the forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;text-align: left;">batch_output <span style=" color: #C71F43;">= </span>self.forward(batch_X, <span class="s23">training</span><span style=" color: #C71F43;">=</span><span style=" color: #7358A5;">False</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Append batch prediction to the list of predictions <span style=" color: #231F20;">output.append(batch_output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Stack and return results <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">np.vstack(output)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs forward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">forward</span>(<span class="s23">self</span>, <span class="s23">X</span>, <span class="s23">training</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call forward method on the input layer # this will set the output property that</p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># the first layer in &quot;prev&quot; object is expecting <span style=" color: #231F20;">self.input_layer.forward(X, training)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Call forward method of every object in a chain</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Pass output of the previous object as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.forward(layer.prev.output, training)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># &quot;layer&quot; is now the last object from the list, # return its output</p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">return <span style=" color: #231F20;">layer.output</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Performs backward pass</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">backward</span>(<span class="s23">self</span>, <span class="s23">output</span>, <span class="s23">y</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># If softmax classifier</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">if <span style=" color: #231F20;">self.softmax_classifier_output </span>is not <span style=" color: #7358A5;">None</span><span style=" color: #231F20;">: </span><span style=" color: #A5A4A5;"># First call backward method</span></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># on the combined activation/loss # this will set dinputs property</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.backward(output, y)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Since we&#39;ll not call backward method of the last layer # which is Softmax activation</p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"># as we used combined activation/loss</p><p class="s10" style="padding-top: 1pt;padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># object, let&#39;s set dinputs in this object </span>self.layers[<span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>].dinputs <span style=" color: #C71F43;">= </span>\</p><p class="s10" style="padding-left: 116pt;text-indent: 0pt;text-align: left;">self.softmax_classifier_output.dinputs</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Call backward method going through # all the objects but last</p><p class="s11" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers[:</span>-<span style=" color: #7358A5;">1</span><span style=" color: #231F20;">]):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 24pt;line-height: 216%;text-align: left;">layer.backward(layer.next.dinputs) <span style=" color: #C71F43;">return</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># First call backward method on the loss</p><p class="s17" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># this will set dinputs property that the last # layer will try to access shortly <span style=" color: #231F20;">self.loss.backward(output, y)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Call backward method going through all the objects # in reversed order passing dinputs as a parameter </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #32A7BD;">reversed</span><span style=" color: #231F20;">(self.layers):</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">layer.backward(layer.next.dinputs)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Retrieves and returns parameters of trainable layers</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">get_parameters</span>(<span class="s23">self</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create a list for parameters </span>parameters <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Iterable trainable layers and get their parameters </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">self.trainable_layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">parameters.append(layer.get_parameters())</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return a list <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">parameters</span></p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"># Updates the model with new parameters</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">set_parameters</span>(<span class="s23">self</span>, <span class="s23">parameters</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Iterate over the parameters and layers</p><p class="s11" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and update each layers with each set of the parameters </span>for <span style=" color: #231F20;">parameter_set, layer </span>in <span style=" color: #32A7BD;">zip</span><span style=" color: #231F20;">(parameters,</span></p><p class="s10" style="padding-left: 92pt;text-indent: 169pt;line-height: 108%;text-align: left;">self.trainable_layers): layer.set_parameters(<span style=" color: #C71F43;">*</span>parameter_set)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Saves the parameters to a file</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">save_parameters</span>(<span class="s23">self</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Open a file in the binary-write mode # and save parameters into it</p><p class="s10" style="padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;"><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;wb&#39;</span>) <span style=" color: #C71F43;">as </span>f: pickle.dump(self.get_parameters(), f)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Loads the weights and updates a model instance with them</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load_parameters</span>(<span class="s23">self</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Open file in the binary-read mode,</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># load weights and update trainable layers </span><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;rb&#39;</span>) <span style=" color: #C71F43;">as </span>f:</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">self.set_parameters(pickle.load(f))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Saves the model</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">save</span>(<span class="s23">self</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Make a deep copy of current model instance </span>model <span style=" color: #C71F43;">= </span>copy.deepcopy(self)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Reset accumulated values in loss and accuracy objects <span style=" color: #231F20;">model.loss.new_pass()</span></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;text-align: left;">model.accuracy.new_pass()</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># Remove data from the input layer</p><p class="s10" style="padding-top: 1pt;padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># and gradients from the loss object </span>model.input_layer.<u> </u>dict<u> </u>.pop(<span style=" color: #8F8633;">&#39;output&#39;</span>, <span style=" color: #7358A5;">None</span>) model.loss.<u> </u>dict<u> </u>.pop(<span style=" color: #8F8633;">&#39;dinputs&#39;</span>, <span style=" color: #7358A5;">None</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each layer remove inputs, output and dinputs properties </span>for <span style=" color: #231F20;">layer </span>in <span style=" color: #231F20;">model.layers:</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;"><span style=" color: #C71F43;">for </span><span style=" color: #32A7BD;">property </span><span style=" color: #C71F43;">in </span>[<span style=" color: #8F8633;">&#39;inputs&#39;</span>, <span style=" color: #8F8633;">&#39;output&#39;</span>, <span style=" color: #8F8633;">&#39;dinputs&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 116pt;text-indent: 78pt;line-height: 108%;text-align: left;"><span style=" color: #8F8633;">&#39;dweights&#39;</span>, <span style=" color: #8F8633;">&#39;dbiases&#39;</span>]: layer.<u> </u>dict<u> </u>.pop(<span style=" color: #32A7BD;">property</span>, <span style=" color: #7358A5;">None</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Open a file in the binary-write mode and save the model </span><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;wb&#39;</span>) <span style=" color: #C71F43;">as </span>f:</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">pickle.dump(model, f)</p><p class="s17" style="padding-top: 2pt;padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Loads and returns a model <span style=" color: #231F20;">@</span><span style=" color: #32A7BD;">staticmethod</span></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load</span>(<span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Open file in the binary-read mode, load a model </span><span style=" color: #C71F43;">with </span><span style=" color: #32A7BD;">open</span>(path, <span style=" color: #8F8633;">&#39;rb&#39;</span>) <span style=" color: #C71F43;">as </span>f:</p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>pickle.load(f)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;line-height: 108%;text-align: left;"># Return a model <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">model</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Loads a MNIST dataset</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">load_mnist_dataset</span>(<span class="s23">dataset</span>, <span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Scan all the directories and create a list of labels </span>labels <span style=" color: #C71F43;">= </span>os.listdir(os.path.join(path, dataset))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Create lists for samples and labels </span>X <span style=" color: #C71F43;">= </span>[]</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;">y <span style=" color: #C71F43;">= </span>[]</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># For each label folder </span>for <span style=" color: #231F20;">label </span>in <span style=" color: #231F20;">labels:</span></p><p class="s17" style="padding-left: 68pt;text-indent: 0pt;text-align: left;"># And for each image in given folder</p><p class="s11" style="padding-top: 1pt;padding-left: 92pt;text-indent: -24pt;line-height: 108%;text-align: left;">for <span style=" color: #231F20;">file </span>in <span style=" color: #231F20;">os.listdir(os.path.join(path, dataset, label)): </span><span style=" color: #A5A4A5;"># Read the image</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">image <span style=" color: #C71F43;">= </span>cv2.imread(</p><p class="s10" style="padding-top: 1pt;padding-left: 165pt;text-indent: 0pt;line-height: 108%;text-align: left;">os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 92pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And append it and a label to the lists <span style=" color: #231F20;">X.append(image)</span></p><p class="s10" style="padding-left: 92pt;text-indent: 0pt;text-align: left;">y.append(label)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Convert the data to proper numpy arrays and return </span><span style=" color: #C71F43;">return </span>np.array(X), np.array(y).astype(<span style=" color: #8F8633;">&#39;uint8&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># MNIST dataset (train + test)</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;"><span class="s31">def </span><span style=" color: #427F3C;">create_data_mnist</span>(<span class="s23">path</span>):</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"># Load both sets separately</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X, y <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;train&#39;</span>, path)</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">X_test, y_test <span style=" color: #C71F43;">= </span>load_mnist_dataset(<span style=" color: #8F8633;">&#39;test&#39;</span>, path)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 44pt;text-indent: 0pt;line-height: 108%;text-align: left;"># And return all the data <span style=" color: #C71F43;">return </span><span style=" color: #231F20;">X, y, X_test, y_test</span></p><p class="s10" style="padding-top: 2pt;padding-left: 20pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Label index to label name relation </span>fashion_mnist_labels <span style=" color: #C71F43;">= </span>{</p><p class="s10" style="padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">0</span>: <span style=" color: #8F8633;">&#39;T-shirt/top&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">1</span>: <span style=" color: #8F8633;">&#39;Trouser&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">2</span>: <span style=" color: #8F8633;">&#39;Pullover&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">3</span>: <span style=" color: #8F8633;">&#39;Dress&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">4</span>: <span style=" color: #8F8633;">&#39;Coat&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">5</span>: <span style=" color: #8F8633;">&#39;Sandal&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">6</span>: <span style=" color: #8F8633;">&#39;Shirt&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">7</span>: <span style=" color: #8F8633;">&#39;Sneaker&#39;</span>,</p><p class="s10" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;"><span style=" color: #7358A5;">8</span>: <span style=" color: #8F8633;">&#39;Bag&#39;</span>,</p><p class="s14" style="padding-top: 1pt;padding-left: 44pt;text-indent: 0pt;text-align: left;">9<span style=" color: #231F20;">: </span><span style=" color: #8F8633;">&#39;Ankle boot&#39;</span></p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">}</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Read an image</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>cv2.imread(<span style=" color: #8F8633;">&#39;pants.png&#39;</span>, cv2.IMREAD_GRAYSCALE)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s10" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Resize to the same size as Fashion MNIST images </span>image_data <span style=" color: #C71F43;">= </span>cv2.resize(image_data, (<span style=" color: #7358A5;">28</span>, <span style=" color: #7358A5;">28</span>))</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s11" style="padding-left: 19pt;text-indent: 0pt;line-height: 108%;text-align: left;"><span style=" color: #A5A4A5;"># Invert image colors </span><span style=" color: #231F20;">image_data </span>= <span style=" color: #7358A5;">255 </span>- <span style=" color: #231F20;">image_data</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Reshape and scale pixel data</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">image_data <span style=" color: #C71F43;">= </span>(image_data.reshape(<span style=" color: #7358A5;">1</span>, <span style=" color: #C71F43;">-</span><span style=" color: #7358A5;">1</span>).astype(np.float32) <span style=" color: #C71F43;">- </span><span style=" color: #7358A5;">127.5</span>) <span style=" color: #C71F43;">/ </span><span style=" color: #7358A5;">127.5</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Load the model</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">model <span style=" color: #C71F43;">= </span>Model.load(<span style=" color: #8F8633;">&#39;fashion_mnist.model&#39;</span>)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Predict on the image</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">confidences <span style=" color: #C71F43;">= </span>model.predict(image_data)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Get prediction instead of confidence levels</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;text-align: left;">predictions <span style=" color: #C71F43;">= </span>model.output_layer_activation.predictions(confidences)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s17" style="padding-left: 19pt;text-indent: 0pt;text-align: left;"># Get label name from label index</p><p class="s10" style="padding-top: 1pt;padding-left: 19pt;text-indent: 0pt;line-height: 225%;text-align: left;">prediction <span style=" color: #C71F43;">= </span>fashion_mnist_labels[predictions[<span style=" color: #7358A5;">0</span>]] <span style=" color: #32A7BD;">print</span>(prediction)</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-top: 4pt;padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch22" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch22<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s7" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a name="bookmark348">Chapter 23</a><a name="bookmark349">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s2" style="padding-left: 8pt;text-indent: 0pt;text-align: left;">Closing</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 30pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">By now, you should have a full understanding of how data passes forward through neural networks, how error and accuracy are measured, how to perform backpropagation to acquire a gradient, how network parameters are optimized based on the gradient, and how to use a trained model to make predictions. As we close this book, the reality is that you are just getting started, as is this field. There are various types of layers besides the dense layer used here, mainly <b>recurrent </b>and <b>convolutional </b>layers, and many more use-cases for neural networks than what we covered here (classification and regression). The field of deep learning is constantly evolving and building on top of the foundations introduced in this book. Our hope is that we’ve equipped you with</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">the ability to move with these advances and to best leverage neural networks of today to solve problems.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Whether you’re using a neural network to classify cats vs dogs, to generate audio, or for a chatbot, the challenge is still the same — <i>how do you best structure your problem so a neural network can fit and generalize it?</i></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Inevitably, when things don’t work the way you hoped — <i>how do you interpret the network’s failure?</i></p><p style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">You’re now equipped with the knowledge required to get a complete picture of what’s happening (or not happening) within the network. What’s happening to the input as it goes through layers?</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Is gradient exploding? You can feed a sample through and see exactly how values throughout the network are calculated. At this point as well, you’ve seen us break down the calculation of partial derivatives for four loss functions (five, counting softmax and categorical cross-entropy</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">combined). From here, if you were so inclined, you could even attempt to create your own custom loss function. Where to go from here is up to you.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">We’d now like to enumerate and acknowledge those who have helped to make this book possible. First, we’d like to thank the teams behind the following packages which have helped ordinary people like us to instantly leverage an incredible amount of knowledge and intellect with</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">respect to deep learning with a simple <i>pip install</i>: NumPy, TensorFlow, Keras, Pytorch, Theano, Matplotlib, and OpenCV.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">More specifically, we’d like to thank Grant Sanderson (<i>3Blue1Brown </i>on YouTube) for the <i>Manim </i>package and for pushing the limits of mathematics-based, high quality, visualizations, and sharing this with the world both through his own teaching material as well as through sharing the <i>Manim </i>package. We extended this package for use with almost all of our animations and figures.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Finally, we’d like to thank the contributors to this book. Both <b>Spencer Churchill </b>and <b>Thomas Halvorson </b>have been incredibly helpful as editors of their own volition and free time. With countless improvements and suggestions, Spencer and Thomas stood out with firm grasps of both the English language and the subject matter contained within this book. A huge thank you to you both from the authors.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">There have been many other people who have made contributions to the text as well via the Google Doc, which we’ll list here:</p><p class="s3" style="padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Kevin C, Wayne Kiely, Chad DeRosier, Christian Liz-Fonts, Steve Martin, Wayne Kiely, David Arkless, William Mailhot, Tom H, Andrew Doud, Larry Cawley, Yuval Omer, Bryan Bischof, Rasmus Larsson, Armando Bezies, Michael Pegnam, Maarten Roos, Adam Walter, Christian Stankovic, Leon Shams-Schaal, Emilie Parent, Andy Brice, Aaron Passchier, Payson Chadrow, Andrew Doud, Andy Passmore, Jyotinder Singh, Coltan Gowan, Steffan Johansson, Dimitris Mermiklis, Philipp Tölle, Chris Gooding, Ernesto Ramos, Pramit Shah, Wayne Poulson, Sai Anish Malla, Mike Gropp, RD Lagos, Dr Butensky Moshe, Nicholas Obert, Adrian Bool, Jyotinder Singh, Lucas Parzianello, Leene 04, Rudy Depena, Jonathan Lindbloom, Ahmed Hassan, Sebastian Wette, Matheus Ferreira, Dimitris Mermiklis, Lukas Heine, Harshith Thota, Adam Strike, Andrew Doud, Kevin Byrne, Luca Passani, Kyler Crank, Chris Hahn, Büşra Emir , Ethan Zemelman, Jozef Alexovic, Tayler Porter, Roberto Tomás, Gavin Reddy, CurryWaffel, Daniel Alves, Mees Broer, David Canaday, Austin Johansen, Jan Kuparinen, Daniel Tsang, Nate Krasner, Everman Robert, Rick Roach, Addison Weatherhead, Christian Liz-Fonts, Christopher Varnon, Michael Peres, Eytan Ohana, Wes Simpson, Tommy Katzenellenbogen, João Pedro Ferreira,</p><p class="s3" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 115%;text-align: left;">Patrick le, Yannick Poulin-giroux, Alex Duenas, Omota Paul-Daniel, Akira Kurusu, Andrew Yatzkan, Jesús Peña Martín, Ben Schneider, Steve, Stephen O’Brien, Steve, Diego López González, Andrew Doud, Juan Felix, Jeff Echt, Dimitris Mermiklis, Adrian Bool, Ulrich Storkenmaier- Fabinyi, John Smith, Allan McElroy, Leo Liang, <span class="p">and </span>Harrison McCarty, Matt Williams, Magnus Imber, Ag, Andy Junghyun Kim, Alec Mather, Marijn Morssink, Stijn Schatteman, Yang Liu, Andrew Krcatovich</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s16" style="padding-left: 103pt;text-indent: 27pt;line-height: 115%;text-align: left;"><a href="https://nnfs.io/ch23" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Supplementary Material: </a>https://nnfs.io/ch23<span class="s13"> </span><span class="p">Chapter code, further resources, and errata for this chapter.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s84" style="padding-top: 17pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">Symbols</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark205" class="s86">1/t decaying 274</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">A</p><p style="padding-left: 15pt;text-indent: -7pt;text-align: left;"><a href="#bookmark121" class="s86">accuracy categorical 129</a></p><p style="padding-left: 15pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark293" class="s86">regression 432</a></p><p class="s87" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">Index</p><p style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark224" class="s86">cross-validation 329</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">D</p><p class="s85" style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;">data</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark217" class="s86">out-of-sample 321</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark217" class="s86">testing 321</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s86">training </a><a href="#bookmark227" class="s86">62, 332</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark223" class="s86">validation 328</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark228" class="s86">data augmentation 333</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark314" class="s86">data balancing 537</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark317" class="s86">data flattening 544</a></p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark313" class="s86">data normalization 536</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark85" class="s86">Activation Function (concept) </a><a href="#bookmark95" class="s86">72, </a><a href="#bookmark217" class="s86">77, </a><a href="#bookmark223" class="s86">321, </a><a href="#bookmark227" class="s86">328, 332,</a></p><p style="padding-left: 36pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s86">335, 361</a></p><p style="padding-left: 15pt;text-indent: -7pt;text-align: left;"><a href="#bookmark89" class="s86">Activation Functions linear </a><a href="#bookmark283" class="s86">74, 425</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark93" class="s86">rectified linear (ReLU) </a><a href="#bookmark98" class="s86">76, </a><a href="#bookmark100" class="s86">81, </a><a href="#bookmark102" class="s86">85, 95</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark91" class="s86">sigmoid </a><a href="#bookmark259" class="s86">75, 389</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark104" class="s86">softmax </a><a href="#bookmark179" class="s86">98, </a><a href="#bookmark182" class="s86">220, 230</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark87" class="s86">step 73</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark209" class="s86">AdaGrad 293</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark209" class="s86">adaptive gradient 293</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark213" class="s86">Adaptive Momentum 304 Adaptive Momentum (Adam) </a><a href="#bookmark20" class="s86">304 adjustable parameters 23</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" class="s86">analytical derivative 154</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" class="s86">analytical differentiation 154</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" class="s86">array </a><a href="#bookmark40" class="s86">34, 35</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">B</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark173" class="s86">backpropagation </a><a href="#bookmark174" class="s86">180, 196</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s86">batch 250</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark50" class="s86">batch (data) </a><a href="#bookmark57" class="s86">44, 54</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s86">batches 549</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s86">batch gradient descent 250 Batch Gradient Descent </a><a href="#bookmark322" class="s86">250 batch size 549</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark244" class="s86">Bernoulli distribution 362</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark17" class="s86">biases </a><a href="#bookmark59" class="s86">15, 57</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark19" class="s86">binary classification 19</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark263" class="s86">binary cross-entropy (loss) </a><a href="#bookmark257" class="s86">396 binary logistic regression 388</a></p><p class="s84" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">C</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark211" class="s86">cache (RMSProp) 298</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark114" class="s86">categorical cross-entropy (loss) </a><a href="#bookmark116" class="s86">112, </a><a href="#bookmark176" class="s86">114, </a><a href="#bookmark182" class="s86">215, 230</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark162" class="s86">Chain Rule </a><a href="#bookmark174" class="s86">174, 196</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark341" class="s86">checkpoint 612</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark19" class="s86">classification 19</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark13" class="s86">classification (model) 12</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark13" class="s86">classifications 12</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s86">co-adoption 361</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark104" class="s86">confidence scores 98</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark112" class="s86">cost function 111</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark43" class="s86">cross product 38</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark227" class="s86">data preprocessing </a><a href="#bookmark316" class="s86">332, 543</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark316" class="s86">data scaling 543</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark318" class="s86">data shuffling </a><a href="#bookmark320" class="s86">545, 546</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark70" class="s86">dead neurons 67</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s86">decay rate 274</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark65" class="s86">deep neural network </a><a href="#bookmark16" class="s86">59 dense (layer) </a><a href="#bookmark69" class="s86">14, 66</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark134" class="s86">derivative 139</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" class="s86">analytical 154</a></p><p class="s85" style="padding-left: 8pt;text-indent: 7pt;text-align: left;"><a href="#bookmark140" class="s86">numerical </a>146 differentiation</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark142" class="s86">analytical 154</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark140" class="s86">numerical 146</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark43" class="s86">dot product </a><a href="#bookmark48" class="s86">38, </a><a href="#bookmark58" class="s86">42, 55</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s86">dropout 361</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s86">dropout layer 361</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">E</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark209" class="s86">epsilon (AdaGrad) 293</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark205" class="s86">exponential decaying 274</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">F</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark310" class="s86">Fashion MNIST 532</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark50" class="s86">feature set 44</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark69" class="s86">forward pass 66</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark37" class="s86">fully connected </a><a href="#bookmark69" class="s86">32, 66</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">G</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark70" class="s86">Gaussian distribution 67</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark20" class="s86">generalization 23,</a><a href="#bookmark51" class="s86"> 45</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark201" class="s86">global minimum 258</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark155" class="s86">gradient </a><a href="#bookmark160" class="s86">167, 173</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s86">Gradient Descent 250</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark203" class="s86">gradient explosion 267</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark13" class="s86">ground-truth 12</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">H</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark65" class="s86">hidden layer 59</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark65" class="s86">hidden layers 59</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark40" class="s86">homologous (array) 35</a></p><p class="s84" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">I</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark299" class="s86">inference </a><a href="#bookmark346" class="s86">479, 617</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark57" class="s86">inputs 54</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark138" class="s86">instantaneous slope 145</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">L</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark235" class="s86">L1 regularization 336</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark235" class="s86">L2 regularization 336</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark13" class="s86">labels 12</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark235" class="s86">lambda (regularization) 336</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark36" class="s86">layer (neurons) </a><a href="#bookmark47" class="s86">30, </a><a href="#bookmark57" class="s86">41, </a><a href="#bookmark65" class="s86">54, </a><a href="#bookmark205" class="s86">59 learning rate decay </a><a href="#bookmark198" class="s86">274 learning rate (LR) </a><a href="#bookmark200" class="s86">256, 257</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark143" class="s86">Leibniz’s notation 155</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark89" class="s86">linear (activation) 74</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" class="s86">list (programming) 34</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark202" class="s86">local minimum 259</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s86">log 115</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s86">logarithm 115</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark117" class="s86">log (logarithm) 115</a></p><p class="s85" style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark112" class="s86">loss function </a>111 Loss Functions</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark263" class="s86">binary cross-entropy </a><a href="#bookmark265" class="s86">396, 398</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark114" class="s86">categorical cross-entropy </a><a href="#bookmark176" class="s86">112, </a><a href="#bookmark182" class="s86">215, </a><a href="#bookmark289" class="s86">230 mean absolute error loss (MAE) </a><a href="#bookmark285" class="s86">429 mean squared error loss (MSE) 426</a></p><p class="s84" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">M</p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark289" class="s86">MAE (mean absolute error) </a><a href="#bookmark39" class="s86">429 matrix </a><a href="#bookmark40" class="s86">34, 35</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark53" class="s86">matrix product </a><a href="#bookmark58" class="s86">47, </a><a href="#bookmark289" class="s86">55 Mean Absolute Error 429</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark289" class="s86">mean absolute error (loss) </a><a href="#bookmark285" class="s86">429 mean squared error (loss) </a><a href="#bookmark197" class="s86">426 mini-batch 250</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark197" class="s86">Mini-batch Gradient Descent 250</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark197" class="s86">Mini-batch Gradient Descent, MBGD </a><a href="#bookmark310" class="s86">250 MNIST 532</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark328" class="s86">model evaluation 594</a></p><p class="s85" style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark299" class="s86">model inference </a>479 Model Types</p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark257" class="s86">binary logistic regression </a><a href="#bookmark13" class="s86">388 classiffication 12</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark281" class="s86">regression 423</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark285" class="s86">MSE (mean squared error) 426</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">N</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark117" class="s86">natural logarithm 115</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark15" class="s86">neural network 13</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark34" class="s86">neuron </a><a href="#bookmark46" class="s86">26, 40</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark242" class="s86">noise 361</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark140" class="s86">numerical derivative 146</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark140" class="s86">numerical differentiation 146</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark32" class="s86">NumPy 25</a></p><p class="s84" style="padding-top: 3pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">O</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark50" class="s86">observation 44</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark115" class="s86">one-hot (vector) 113</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark125" class="s86">optimization 131</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark195" class="s86">Optimizers 249</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark209" class="s86">AdaGrad 293</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark213" class="s86">Adam 304</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark211" class="s86">RMSProp 298</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s86">Stochastic Gradient Descent (SGD) </a><a href="#bookmark207" class="s86">250 Stochastic Gradient Descent with Momentum 283</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark219" class="s86">out-of-sample data 323</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark20" class="s86">overfitting </a><a href="#bookmark218" class="s86">23, 322</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">P</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark20" class="s86">parameters 23</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark155" class="s86">partial derivative 167</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark346" class="s86">prediction 617</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark19" class="s86">preprocess 19</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark227" class="s86">preprocessing </a><a href="#bookmark228" class="s86">332, </a><a href="#bookmark316" class="s86">333, 543</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">R</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark69" class="s86">random initialization 66</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark93" class="s86">rectified linear (activation) </a><a href="#bookmark98" class="s86">76, </a><a href="#bookmark100" class="s86">81, </a><a href="#bookmark102" class="s86">85, 95</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark281" class="s86">regression (model) 423</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark233" class="s86">Regularization 335</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark235" class="s86">L1 regularization 336</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark235" class="s86">L2 regularization 336</a></p><p style="padding-left: 15pt;text-indent: 0pt;text-align: left;"><a href="#bookmark235" class="s86">lambda 336</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark93" class="s86">ReLU </a><a href="#bookmark98" class="s86">76, </a><a href="#bookmark100" class="s86">81, 85,</a><a href="#bookmark102" class="s86"> 95</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark211" class="s86">rho (RMSProp) 298</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark211" class="s86">RMSProp 298</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark211" class="s86">Root Mean Square Propagation (RMSProp) 298</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">S</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark50" class="s86">sample 44</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark91" class="s86">sigmoid (activation) </a><a href="#bookmark259" class="s86">75, 389</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark137" class="s86">slope 142</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark104" class="s86">softmax (activation) </a><a href="#bookmark179" class="s86">98, </a><a href="#bookmark182" class="s86">220, 230</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark114" class="s86">squared error 112</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark322" class="s86">step 549</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark87" class="s86">step activation 73</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark18" class="s86">step function 18</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark87" class="s86">step function (activation) 73</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s86">Stochastic Gradient Descent (SGD) </a><a href="#bookmark207" class="s86">250 Stochastic Gradient Descent with Momentum </a><a href="#bookmark13" class="s86">283 supervised machine learning 12</a></p><p class="s84" style="padding-top: 8pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">T</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark138" class="s86">tangent line 145</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark13" class="s86">targets 12</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark39" class="s86">tensor </a><a href="#bookmark41" class="s86">34, 37</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark217" class="s86">testing data </a><a href="#bookmark219" class="s86">321, 323</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark324" class="s86">training 563</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark67" class="s86">training data </a><a href="#bookmark219" class="s86">62, 323</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark227" class="s86">training dataset 332</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s62" style="padding-top: 5pt;text-indent: 0pt;text-align: right;">Index | <span class="s63">665</span></p><p style="padding-top: 4pt;padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark55" class="s86">transposition </a><a href="#bookmark58" class="s86">50, 55</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">U</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark13" class="s86">unsupervised machine learning 12</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">V</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark220" class="s86">validation accuracy 324</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark223" class="s86">validation data 328</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark220" class="s86">validation loss 324</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark197" class="s86">Vanilla Gradient Descent </a><a href="#bookmark43" class="s86">250 vector 38</a></p><p style="padding-left: 8pt;text-indent: 0pt;text-align: left;"><a href="#bookmark44" class="s86">vector addition 39</a></p><p class="s84" style="padding-top: 9pt;padding-left: 8pt;text-indent: 0pt;line-height: 20pt;text-align: left;">W</p><p style="padding-left: 8pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark17" class="s86">weights </a><a href="#bookmark57" class="s86">15, 54</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 198pt;text-indent: 0pt;text-align: left;"><span/></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 10pt;padding-left: 72pt;text-indent: 0pt;text-align: center;"><a href="https://nnfs.io/" style=" color: #231F20; font-family:&quot;Times New Roman&quot;, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 12pt;" target="_blank">Website: </a><a href="https://nnfs.io/" class="s9" target="_blank">https://nnfs.io/</a></p></body></html>
